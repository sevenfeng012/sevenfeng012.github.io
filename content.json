{"meta":{"title":"值得荐","subtitle":"值得推荐的原创文章，技术文章或是科普文章","description":"同乐科技","author":"1One's Dad","url":"http://www.wortyby.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-08-29T02:08:14.000Z","updated":"2018-08-29T05:24:24.709Z","comments":false,"path":"/404.html","permalink":"http://www.wortyby.com//404.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-31T09:06:48.000Z","updated":"2018-08-31T09:12:59.268Z","comments":true,"path":"categories/index.html","permalink":"http://www.wortyby.com/categories/index.html","excerpt":"","text":""},{"title":"datastructure","date":"2018-08-31T09:09:28.000Z","updated":"2018-08-31T09:12:41.723Z","comments":true,"path":"datastructure/index.html","permalink":"http://www.wortyby.com/datastructure/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-08-31T09:08:27.000Z","updated":"2018-08-31T09:13:13.095Z","comments":true,"path":"tags/index.html","permalink":"http://www.wortyby.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"矩阵运算和文本处理中的分类问题","slug":"矩阵运算和文本处理中的分类问题","date":"2018-09-05T06:10:34.388Z","updated":"2018-09-05T06:16:39.459Z","comments":true,"path":"2018/09/05/矩阵运算和文本处理中的分类问题/","link":"","permalink":"http://www.wortyby.com/2018/09/05/矩阵运算和文本处理中的分类问题/","excerpt":"","text":"我在大学学习线性代数时，实在想不出它除了告诉我们如何解线性方程外，还能有什么别的用途。关于矩阵的许多概念，比如特征值等等，更是脱离日常生活。后来在数值分析中又学了很多矩阵的近似算法，还是看不到可以应用的地方。当时选这些课，完全是为了混学分的学位。我想，很多同学都多多少少有过类似的经历。直到后来长期做自然语言处理的研究，我才发现数学家们提出那些矩阵的概念和算法，是有实际应用的意义的。在自然语言处理中，最常见的两类的分类问题分别是，将文本按主题归类（比如将所有介绍亚运会的新闻归到体育类）和将词汇表中的字词按意思归类（比如将各种体育运动的名称个归成一类）。这两种分类问题都可用通过矩阵运算来圆满地、同时解决。为了说明如何用矩阵这个工具类解决这两个问题的，让我们先来来回顾一下我们在余弦定理和新闻分类中介绍的方法。分类的关键是计算相关性。我们首先对两个文本计算出它们的内容词，或者说实词的向量，然后求这两个向量的夹角。当这两个向量夹角为零时，新闻就相关；当它们垂直或者说正交时，新闻则无关。当然，夹角的余弦等同于向量的内积。从理论上讲，这种算法非常好。但是计算时间特别长。通常，我们要处理的文章的数量都很大，至少在百万篇以上，二次回标有非常长，比如说有五十万个词（包括人名地名产品名称等等）。如果想通过对一百万篇文章两篇两篇地成对比较，来找出所有共同主题的文章，就要比较五千亿对文章。现在的计算机一秒钟最多可以比较一千对文章，完成这一百万篇文章相关性比较就需要十五年时间。注意，要真正完成文章的分类还要反复重复上述计算。在文本分类中，另一种办法是利用矩阵运算中的奇异值分解（Singular Value Decomposition，简称 SVD)。现在让我们来看看奇异值分解是怎么回事。首先，我们可以用一个大矩阵A来描述这一百万篇文章和五十万词的关联性。这个矩阵中，每一行对应一篇文章，每一列对应一个词。在上面的图中，M=1,000,000，N=500,000。第 i 行，第 j 列的元素，是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF)。读者可能已经注意到了，这个矩阵非常大，有一百万乘以五十万，即五千亿个元素。奇异值分解就是把上面这样一个大矩阵，分解成三个小矩阵相乘，如下图所示。比如把上面的例子中的矩阵分解成一个一百万乘以一百的矩阵X，一个一百乘以一百的矩阵B，和一个一百乘以五十万的矩阵Y。这三个矩阵的元素总数加起来也不过1.5亿，仅仅是原来的三千分之一。相应的存储量和计算量都会小三个数量级以上。三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。现在剩下的唯一问题，就是如何用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值等等，以及数值分析的各种算法就统统用上了。在很长时间内，奇异值分解都无法并行处理。（虽然 Google 早就有了MapReduce 等并行计算的工具，但是由于奇异值分解很难拆成不相关子运算，即使在 Google 内部以前也无法利用并行计算的优势来分解矩阵。）最近，Google 中国的张智威博士和几个中国的工程师及实习生已经实现了奇异值分解的并行算法，我认为这是 Google 中国对世界的一个贡献。","categories":[{"name":"数学","slug":"数学","permalink":"http://www.wortyby.com/categories/数学/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"http://www.wortyby.com/tags/线性代数/"},{"name":"分类","slug":"分类","permalink":"http://www.wortyby.com/tags/分类/"}]},{"title":"PySpark 初体验之DataFrame","slug":"PySpark 初体验之DataFrame","date":"2018-09-04T11:06:16.164Z","updated":"2018-09-04T11:47:24.031Z","comments":true,"path":"2018/09/04/PySpark 初体验之DataFrame/","link":"","permalink":"http://www.wortyby.com/2018/09/04/PySpark 初体验之DataFrame/","excerpt":"","text":"##背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 鉴于这篇文章介绍过RDD，本篇文章重点介绍DataFrame。DataFrame 是一个组织成命名列的数据集。它在概念上等同于关系数据库中的表或是Python 中的里的数据框架(pandas)，但其经过了优化。DataFrames 可以从各种各样的源构建，例如结构化数据文件,Hive 中的表,外部数据库,RDD 转换DataFrame API 可以被Scala，Java，Python和R调用。Python 中 DataFrame 由 Row 的数据集表示 。由于我们现在是在PySpark 的环境中使用，所以我们就谈谈 DataFrame 在 Python 中的使用。或许之前了解过 pandas 里的dataframe 操作，也可以对比着了解，同时 也支持 PySpark DataFrame 与 pandas 的 dataframe 之间的转换。该文将以 Spark初始化，DataFrame 初始化，DataFrame 运算，RDD 与 DataFrame 转化，DataFrame 与 pandas dataframe 转化 四个部分展开 Spark 初始化：在 进行 DataFrame 操作 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkSession 与 SparkContext 初始化123456789from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(&quot;RDD_and_DataFrame&quot;) \\ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \\ .getOrCreate() sc = spark.sparkContextsparkContext 初始化之后，需要进行数据读取来初始化DataFrame DataFrame 初始化我们以从 txt 文档读取数据的方式来初始化DataFrame 的例子来说明 DataFrame 的相关运算方式以及相关性质 创建DataFrame 以及 DataFrame 与 RDD 的转换123456789101112131415161718192021 lines = sc.textFile(&quot;employee.txt&quot;)parts = lines.map(lambda l: l.split(&quot;,&quot;))employee = parts.map(lambda p: Row(name=p[0], salary=int(p[1]))) #RDD转换成DataFrameemployee_temp = spark.createDataFrame(employee) #显示DataFrame数据employee_temp.show() #创建视图employee_temp.createOrReplaceTempView(&quot;employee&quot;)#过滤数据employee_result = spark.sql(&quot;SELECT name,salary FROM employee WHERE salary &gt;= 14000 AND salary &lt;= 20000&quot;) # DataFrame转换成RDDresult = employee_result.rdd.map(lambda p: &quot;name: &quot; + p.name + &quot; salary: &quot; + str(p.salary)).collect() #打印RDD数据for n in result:","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"DataFrame","slug":"DataFrame","permalink":"http://www.wortyby.com/tags/DataFrame/"}]},{"title":"PySpark 初体验之RDD","slug":"PySpark 初体验之运算规则","date":"2018-09-03T08:24:27.468Z","updated":"2018-09-04T11:43:21.196Z","comments":true,"path":"2018/09/03/PySpark 初体验之运算规则/","link":"","permalink":"http://www.wortyby.com/2018/09/03/PySpark 初体验之运算规则/","excerpt":"","text":"背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 本文重点介绍RDD，下篇文章重点介绍DataFrame。RDD 可以导入外部存储系统的数据集，例如：HDFS，HBASE 或者 本地文件 *.JSON 或者*.XML 等数据源。该文将以 SparkContext初始化，RDD 初始化，运算，持久化 四个部分展开 SparkContext 初始化：在 进行RDD 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkContext123from pyspark import SparkConf, SparkContextconf = *** #spark运行环境的配置信息初始化sc = SparkContext(conf)sparkContext 初始化之后，需要进行数据读取来初始化RDD RDD 初始化我们以初始化数字和字符串的例子来说明 RDD的相关运算方式以及相关性质 创建RDD1kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3)]) RDD 转化为 Python 数据类型RDD 类型的数据可以使用 collect 方法转化为 Python 的数据类型:1print (kvRDD.collect())输出为1[(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;, 3)] RDD 基本运算 map 运算map运算可以通过传入的函数，将每一个元素经过函数运算产生另外一个RDD。下面这个例子将 value 值统一加 1之后进行返回 关于 value 的list 。1print ((kvRDD.map(lambda x:x[1]+1).collect()))输出为1[5, 7, 7, 3, 4] filter 运算filter可以用于对RDD内每一个元素进行筛选，并产生另外一个RDD。下面的例子中，我们筛选kvRDD中数字小于3的元素。1print(kvRDD.filter(lambda a:a[0]&gt;3).collect())输出为1[(5, 6), (&apos;sd&apos;, 3)] distinct 运算distinct运算会删除重复的元素：下面的例子中，我们去除kvRDD中重复的的元素('sd',3)。12kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])print(kvRDD. distinct().collect())输出为1[(&apos;sd&apos;, 3), (5, 6), (1, 2), (3, 4), (3, 6)] randomSplit 运算randomSplit 运算将整个集合以随机数的方式按照比例分为多个RDD，比如按照0.4和0.6的比例将kvRDD分为两个RDD，并输出：1234567kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])sRDD = kvRDD.randomSplit([0.4,0.6])print (len(sRDD))print (sRDD[0].collect())print (sRDD[1].collect())输出为1232[(5, 6), (1, 2)][(3, 4), (3, 6), (&apos;sd&apos;, 3), (&apos;sd&apos;, 3)] groupBy 运算groupBy运算可以按照传入匿名函数的规则，将数据分为多个Array。123kvRDD = sc.parallelize([1,2,3,4,5])result = kvRDD.groupBy(lambda x: x % 2).collect()print(sorted([(x, sorted(y)) for (x, y) in result]))输出为1[(0, [2, 4]), (1, [1, 3, 5])] RDD 复合运算RDD 也支持执行多个RDD的运算,即支持 复合运算RDD 复合运算主要包含 并集运算、交集运算、差集运算、笛卡尔积运算先准备三个DEMO RDD123RDD1 = sc.parallelize([3,1,2,5,5])RDD2 = sc.parallelize([5,6])RDD3 = sc.parallelize([2,7]) 并集运算通过使用 union 函数进行并集运算:输入1print(RDD1.union(RDD2).union(RDD3).collect())输出1[3, 1, 2, 5, 5, 5, 6, 2, 7] 交集运算可以使用intersection进行交集运算：输入1print(RDD1.intersection(RDD3).collect())输出1[2] 差集运算可以使用subtract函数进行差集运算:输入1print (RDD1.subtract(RDD2).collect())由于两个RDD的重复部分为5，所以输出为[1,2,3]:输出1[1,2,3] 笛卡尔积运算可以使用cartesian函数进行笛卡尔乘积运算:输入1print (RDD1.cartesian(RDD2).collect())由于两个RDD分别有5个元素和2个元素，所以返回结果有10各元素：输出1[(3, 5), (3, 6), (1, 5), (1, 6), (2, 5), (2, 6), (5, 5), (5, 5), (5, 6), (5, 6)] RDD 基本 “动作” 运算 读取元素可以使用下列命令读取RDD内的元素，这是Actions运算，所以会马上执行：12345678#取第一条数据print (RDD1.first())#取前两条数据print (RDD1.take(2))#升序排列，并取前3条数据print (RDD1.takeOrdered(10))#降序排列，并取前3条数据print (RDD1.takeOrdered(3,lambda x:-x))输出为:12343[3, 1][1, 2, 3, 5, 5][5, 5, 3] 统计功能可以将RDD内的元素进行统计运算：1234567891011121314#统计print (RDD1.stats())#最小值print (RDD1.min())#最大值print (RDD1.max())#标准差print (RDD1.stdev())#计数print (RDD1.count())#求和print (RDD1.sum())#平均print (RDD1.mean())输出为:1234567(count: 5, mean: 3.2, stdev: 1.6, max: 5.0, min: 1.0)151.65163.2 RDD key-Value 基本 “转换” 运算Spark RDD支持键值对运算，Key-Value运算是 MR*(mapreduce)*运算的基础，本节介绍RDD键值的基本“转换”运算。 初始化我们用元素类型为tuple元组的数组初始化我们的RDD，这里，每个tuple的第一个值将作为键，而第二个元素将作为值。1kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)]) 得到key和value值可以使用keys和values函数分别得到RDD的键数组和值数组：12print (kvRDD1.keys().collect())print (kvRDD1.values().collect())输出为：12[3, 3, 5, 1][4, 6, 6, 2] 筛选元素可以按照键进行元素筛选，也可以通过值进行元素筛选，和之前的一样，使用filter函数，这里要注意的是，虽然RDD中是以键值对形式存在，但是本质上还是一个二元组，二元组的第一个值代表键，第二个值代表值，所以按照如下的代码既可以按照键进行筛选，我们筛选键值小于5的数据：1print (kvRDD1.filter(lambda x:x[0] &lt; 5).collect())输出为：1[(3, 4), (3, 6), (1, 2)]同样，将x[0]替换为x[1]就是按照值进行筛选，我们筛选值小于5的数据：1print (kvRDD1.filter(lambda x:x[1] &lt; 5).collect())输出为：1[(3, 4), (1, 2)] 值运算我们可以使用mapValues方法处理value值，下面的代码将value值进行了平方处理：1print (kvRDD1.mapValues(lambda x:x**2).collect())输出为：1[(3, 16), (3, 36), (5, 36), (1, 4)] 按照key排序可以使用sortByKey按照key进行排序，传入参数的默认值为true，是按照从小到大排序，也可以传入参数false，表示从大到小排序：123print (kvRDD1.sortByKey().collect())print (kvRDD1.sortByKey(True).collect())print (kvRDD1.sortByKey(False).collect())输出为：123[(1, 2), (3, 4), (3, 6), (5, 6)][(1, 2), (3, 4), (3, 6), (5, 6)][(5, 6), (3, 4), (3, 6), (1, 2)] 合并相同key值的数据使用reduceByKey函数可以对具有相同key值的数据进行合并。比如下面的代码，由于RDD中存在（3,4）和（3,6）两条key值均为3的数据，他们将被合为一条数据：1print (kvRDD1.reduceByKey(lambda x,y:x+y).collect())输出为1[(1, 2), (3, 10), (5, 6)] 多个RDD Key-Value“转换”运算 初始化首先我们初始化两个k-v的RDD：12kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])kvRDD2 = sc.parallelize([(3,8)]) 内连接运算join运算可以实现类似数据库的内连接，将两个RDD按照相同的key值join起来，kvRDD1与kvRDD2的key值唯一相同的是3，kvRDD1中有两条key值为3的数据（3,4）和（3,6），而kvRDD2中只有一条key值为3的数据（3,8），所以join的结果是（3，（4,8）） 和（3，（6，8））：1print (kvRDD1.join(kvRDD2).collect())输出为:1[(3, (4, 8)), (3, (6, 8))] 左外连接使用leftOuterJoin可以实现类似数据库的左外连接，如果kvRDD1的key值对应不到kvRDD2，就会显示None1print (kvRDD1.leftOuterJoin(kvRDD2).collect())输出为:1[(1, (2, None)), (3, (4, 8)), (3, (6, 8)), (5, (6, None))] 右外连接使用rightOuterJoin可以实现类似数据库的右外连接，如果kvRDD2的key值对应不到kvRDD1，就会显示None1print (kvRDD1.rightOuterJoin(kvRDD2).collect())输出为：1[(3, (4, 8)), (3, (6, 8))] 删除相同key值数据使用subtractByKey运算会删除相同key值得数据：1print (kvRDD1.subtractByKey(kvRDD2).collect())结果为：1[(1, 2), (5, 6)] Key-Value“动作”运算 读取数据可以使用下面的几种方式读取RDD的数据：1234567891011121314#读取第一条数据print (kvRDD1.first())#读取前两条数据print (kvRDD1.take(2))#读取第一条数据的key值print (kvRDD1.first()[0])#读取第一条数据的value值print (kvRDD1.first()[1])输出为:(3, 4)[(3, 4), (3, 6)]34 按key值统计：使用countByKey函数可以统计各个key值对应的数据的条数：1print (kvRDD1.countByKey().collect())输出为：1defaultdict(&lt;type &apos;int&apos;&gt;, &#123;1: 1, 3: 2, 5: 1&#125;) lookup查找运算使用lookup函数可以根据输入的key值来查找对应的Value值：1print (kvRDD1.lookup(3))输出为：1[4, 6] 持久化操作spark RDD的持久化机制，可以将需要重复运算的RDD存储在内存中，以便大幅提升运算效率，有两个主要的函数：持久化使用persist函数对RDD进行持久化：1kvRDD1.persist()在持久化的同时我们可以指定持久化存储等级：等级说明MEMORY_ONLY以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将不会被缓存， 这样当再次需要这些分区的时候，将会重新计算。这是默认的级别。MEMORY_AND_DISK以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将将会缓存在磁盘上，再次需要的时候从磁盘读取。MEMORY_ONLY_SER以序列化JAVA对象的方式存储 (每个分区一个字节数组). 相比于反序列化的方式,这样更高效的利用空间， 尤其是使用快速序列化时。但是读取是CPU操作很密集。MEMORY_AND_DISK_SER与MEMORY_ONLY_SER相似, 区别是但内存不足时，存储在磁盘上而不是每次重新计算。DISK_ONLY只存储RDD在磁盘MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.与上面的级别相同，只不过每个分区的副本只存储在两个集群节点上。OFF_HEAP (experimental)将RDD以序列化的方式存储在 Tachyon. 与 MEMORY_ONLY_SER相比, OFF_HEAP减少了垃圾回收。允许执行体更小通过共享一个内存池。因此对于拥有较大堆内存和高并发的环境有较大的吸引力。更重要的是，因为RDD存储在Tachyon上，执行体的崩溃不会造成缓存的丢失。在这种模式下.Tachyon中的内存是可丢弃的，这样 Tachyon 对于从内存中挤出的块不会试图重建它。如果你打算使用Tachyon作为堆缓存，Spark提供了与Tachyon相兼容的版本。首先我们导入相关函数：1from pyspark.storagelevel import StorageLevel在scala中可以直接使用上述的持久化等级关键词，但是在pyspark中封装为了一个类，StorageLevel类，并在初始化时指定一些参数，通过不同的参数组合，可以实现上面的不同存储等级。StorageLevel类的初始化函数如下：123456def __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1): self.useDisk = useDisk self.useMemory = useMemory self.useOffHeap = useOffHeap self.deserialized = deserialized self.replication = replication那么不同的存储等级对应的参数为:1234567891011121314151617181920StorageLevel.DISK_ONLY = StorageLevel(True, False, False, False)StorageLevel.DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)StorageLevel.MEMORY_ONLY = StorageLevel(False, True, False, False)StorageLevel.MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)StorageLevel.MEMORY_AND_DISK = StorageLevel(True, True, False, False)StorageLevel.MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)StorageLevel.OFF_HEAP = StorageLevel(True, True, True, False, 1)&quot;&quot;&quot;.. note:: The following four storage level constants are deprecated in 2.0, since the records \\will always be serialized in Python.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER = StorageLevel.MEMORY_ONLY&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER_2 = StorageLevel.MEMORY_ONLY_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY_2`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER = StorageLevel.MEMORY_AND_DISK&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER_2 = StorageLevel.MEMORY_AND_DISK_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK_2`` instead.&quot;&quot;&quot; 取消持久化使用unpersist函数对RDD进行持久化：1kvRDD1.unpersist() 综上所述:哇，有关pyspark的RDD的基本操作就是上面这些啦，想要了解更多的盆友们可以参照官网给出的官方文档今天主要介绍了两种RDD，基本的RDD和Key-Value形式的RDD，介绍了他们的几种“转换”运算和“动作”运算，整理如下：RDD运算说明基本RDD“转换”运算map（对各数据进行转换），filter（过滤符合条件的数据），distinct（去重运算），randomSplit（根据指定的比例随机分为N各RDD），groupBy（根据条件对数据进行分组），union（两个RDD取并集），intersection（两个RDD取交集），subtract（两个RDD取差集）。cartesian（两个RDD进行笛卡尔积运算）基本RDD“动作”运算first（取第一条数据），take（取前几条数据），takeOrdered（排序后取前N条数据），统计函数Key-Value形式 RDD“转换”运算filter（过滤符合条件的数据），mapValues（对value值进行转换），sortByKey（根据key值进行排序），reduceByKey（合并相同key值的数据），join（内连接两个KDD），leftOuterJoin（左外连接两个KDD），rightOuterJoin（右外连接两个RDD），subtractByKey（相当于key值得差集运算）Key-Value形式 RDD“动作”运算first（取第一条数据），take（取前几条数据），countByKey（根据key值分组统计），lookup（根据key值查找value值）RDD持久化persist用于对RDD进行持久化，unpersist取消RDD的持久化，注意持久化的存储等级","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"RDD","slug":"RDD","permalink":"http://www.wortyby.com/tags/RDD/"}]},{"title":"tmpwatch 工具使用说明","slug":"tmpwatch 工具使用说明","date":"2018-08-28T11:47:00.266Z","updated":"2018-08-31T09:14:56.837Z","comments":true,"path":"2018/08/28/tmpwatch 工具使用说明/","link":"","permalink":"http://www.wortyby.com/2018/08/28/tmpwatch 工具使用说明/","excerpt":"","text":"背景tmp 目录原来只有在启动的时候才会被清理…… 不同的 Linux 发行版其实对 /tmp 目录的清理方式有所不同：在 Debian-like 的系统，启动的时候才会清理 (规则定义在 /etc/default/rcS)在 RedHat-like 的系统，按文件存在时间定时清理 (RHEL6 规则定义在 /etc/cron.daily/tmpwatch; RHEL7 以及 RedHat-like with systemd 规则定义在 /usr/lib/tmpfiles.d/tmp.conf, 通过 systemd-tmpfiles-clean.service 服务调用)在 CentOS 里，也是按文件存在时间清理的 (通过 crontab 的配置 /etc/cron.daily 定时执行 tmpwatch 来实现)在 Gentoo 里也是启动清理，规则定义在 /etc/conf.d/bootmisc，大 Gentoo 就是不走寻常路对于那些只能开机清理临时文件的发行版，如果作为服务器，这种不重启就对临时文件目录的垃圾不问不管的做事风格实在是很不靠谱。不过从上面其他发行版大家估计也会发现，解决此问题的关键就在于 tmpwatch 和定时任务的配合使用。 服务器除了调用用户的计划任务外，还会执行系统自己的，比如：12345/etc/cron.hourly/etc/cron.daily/etc/cron.daily目的能够自动删除不经常使用的临时文件为了保证tmp目录不爆满，系统默认情况下每日会处理一次tmp目录文件。安装1yum install tmpwatch -y功能说明删除暂存文件(默认是240小时，10天)tmpwatch 是专门用于解决“删除 xxx 天没有被访问/修改过的文件”这样需求的命令。使用方式也极其简单：除了删除tmp 文件夹之外也可以删除任何其他指定文件夹语法tmpwatch [-afqv][–test][超期时间][目录…]执行tmpwatch指令可删除不必要的暂存文件，您可以设置文件超期时间，单位以小时计算。 tmpwatch 参数说明123456789101112131415-u, --atime 基于访问时间来删除文件，默认的。-m, --mtime 基于修改时间来删除文件。-c, --ctime 基于创建时间来删除文件，对于目录，基于mtime。-M, --dirmtime 删除目录基于目录的修改时间而不是访问时间。-a, --all 删除所有的文件类型，不只是普通文件，符号链接和目录。-d, --nodirs 不尝试删除目录，即使是空目录。-d, --nosymlinks 不尝试删除符号链接。-f, --force 强制删除。-q, --quiet 只报告错误信息。-s, --fuser 如果文件已经是打开状态在删除前，尝试使用“定影”命令。默认不启用。-t, --test 仅作测试，并不真的删除文件或目录。-U, --exclude-user=user 不删除属于谁的文件。-v, --verbose 打印详细信息。-x, --exclude=path 排除路径，如果路径是一个目录，它包含的所有文件被排除了。如果路径不存在，它必须是一个绝对路径不包含符号链接。-X, --exclude-pattern=pattern 排除某规则下的路径。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"清理","slug":"清理","permalink":"http://www.wortyby.com/tags/清理/"}]}]}