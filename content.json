{"meta":{"title":"值得荐","subtitle":"值得推荐的原创文章，技术文章或是科普文章","description":"同乐科技","author":"1One's Dad","url":"http://www.wortyby.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-08-29T02:08:14.000Z","updated":"2018-08-29T05:24:24.709Z","comments":false,"path":"/404.html","permalink":"http://www.wortyby.com//404.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-31T09:06:48.000Z","updated":"2018-08-31T09:12:59.268Z","comments":true,"path":"categories/index.html","permalink":"http://www.wortyby.com/categories/index.html","excerpt":"","text":""},{"title":"datastructure","date":"2018-08-31T09:09:28.000Z","updated":"2018-08-31T09:12:41.723Z","comments":true,"path":"datastructure/index.html","permalink":"http://www.wortyby.com/datastructure/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-08-31T09:08:27.000Z","updated":"2018-08-31T09:13:13.095Z","comments":true,"path":"tags/index.html","permalink":"http://www.wortyby.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"HBase中CloumnFamily的设计规则","slug":"HBase中CloumnFamily的设计规则","date":"2018-09-14T05:45:09.944Z","updated":"2018-09-14T05:46:13.990Z","comments":true,"path":"2018/09/14/HBase中CloumnFamily的设计规则/","link":"","permalink":"http://www.wortyby.com/2018/09/14/HBase中CloumnFamily的设计规则/","excerpt":"","text":"HBase本身的设计目标是 支持稀疏表，而 稀疏表通常会有很多列，但是每一行有值的列又比较少。如果不使用Column Family的概念，那么有两种设计方案：1.把所有列的数据放在一个文件中（也就是传统的按行存储）。那么当我们想要访问少数几个列的数据时，需要遍历每一行，读取整个表的数据，这样子是很低效的。2.把每个列的数据单独分开存在一个文件中（按列存储）。那么当我们想要访问少数几个列的数据时，只需要读取对应的文件，不用读取整个表的数据，读取效率很高。然而，由于稀疏表通常会有很多列，这会导致文件数量特别多，这本身会影响文件系统的效率。而Column Family的提出就是为了在上面两种方案中做一个折中。HBase中 将一个Column Family中的列存在一起，而不同Column Family的数据则分开。由于在HBase中Column Family的数量通常很小，同时HBase建议把经常一起访问的比较类似的列放在同一个Column Family中，这样就可以在访问少数几个列时，只读取尽量少的数据。优化：因为一直在做hbase的应用层面的开发，所以体会的比较深的一点是hbase的表结构设计会对系统的性能以及开销上造成很大的区别，本篇文章先按照hbase表中的rowkey、columnfamily、column、timestamp几个方面进行一些分析。最后结合分析如何设计一种适合应用的高效表结构。1、表的属性(1)最大版本数：通常是3，如果对于更新比较频繁的应用完全可以设置为1，能够快速的淘汰无用数据，对于节省存储空间和提高查询速度有效果。不过这类需求在海量数据领域比较小众。(2)压缩算法：可以尝试一下最新出炉的snappy算法，相对lzo来说，压缩率接近，压缩效率稍高，解压效率高很多。(3)inmemory：表在内存中存放，一直会被忽略的属性。如果完全将数据存放在内存中，那么hbase和现在流行的内存数据库memorycached和redis性能差距有多少，尚待实测。(4)bloomfilter：根据应用来定，看需要精确到rowkey还是column。不过这里需要理解一下原理，bloomfilter的作用是对一个region下查找记录所在的hfile有用。即如果一个region下的hfile数量很多，bloomfilter的作用越明显。适合那种compaction赶不上flush速度的应用。2、rowkeyrowkey是hbase的key-value存储中的key，通常使用用户要查询的字段作为rowkey，查询结果作为value。可以通过设计满足几种不同的查询需求。(1)数字rowkey的从大到小排序：原生hbase只支持从小到大的排序，这样就对于排行榜一类的查询需求很尴尬。那么采用rowkey = Integer.MAX_VALUE-rowkey的方式将rowkey进行转换，最大的变最小，最小的变最大。在应用层再转回来即可完成排序需求。(2)rowkey的散列原则：如果rowkey是类似时间戳的方式递增的生成，建议不要使用正序直接写入rowkey，而是采用reverse的方式反转rowkey，使得rowkey大致均衡分布，这样设计有个好处是能将regionserver的负载均衡，否则容易产生所有新数据都在一个regionserver上堆积的现象，这一点还可以结合table的预切分一起设计。3、columnfamilycolumnfamily尽量少，原因是过多的columnfamily之间会互相影响。4、column对于column需要扩展的应用，column可以按普通的方式设计，但是对于列相对固定的应用，最好采用将一行记录封装到一个column中的方式，这样能够节省存储空间。封装的方式推荐protocolbuffer。以下会分场景介绍一些特殊的表结构设计方法，只是一些摸索，欢迎讨论：value数目过多场景下的表结构设计：目前我碰到了一种key-value的数据结构，某一个key下面包含的column很多，以致于客户端查询的时候oom，bulkload写入的时候oom，regionsplit的时候失败这三种后果。通常来讲，hbase的column数目不要超过百万这个数量级。在官方的说明和我实际的测试中都验证了这一点。有两种思路可以参考，第一种是单独处理这些特殊的rowkey，第二种如下：可以考虑将column设计到rowkey的方法解决。例如原来的rowkey是uid1,，column是uid2，uid3…。重新设计之后rowkey为&lt;uid2&gt;，&lt;uid1&gt;…当然大家会有疑问，这种方式如何查询，如果要查询uid1下面的所有uid怎么办。这里说明一下hbase并不是只有get一种随机读取的方法。而是含有scan(startkey,endkey)的扫描方法，而这种方法和get的效率相当。需要取得uid1下的记录只需要new Scan(“uid1&quot;,&quot;uid1~”)即可。这里的设计灵感来自于hadoop world大会上的一篇文章，这篇文章本身也很棒，推荐大家看一下http://www.cloudera.com/resource/hadoop-world-2011-presentation-slides-advanced-hbase-schema-design/其他参考资料： HBase性能优化方法总结（一）：表的设计http://www.cnblogs.com/panfeng412/archive/2012/03/08/hbase-performance-tuning-section1.html关于cloumn family的描述：不要在一张表里定义太多的****column family。目前Hbase并不能很好的处理超过2~3个column family的表。因为某个column family在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://www.wortyby.com/tags/HBASE/"}]},{"title":"Python 进程管理工具 Supervisor 使用教程","slug":"Python 进程管理工具 Supervisor 使用教程","date":"2018-09-06T11:16:21.880Z","updated":"2018-09-06T11:57:32.700Z","comments":true,"path":"2018/09/06/Python 进程管理工具 Supervisor 使用教程/","link":"","permalink":"http://www.wortyby.com/2018/09/06/Python 进程管理工具 Supervisor 使用教程/","excerpt":"","text":"Supervisor 是基于 Python 的进程管理工具，可以帮助我们更简单的启动、重启和停止服务器上的后台进程，是 Linux 服务器管理的效率工具。什么情况下我们需要进程管理呢？就是执行一些需要以守护进程方式启动的程序，比如一个后台任务、一组 Web 服务的进程（说是一组，是因为经常用 Nginx 来做负载均衡），这些很可能是一些网站、REST API 的服务、消息推送的后台服务、日志数据的处理分析服务等等。需要注意的是 Supervisor 是通用的进程管理工具，可以用来启动任意进程，不仅仅是用来管理 Python 进程。Supervisor 经常被用来管理由 gunicorn 启动的 Django 或 Flask 等 Web 服务的进程。我最常用的是用来管理和启动一组 Tornado 进程来实现负载均衡。除此之外，Supervisor 还能很友好的管理程序在命令行上输出的日志，可以将日志重定向到自定义的日志文件中，还能按文件大小对日志进行分割。目前 Supervisor 只能运行在 Unix-Like 的系统上，也就是无法运行在 Windows 上。Supervisor 官方版目前只能运行在 Python 2.4 以上版本，但是还无法运行在 Python 3 上，不过已经有一个 Python 3 的移植版 supervisor-py3k。Supervisor 有两个主要的组成部分：supervisord，运行 Supervisor 时会启动一个进程 supervisord，它负责启动所管理的进程，并将所管理的进程作为自己的子进程来启动，而且可以在所管理的进程出现崩溃时自动重启。supervisorctl，是命令行管理工具，可以用来执行 stop、start、restart 等命令，来对这些子进程进行管理。 安装sudo pip install supervisor 创建配置文件echo_supervisord_conf &gt; /etc/supervisord.conf 如果出现没有权限的问题，可以使用这条命令sudo su - root -c &quot;echo_supervisord_conf &gt; /etc/supervisord.conf&quot; 配置文件说明想要了解怎么配置需要管理的进程，只要打开 supervisord.conf 就可以了，里面有很详细的注释信息。打开配置文件vim /etc/supervisord.conf 默认的配置文件是下面这样的，但是这里有个坑需要注意，supervisord.pid 以及 supervisor.sock 是放在 /tmp 目录下，但是 /tmp 目录是存放临时文件，里面的文件是会被 Linux 系统删除的，一旦这些文件丢失，就无法再通过 supervisorctl 来执行 restart 和 stop 命令了，将只会得到 unix:///tmp/supervisor.sock 不存在的错误 。因此可以单独建一个文件夹，来存放这些文件，比如放在 /home/supervisor/创建文件夹mkdir /home/supervisor mkdir /var/log/supervisor mkdir /etc/supervisor.d 然后对一些配置进行修改1234567891011121314151617181920212223242526272829303132333435363738394041[unix_http_server] ;file=/tmp/supervisor.sock ; (the path to the socket file) ;修改为 /home/supervisor 目录，避免被系统删除 file=/home/supervisor/supervisor.sock ; (the path to the socket file) ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for ;all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ... [supervisord] ;logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log) ;修改为 /var/log 目录，避免被系统删除 logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) ; 日志文件多大时进行分割 logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) ; 最多保留多少份日志文件 logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) ;pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ;修改为 /home/supervisor 目录，避免被系统删除 pidfile=/home/supervisor/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ... ;设置启动supervisord的用户，一般情况下不要轻易用root用户来启动，除非你真的确定要这么做 ;user=chrism ; (default is current user, required if root) ... [supervisorctl] ; 必须和&apos;unix\\_http\\_server&apos;里面的设定匹配 ;serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socket ;修改为 /home/supervisor 目录，避免被系统删除 serverurl=unix:///home/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set ...默认情况下，进程的日志文件达到50MB时，将进行分割，最多保留10个文件，当然这些配置也可以对每个进程单独配置。 权限问题设置好配置文件后，应先创建上述配置文件中新增的文件夹。如果指定了启动用户 user，这里以 oxygen 为例，那么应注意相关文件的权限问题，包括日志文件，否则会出现没有权限的错误。例如设置了启动用户 oxygen，然后启动 supervisord 出现错误Error: Cannot open an HTTP server: socket.error reported errno.EACCES (13) 就是由于上面的配置文件中 /home/supervisor 文件夹，没有授予启动 supervisord 的用户 oxygen 的写权限，可以将这个文件夹的拥有者设置该该账号sudo chown oxygen /home/supervisor 一般情况下，我们可以用 root 用户启动 supervisord 进程，然后在其所管理的进程中，再具体指定需要以那个用户启动这些进程。 使用浏览器来管理supervisor 同时提供了通过浏览器来管理进程的方法，只需要注释掉如下几行就可以了。12345678910;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for ;all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) [supervisorctl] ... ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set 使用 include在配置文件的最后，有一个 [include] 的配置项，跟 Nginx 一样，可以 include 某个文件夹下的所有配置文件，这样我们就可以为每个进程或相关的几个进程的配置单独写成一个文件。12[include]files = /etc/supervisor.d/*.ini 进程的配置样例sudo pip install supervisor 一个简单的例子如下; 设置进程的名称，使用 supervisorctl 来管理进程时需要使用该进程名 [program:your_program_name] command=python server.py --port=9000 ;numprocs=1 ; 默认为1 ;process_name=%(program_name)s ; 默认为 %(program_name)s，即 [program:x] 中的 x directory=/home/python/tornado_server ; 执行 command 之前，先切换到工作目录 user=oxygen ; 使用 oxygen 用户来启动该进程 ; 程序崩溃时自动重启，重启次数是有限制的，默认为3次 autorestart=true redirect_stderr=true ; 重定向输出的日志 stdout_logfile = /var/log/supervisor/tornado_server.log loglevel=info 设置日志级别loglevel 指定了日志的级别，用 Python 的 print 语句输出的日志是不会被记录到日志文件中的，需要搭配 Python 的 logging 模块来输出有指定级别的日志。 多个进程按照官方文档的定义，一个 [program:x] 实际上是表示一组相同特征或同类的进程组，也就是说一个 [program:x] 可以启动多个进程。这组进程的成员是通过 numprocs 和 process_name 这两个参数来确定的，这句话什么意思呢，我们来看这个例子。12345678910111213; 设置进程的名称，使用 supervisorctl 来管理进程时需要使用该进程名 [program:foo] ; 可以在 command 这里用 python 表达式传递不同的参数给每个进程 command=python server.py --port=90%(process_num)02d directory=/home/python/tornado_server ; 执行 command 之前，先切换到工作目录 ; 若 numprocs 不为1，process\\_name 的表达式中一定要包含 process_num 来区分不同的进程 numprocs=2 process_name=%(program_name)s_%(process_num)02d; user=oxygen ; 使用 oxygen 用户来启动该进程 autorestart=true ; 程序崩溃时自动重启 redirect_stderr=true ; 重定向输出的日志 stdout_logfile = /var/log/supervisor/tornado_server.log loglevel=info上面这个例子会启动两个进程，process_name 分别为 foo:foo_01 和 foo:foo_02。通过这样一种方式，就可以用一个 [program:x] 配置项，来启动一组非常类似的进程。再介绍两个配置项 stopasgroup 和 killasgroup12345; 默认为 false，如果设置为 true，当进程收到 stop 信号时，会自动将该信号发给该进程的子进程。如果这个配置项为 true，那么也隐含 killasgroup 为 true。例如在 Debug 模式使用 Flask 时，Flask 不会将接收到的 stop 信号也传递给它的子进程，因此就需要设置这个配置项。 stopasgroup=false ; send stop signal to the UNIX process ; 默认为 false，如果设置为 true，当进程收到 kill 信号时，会自动将该信号发给该进程的子进程。如果这个程序使用了 python 的 multiprocessing 时，就能自动停止它的子线程。 killasgroup=false ; SIGKILL the UNIX process group (def false)更详细的配置例子，可以参考如下，官方文档在这里123456789101112131415161718192021222324252627282930;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;autorestart=unexpected ; whether/when to restart (default: unexpected) ;startsecs=1 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; &apos;expected&apos; exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout\\_logfile\\_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_capture\\_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=&quot;1&quot;,B=&quot;2&quot; ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) 将多个进程按组管理Supervisor 同时还提供了另外一种进程组的管理方式，通过这种方式，可以使用 supervisorctl 命令来管理一组进程。跟 [program:x] 的进程组不同的是，这里的进程是一个个的 [program:x] 。123[group:thegroupname] programs=progname1,progname2 ; each refers to &apos;x&apos; in \\[program:x\\] definitions priority=999 ; the relative start priority (default 999)当添加了上述配置后，progname1 和 progname2 的进程名就会变成 thegroupname:progname1 和 thegroupname:progname2 以后就要用这个名字来管理进程了，而不是之前的 progname1。以后执行 supervisorctl stop thegroupname: 就能同时结束 progname1 和 progname2，执行 supervisorctl stop thegroupname:progname1 就能结束 progname1。supervisorctl 的命令我们稍后介绍。 启动 supervisord执行 supervisord 命令，将会启动 supervisord 进程，同时我们在配置文件中设置的进程也会相应启动。123456# 使用默认的配置文件 /etc/supervisord.conf supervisord # 明确指定配置文件 supervisord -c /etc/supervisord.conf # 使用 user 用户启动 supervisord supervisord -u user更多参数请参考文档 supervisorctl 命令介绍12345678910111213141516# 停止某一个进程，program_name 为 [program:x] 里的 x supervisorctl stop program_name # 启动某个进程 supervisorctl start program_name # 重启某个进程 supervisorctl restart program_name # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理) supervisorctl stop groupworker: # 结束 groupworker:name1 这个进程 (start，restart 同理) supervisorctl stop groupworker:name1 # 停止全部进程，注：start、restart、stop 都不会载入最新的配置文件 supervisorctl stop all # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程 supervisorctl reload # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启 supervisorctl update注意：显示用 stop 停止掉的进程，用 reload 或者 update 都不会自动重启。也可以参考这里 开机自动启动 Supervisord 方法1有一个简单的方法，因为 Linux 在启动的时候会执行 /etc/rc.local 里面的脚本，所以只要在这里添加执行命令就可以12# 如果是 Ubuntu 添加以下内容 /usr/local/bin/supervisord -c /etc/supervisord.conf12# 如果是 Centos 添加以下内容 /usr/bin/supervisord -c /etc/supervisord.conf以上内容需要添加在 exit 命令前，而且由于在执行 rc.local 脚本时，PATH 环境变量未全部初始化，因此命令需要使用绝对路径。可以用 which supervisord 查看一下 supervisord 所在的路径。在添加前，先在终端测试一下命令是否能正常执行，如果找不到 supervisord，可以用如下命令找到sudo find / -name supervisord 如果是 Ubuntu 16.04 以上，rc.local 被当成了服务，而且默认是不会启动，需要手动启用一下服务。https://askubuntu.com/questions/765120/after-upgrade-to-16-04-lts-rc-local-not-executing-command启用 rc.local 服务sudo systemctl enable rc-local.service 方法2Supervisord 默认情况下并没有被安装成服务，它本身也是一个进程。官方已经给出了脚本可以将 Supervisord 安装成服务，可以参考这里查看各种操作系统的安装脚本，但是我用官方这里给的 Ubuntu 脚本却无法运行。安装方法可以参考 serverfault 上的回答。比如我是 Ubuntu 系统，可以这么安装，这里选择了另外一个脚本123456789# 下载脚本 sudo su - root -c &quot;sudo curl https://gist.githubusercontent.com/howthebodyworks/176149/raw/d60b505a585dda836fadecca8f6b03884153196b/supervisord.sh &gt; /etc/init.d/supervisord&quot; # 设置该脚本为可以执行 sudo chmod +x /etc/init.d/supervisord # 设置为开机自动运行 sudo update-rc.d supervisord defaults # 试一下，是否工作正常 service supervisord stop service supervisord start注意：这个脚本下载下来后，还需检查一下与我们的配置是否相符合，比如默认的配置文件路径，pid 文件路径等，如果存在不同则需要进行一些修改。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"python","slug":"python","permalink":"http://www.wortyby.com/tags/python/"}]},{"title":"马尔可夫链的扩展 贝叶斯网络","slug":"马尔可夫链的扩展 贝叶斯网络 ","date":"2018-09-05T06:24:26.034Z","updated":"2018-09-05T06:29:01.637Z","comments":true,"path":"2018/09/05/马尔可夫链的扩展 贝叶斯网络 /","link":"","permalink":"http://www.wortyby.com/2018/09/05/马尔可夫链的扩展 贝叶斯网络 /","excerpt":"","text":"我们在前面的系列中多次提到马尔可夫链 (MarkovChain)，它描述了一种状态序列，其每个状态值取决于前面有限个状态。这种模型，对很多实际问题来讲是一种很粗略的简化。在现实生活中，很多事物相互的关系并不能用一条链来串起来。它们之间的关系可能是交叉的、错综复杂的。比如在下图中可以看到，心血管疾病和它的成因之间的关系是错综复杂的。显然无法用一个链来表示。 信念网络我们可以把上述的有向图看成一个网络，它就是**贝叶斯网络。其中每个圆圈表示一个状态。状态之间的连线表示它们的因果关系。比如从心血管疾病出发到吸烟的弧线表示心血管疾病可能和吸烟有关。当然，这些关系可以有一个量化的可信度 (belief)，用一个概率描述。我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名。由于网络的每个弧有一个可信度，贝叶斯网络也被称作信念网络** (belief networks)。和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述事件之间的相关性。可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。 贝叶斯网络与马尔科夫链的比较使用贝叶斯网络必须知道各个状态之间相关的概率。得到这些参数的过程叫做训练。和训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据。比如在训练上面的网络，需要知道一些心血管疾病和吸烟、家族病史等有关的情况。相比马尔可夫链，贝叶斯网络的训练比较复杂，从理论上讲，它是一个 NP-complete 问题，也就是说，对于现在的计算机是不可计算的。但是，对于某些应用，这个训练过程可以简化，并在计算上实现。 贝叶斯网络的工具与应用值得一提的是 IBM Watson 研究所的茨威格博士 (Geoffrey Zweig) 和西雅图华盛顿大学的比尔默 (Jeff Bilmes) 教授完成了一个通用的贝叶斯网络的工具包，提供给对贝叶斯网络有兴趣的研究者。贝叶斯网络在图像处理、文字处理、支持决策等方面有很多应用。在文字处理方面，语义相近的词之间的关系可以用一个贝叶斯网络来描述。我们利用贝叶斯网络，可以找出近义词和相关的词，在 Google 搜索和 Google 广告中都有直接的应用。原文链接","categories":[{"name":"数学","slug":"数学","permalink":"http://www.wortyby.com/categories/数学/"}],"tags":[{"name":"分类","slug":"分类","permalink":"http://www.wortyby.com/tags/分类/"},{"name":"图","slug":"图","permalink":"http://www.wortyby.com/tags/图/"}]},{"title":"矩阵运算和文本处理中的分类问题","slug":"矩阵运算和文本处理中的分类问题","date":"2018-09-05T06:10:34.388Z","updated":"2018-09-05T06:57:58.038Z","comments":true,"path":"2018/09/05/矩阵运算和文本处理中的分类问题/","link":"","permalink":"http://www.wortyby.com/2018/09/05/矩阵运算和文本处理中的分类问题/","excerpt":"","text":"背景我在大学学习线性代数时，实在想不出它除了告诉我们如何解线性方程外，还能有什么别的用途。关于矩阵的许多概念，比如特征值等等，更是脱离日常生活。后来在数值分析中又学了很多矩阵的近似算法，还是看不到可以应用的地方。当时选这些课，完全是为了混学分的学位。我想，很多同学都多多少少有过类似的经历。直到后来长期做自然语言处理的研究，我才发现数学家们提出那些矩阵的概念和算法，是有实际应用的意义的。 语音分类问题在自然语言处理中，最常见的两类的分类问题分别是，将文本按主题归类（比如将所有介绍亚运会的新闻归到体育类）和将词汇表中的字词按意思归类（比如将各种体育运动的名称个归成一类）。这两种分类问题都可用通过矩阵运算来圆满地、同时解决。为了说明如何用矩阵这个工具类解决这两个问题的，让我们先来来回顾一下我们在余弦定理和新闻分类中介绍的方法。分类的关键是计算相关性。我们首先对两个文本计算出它们的内容词，或者说实词的向量，然后求这两个向量的夹角。当这两个向量夹角为零时，新闻就相关；当它们垂直或者说正交时，新闻则无关。当然，夹角的余弦等同于向量的内积。从理论上讲，这种算法非常好。但是计算时间特别长。通常，我们要处理的文章的数量都很大，至少在百万篇以上，二次回标有非常长，比如说有五十万个词（包括人名地名产品名称等等）。如果想通过对一百万篇文章两篇两篇地成对比较，来找出所有共同主题的文章，就要比较五千亿对文章。现在的计算机一秒钟最多可以比较一千对文章，完成这一百万篇文章相关性比较就需要十五年时间。注意，要真正完成文章的分类还要反复重复上述计算。 矩阵处理在文本分类中，另一种办法是利用矩阵运算中的奇异值分解（Singular Value Decomposition，简称 SVD)。现在让我们来看看奇异值分解是怎么回事。首先，我们可以用一个大矩阵A来描述这一百万篇文章和五十万词的关联性。这个矩阵中，每一行对应一篇文章，每一列对应一个词。在上面的图中，M=1,000,000，N=500,000。第 i 行，第 j 列的元素，是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF)。读者可能已经注意到了，这个矩阵非常大，有一百万乘以五十万，即五千亿个元素。奇异值分解就是把上面这样一个大矩阵，分解成三个小矩阵相乘，如下图所示。比如把上面的例子中的矩阵分解成一个一百万乘以一百的矩阵X，一个一百乘以一百的矩阵B，和一个一百乘以五十万的矩阵Y。这三个矩阵的元素总数加起来也不过1.5亿，仅仅是原来的三千分之一。相应的存储量和计算量都会小三个数量级以上。三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次**奇异值分解**，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。现在剩下的唯一问题，就是如何用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值等等，以及数值分析的各种算法就统统用上了。在很长时间内，奇异值分解都无法并行处理。（虽然 Google 早就有了MapReduce 等并行计算的工具，但是由于奇异值分解很难拆成不相关子运算，即使在 Google 内部以前也无法利用并行计算的优势来分解矩阵。）最近，Google 中国的张智威博士和几个中国的工程师及实习生已经实现了奇异值分解的并行算法，我认为这是 Google 中国对世界的一个贡献。利用相对熵，我们可以到处信息检索中最重要的一个概念：词频率-逆向文档频率（TF/IDF)。我们下回会介绍如何根据相关性对搜索出的网页进行排序，就要用的餐TF/IDF 的概念。另外，在新闻的分类中也要用到相对熵和 TF/IDF。原文链接信息熵","categories":[{"name":"数学","slug":"数学","permalink":"http://www.wortyby.com/categories/数学/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"http://www.wortyby.com/tags/线性代数/"},{"name":"分类","slug":"分类","permalink":"http://www.wortyby.com/tags/分类/"}]},{"title":"PySpark 初体验之DataFrame","slug":"PySpark 初体验之DataFrame","date":"2018-09-04T11:06:16.164Z","updated":"2018-09-04T11:47:24.031Z","comments":true,"path":"2018/09/04/PySpark 初体验之DataFrame/","link":"","permalink":"http://www.wortyby.com/2018/09/04/PySpark 初体验之DataFrame/","excerpt":"","text":"##背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 鉴于这篇文章介绍过RDD，本篇文章重点介绍DataFrame。DataFrame 是一个组织成命名列的数据集。它在概念上等同于关系数据库中的表或是Python 中的里的数据框架(pandas)，但其经过了优化。DataFrames 可以从各种各样的源构建，例如结构化数据文件,Hive 中的表,外部数据库,RDD 转换DataFrame API 可以被Scala，Java，Python和R调用。Python 中 DataFrame 由 Row 的数据集表示 。由于我们现在是在PySpark 的环境中使用，所以我们就谈谈 DataFrame 在 Python 中的使用。或许之前了解过 pandas 里的dataframe 操作，也可以对比着了解，同时 也支持 PySpark DataFrame 与 pandas 的 dataframe 之间的转换。该文将以 Spark初始化，DataFrame 初始化，DataFrame 运算，RDD 与 DataFrame 转化，DataFrame 与 pandas dataframe 转化 四个部分展开 Spark 初始化：在 进行 DataFrame 操作 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkSession 与 SparkContext 初始化123456789from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(&quot;RDD_and_DataFrame&quot;) \\ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \\ .getOrCreate() sc = spark.sparkContextsparkContext 初始化之后，需要进行数据读取来初始化DataFrame DataFrame 初始化我们以从 txt 文档读取数据的方式来初始化DataFrame 的例子来说明 DataFrame 的相关运算方式以及相关性质 创建DataFrame 以及 DataFrame 与 RDD 的转换123456789101112131415161718192021 lines = sc.textFile(&quot;employee.txt&quot;)parts = lines.map(lambda l: l.split(&quot;,&quot;))employee = parts.map(lambda p: Row(name=p[0], salary=int(p[1]))) #RDD转换成DataFrameemployee_temp = spark.createDataFrame(employee) #显示DataFrame数据employee_temp.show() #创建视图employee_temp.createOrReplaceTempView(&quot;employee&quot;)#过滤数据employee_result = spark.sql(&quot;SELECT name,salary FROM employee WHERE salary &gt;= 14000 AND salary &lt;= 20000&quot;) # DataFrame转换成RDDresult = employee_result.rdd.map(lambda p: &quot;name: &quot; + p.name + &quot; salary: &quot; + str(p.salary)).collect() #打印RDD数据for n in result:","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"DataFrame","slug":"DataFrame","permalink":"http://www.wortyby.com/tags/DataFrame/"}]},{"title":"PySpark 初体验之RDD","slug":"PySpark 初体验之运算规则","date":"2018-09-03T08:24:27.468Z","updated":"2018-09-04T11:43:21.196Z","comments":true,"path":"2018/09/03/PySpark 初体验之运算规则/","link":"","permalink":"http://www.wortyby.com/2018/09/03/PySpark 初体验之运算规则/","excerpt":"","text":"背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 本文重点介绍RDD，下篇文章重点介绍DataFrame。RDD 可以导入外部存储系统的数据集，例如：HDFS，HBASE 或者 本地文件 *.JSON 或者*.XML 等数据源。该文将以 SparkContext初始化，RDD 初始化，运算，持久化 四个部分展开 SparkContext 初始化：在 进行RDD 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkContext123from pyspark import SparkConf, SparkContextconf = *** #spark运行环境的配置信息初始化sc = SparkContext(conf)sparkContext 初始化之后，需要进行数据读取来初始化RDD RDD 初始化我们以初始化数字和字符串的例子来说明 RDD的相关运算方式以及相关性质 创建RDD1kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3)]) RDD 转化为 Python 数据类型RDD 类型的数据可以使用 collect 方法转化为 Python 的数据类型:1print (kvRDD.collect())输出为1[(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;, 3)] RDD 基本运算 map 运算map运算可以通过传入的函数，将每一个元素经过函数运算产生另外一个RDD。下面这个例子将 value 值统一加 1之后进行返回 关于 value 的list 。1print ((kvRDD.map(lambda x:x[1]+1).collect()))输出为1[5, 7, 7, 3, 4] filter 运算filter可以用于对RDD内每一个元素进行筛选，并产生另外一个RDD。下面的例子中，我们筛选kvRDD中数字小于3的元素。1print(kvRDD.filter(lambda a:a[0]&gt;3).collect())输出为1[(5, 6), (&apos;sd&apos;, 3)] distinct 运算distinct运算会删除重复的元素：下面的例子中，我们去除kvRDD中重复的的元素('sd',3)。12kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])print(kvRDD. distinct().collect())输出为1[(&apos;sd&apos;, 3), (5, 6), (1, 2), (3, 4), (3, 6)] randomSplit 运算randomSplit 运算将整个集合以随机数的方式按照比例分为多个RDD，比如按照0.4和0.6的比例将kvRDD分为两个RDD，并输出：1234567kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])sRDD = kvRDD.randomSplit([0.4,0.6])print (len(sRDD))print (sRDD[0].collect())print (sRDD[1].collect())输出为1232[(5, 6), (1, 2)][(3, 4), (3, 6), (&apos;sd&apos;, 3), (&apos;sd&apos;, 3)] groupBy 运算groupBy运算可以按照传入匿名函数的规则，将数据分为多个Array。123kvRDD = sc.parallelize([1,2,3,4,5])result = kvRDD.groupBy(lambda x: x % 2).collect()print(sorted([(x, sorted(y)) for (x, y) in result]))输出为1[(0, [2, 4]), (1, [1, 3, 5])] RDD 复合运算RDD 也支持执行多个RDD的运算,即支持 复合运算RDD 复合运算主要包含 并集运算、交集运算、差集运算、笛卡尔积运算先准备三个DEMO RDD123RDD1 = sc.parallelize([3,1,2,5,5])RDD2 = sc.parallelize([5,6])RDD3 = sc.parallelize([2,7]) 并集运算通过使用 union 函数进行并集运算:输入1print(RDD1.union(RDD2).union(RDD3).collect())输出1[3, 1, 2, 5, 5, 5, 6, 2, 7] 交集运算可以使用intersection进行交集运算：输入1print(RDD1.intersection(RDD3).collect())输出1[2] 差集运算可以使用subtract函数进行差集运算:输入1print (RDD1.subtract(RDD2).collect())由于两个RDD的重复部分为5，所以输出为[1,2,3]:输出1[1,2,3] 笛卡尔积运算可以使用cartesian函数进行笛卡尔乘积运算:输入1print (RDD1.cartesian(RDD2).collect())由于两个RDD分别有5个元素和2个元素，所以返回结果有10各元素：输出1[(3, 5), (3, 6), (1, 5), (1, 6), (2, 5), (2, 6), (5, 5), (5, 5), (5, 6), (5, 6)] RDD 基本 “动作” 运算 读取元素可以使用下列命令读取RDD内的元素，这是Actions运算，所以会马上执行：12345678#取第一条数据print (RDD1.first())#取前两条数据print (RDD1.take(2))#升序排列，并取前3条数据print (RDD1.takeOrdered(10))#降序排列，并取前3条数据print (RDD1.takeOrdered(3,lambda x:-x))输出为:12343[3, 1][1, 2, 3, 5, 5][5, 5, 3] 统计功能可以将RDD内的元素进行统计运算：1234567891011121314#统计print (RDD1.stats())#最小值print (RDD1.min())#最大值print (RDD1.max())#标准差print (RDD1.stdev())#计数print (RDD1.count())#求和print (RDD1.sum())#平均print (RDD1.mean())输出为:1234567(count: 5, mean: 3.2, stdev: 1.6, max: 5.0, min: 1.0)151.65163.2 RDD key-Value 基本 “转换” 运算Spark RDD支持键值对运算，Key-Value运算是 MR*(mapreduce)*运算的基础，本节介绍RDD键值的基本“转换”运算。 初始化我们用元素类型为tuple元组的数组初始化我们的RDD，这里，每个tuple的第一个值将作为键，而第二个元素将作为值。1kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)]) 得到key和value值可以使用keys和values函数分别得到RDD的键数组和值数组：12print (kvRDD1.keys().collect())print (kvRDD1.values().collect())输出为：12[3, 3, 5, 1][4, 6, 6, 2] 筛选元素可以按照键进行元素筛选，也可以通过值进行元素筛选，和之前的一样，使用filter函数，这里要注意的是，虽然RDD中是以键值对形式存在，但是本质上还是一个二元组，二元组的第一个值代表键，第二个值代表值，所以按照如下的代码既可以按照键进行筛选，我们筛选键值小于5的数据：1print (kvRDD1.filter(lambda x:x[0] &lt; 5).collect())输出为：1[(3, 4), (3, 6), (1, 2)]同样，将x[0]替换为x[1]就是按照值进行筛选，我们筛选值小于5的数据：1print (kvRDD1.filter(lambda x:x[1] &lt; 5).collect())输出为：1[(3, 4), (1, 2)] 值运算我们可以使用mapValues方法处理value值，下面的代码将value值进行了平方处理：1print (kvRDD1.mapValues(lambda x:x**2).collect())输出为：1[(3, 16), (3, 36), (5, 36), (1, 4)] 按照key排序可以使用sortByKey按照key进行排序，传入参数的默认值为true，是按照从小到大排序，也可以传入参数false，表示从大到小排序：123print (kvRDD1.sortByKey().collect())print (kvRDD1.sortByKey(True).collect())print (kvRDD1.sortByKey(False).collect())输出为：123[(1, 2), (3, 4), (3, 6), (5, 6)][(1, 2), (3, 4), (3, 6), (5, 6)][(5, 6), (3, 4), (3, 6), (1, 2)] 合并相同key值的数据使用reduceByKey函数可以对具有相同key值的数据进行合并。比如下面的代码，由于RDD中存在（3,4）和（3,6）两条key值均为3的数据，他们将被合为一条数据：1print (kvRDD1.reduceByKey(lambda x,y:x+y).collect())输出为1[(1, 2), (3, 10), (5, 6)] 多个RDD Key-Value“转换”运算 初始化首先我们初始化两个k-v的RDD：12kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])kvRDD2 = sc.parallelize([(3,8)]) 内连接运算join运算可以实现类似数据库的内连接，将两个RDD按照相同的key值join起来，kvRDD1与kvRDD2的key值唯一相同的是3，kvRDD1中有两条key值为3的数据（3,4）和（3,6），而kvRDD2中只有一条key值为3的数据（3,8），所以join的结果是（3，（4,8）） 和（3，（6，8））：1print (kvRDD1.join(kvRDD2).collect())输出为:1[(3, (4, 8)), (3, (6, 8))] 左外连接使用leftOuterJoin可以实现类似数据库的左外连接，如果kvRDD1的key值对应不到kvRDD2，就会显示None1print (kvRDD1.leftOuterJoin(kvRDD2).collect())输出为:1[(1, (2, None)), (3, (4, 8)), (3, (6, 8)), (5, (6, None))] 右外连接使用rightOuterJoin可以实现类似数据库的右外连接，如果kvRDD2的key值对应不到kvRDD1，就会显示None1print (kvRDD1.rightOuterJoin(kvRDD2).collect())输出为：1[(3, (4, 8)), (3, (6, 8))] 删除相同key值数据使用subtractByKey运算会删除相同key值得数据：1print (kvRDD1.subtractByKey(kvRDD2).collect())结果为：1[(1, 2), (5, 6)] Key-Value“动作”运算 读取数据可以使用下面的几种方式读取RDD的数据：1234567891011121314#读取第一条数据print (kvRDD1.first())#读取前两条数据print (kvRDD1.take(2))#读取第一条数据的key值print (kvRDD1.first()[0])#读取第一条数据的value值print (kvRDD1.first()[1])输出为:(3, 4)[(3, 4), (3, 6)]34 按key值统计：使用countByKey函数可以统计各个key值对应的数据的条数：1print (kvRDD1.countByKey().collect())输出为：1defaultdict(&lt;type &apos;int&apos;&gt;, &#123;1: 1, 3: 2, 5: 1&#125;) lookup查找运算使用lookup函数可以根据输入的key值来查找对应的Value值：1print (kvRDD1.lookup(3))输出为：1[4, 6] 持久化操作spark RDD的持久化机制，可以将需要重复运算的RDD存储在内存中，以便大幅提升运算效率，有两个主要的函数：持久化使用persist函数对RDD进行持久化：1kvRDD1.persist()在持久化的同时我们可以指定持久化存储等级：等级说明MEMORY_ONLY以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将不会被缓存， 这样当再次需要这些分区的时候，将会重新计算。这是默认的级别。MEMORY_AND_DISK以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将将会缓存在磁盘上，再次需要的时候从磁盘读取。MEMORY_ONLY_SER以序列化JAVA对象的方式存储 (每个分区一个字节数组). 相比于反序列化的方式,这样更高效的利用空间， 尤其是使用快速序列化时。但是读取是CPU操作很密集。MEMORY_AND_DISK_SER与MEMORY_ONLY_SER相似, 区别是但内存不足时，存储在磁盘上而不是每次重新计算。DISK_ONLY只存储RDD在磁盘MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.与上面的级别相同，只不过每个分区的副本只存储在两个集群节点上。OFF_HEAP (experimental)将RDD以序列化的方式存储在 Tachyon. 与 MEMORY_ONLY_SER相比, OFF_HEAP减少了垃圾回收。允许执行体更小通过共享一个内存池。因此对于拥有较大堆内存和高并发的环境有较大的吸引力。更重要的是，因为RDD存储在Tachyon上，执行体的崩溃不会造成缓存的丢失。在这种模式下.Tachyon中的内存是可丢弃的，这样 Tachyon 对于从内存中挤出的块不会试图重建它。如果你打算使用Tachyon作为堆缓存，Spark提供了与Tachyon相兼容的版本。首先我们导入相关函数：1from pyspark.storagelevel import StorageLevel在scala中可以直接使用上述的持久化等级关键词，但是在pyspark中封装为了一个类，StorageLevel类，并在初始化时指定一些参数，通过不同的参数组合，可以实现上面的不同存储等级。StorageLevel类的初始化函数如下：123456def __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1): self.useDisk = useDisk self.useMemory = useMemory self.useOffHeap = useOffHeap self.deserialized = deserialized self.replication = replication那么不同的存储等级对应的参数为:1234567891011121314151617181920StorageLevel.DISK_ONLY = StorageLevel(True, False, False, False)StorageLevel.DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)StorageLevel.MEMORY_ONLY = StorageLevel(False, True, False, False)StorageLevel.MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)StorageLevel.MEMORY_AND_DISK = StorageLevel(True, True, False, False)StorageLevel.MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)StorageLevel.OFF_HEAP = StorageLevel(True, True, True, False, 1)&quot;&quot;&quot;.. note:: The following four storage level constants are deprecated in 2.0, since the records \\will always be serialized in Python.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER = StorageLevel.MEMORY_ONLY&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER_2 = StorageLevel.MEMORY_ONLY_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY_2`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER = StorageLevel.MEMORY_AND_DISK&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER_2 = StorageLevel.MEMORY_AND_DISK_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK_2`` instead.&quot;&quot;&quot; 取消持久化使用unpersist函数对RDD进行持久化：1kvRDD1.unpersist() 综上所述:哇，有关pyspark的RDD的基本操作就是上面这些啦，想要了解更多的盆友们可以参照官网给出的官方文档今天主要介绍了两种RDD，基本的RDD和Key-Value形式的RDD，介绍了他们的几种“转换”运算和“动作”运算，整理如下：RDD运算说明基本RDD“转换”运算map（对各数据进行转换），filter（过滤符合条件的数据），distinct（去重运算），randomSplit（根据指定的比例随机分为N各RDD），groupBy（根据条件对数据进行分组），union（两个RDD取并集），intersection（两个RDD取交集），subtract（两个RDD取差集）。cartesian（两个RDD进行笛卡尔积运算）基本RDD“动作”运算first（取第一条数据），take（取前几条数据），takeOrdered（排序后取前N条数据），统计函数Key-Value形式 RDD“转换”运算filter（过滤符合条件的数据），mapValues（对value值进行转换），sortByKey（根据key值进行排序），reduceByKey（合并相同key值的数据），join（内连接两个KDD），leftOuterJoin（左外连接两个KDD），rightOuterJoin（右外连接两个RDD），subtractByKey（相当于key值得差集运算）Key-Value形式 RDD“动作”运算first（取第一条数据），take（取前几条数据），countByKey（根据key值分组统计），lookup（根据key值查找value值）RDD持久化persist用于对RDD进行持久化，unpersist取消RDD的持久化，注意持久化的存储等级","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"RDD","slug":"RDD","permalink":"http://www.wortyby.com/tags/RDD/"}]},{"title":"tmpwatch 工具使用说明","slug":"tmpwatch 工具使用说明","date":"2018-08-28T11:47:00.266Z","updated":"2018-08-31T09:14:56.837Z","comments":true,"path":"2018/08/28/tmpwatch 工具使用说明/","link":"","permalink":"http://www.wortyby.com/2018/08/28/tmpwatch 工具使用说明/","excerpt":"","text":"背景tmp 目录原来只有在启动的时候才会被清理…… 不同的 Linux 发行版其实对 /tmp 目录的清理方式有所不同：在 Debian-like 的系统，启动的时候才会清理 (规则定义在 /etc/default/rcS)在 RedHat-like 的系统，按文件存在时间定时清理 (RHEL6 规则定义在 /etc/cron.daily/tmpwatch; RHEL7 以及 RedHat-like with systemd 规则定义在 /usr/lib/tmpfiles.d/tmp.conf, 通过 systemd-tmpfiles-clean.service 服务调用)在 CentOS 里，也是按文件存在时间清理的 (通过 crontab 的配置 /etc/cron.daily 定时执行 tmpwatch 来实现)在 Gentoo 里也是启动清理，规则定义在 /etc/conf.d/bootmisc，大 Gentoo 就是不走寻常路对于那些只能开机清理临时文件的发行版，如果作为服务器，这种不重启就对临时文件目录的垃圾不问不管的做事风格实在是很不靠谱。不过从上面其他发行版大家估计也会发现，解决此问题的关键就在于 tmpwatch 和定时任务的配合使用。 服务器除了调用用户的计划任务外，还会执行系统自己的，比如：12345/etc/cron.hourly/etc/cron.daily/etc/cron.daily目的能够自动删除不经常使用的临时文件为了保证tmp目录不爆满，系统默认情况下每日会处理一次tmp目录文件。安装1yum install tmpwatch -y功能说明删除暂存文件(默认是240小时，10天)tmpwatch 是专门用于解决“删除 xxx 天没有被访问/修改过的文件”这样需求的命令。使用方式也极其简单：除了删除tmp 文件夹之外也可以删除任何其他指定文件夹语法tmpwatch [-afqv][–test][超期时间][目录…]执行tmpwatch指令可删除不必要的暂存文件，您可以设置文件超期时间，单位以小时计算。 tmpwatch 参数说明123456789101112131415-u, --atime 基于访问时间来删除文件，默认的。-m, --mtime 基于修改时间来删除文件。-c, --ctime 基于创建时间来删除文件，对于目录，基于mtime。-M, --dirmtime 删除目录基于目录的修改时间而不是访问时间。-a, --all 删除所有的文件类型，不只是普通文件，符号链接和目录。-d, --nodirs 不尝试删除目录，即使是空目录。-d, --nosymlinks 不尝试删除符号链接。-f, --force 强制删除。-q, --quiet 只报告错误信息。-s, --fuser 如果文件已经是打开状态在删除前，尝试使用“定影”命令。默认不启用。-t, --test 仅作测试，并不真的删除文件或目录。-U, --exclude-user=user 不删除属于谁的文件。-v, --verbose 打印详细信息。-x, --exclude=path 排除路径，如果路径是一个目录，它包含的所有文件被排除了。如果路径不存在，它必须是一个绝对路径不包含符号链接。-X, --exclude-pattern=pattern 排除某规则下的路径。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"清理","slug":"清理","permalink":"http://www.wortyby.com/tags/清理/"}]}]}