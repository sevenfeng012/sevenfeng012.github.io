{"meta":{"title":"值得荐","subtitle":"值得推荐的原创文章，技术文章或是科普文章","description":"同乐科技","author":"1One's Dad","url":"http://www.wortyby.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-08-29T02:08:14.000Z","updated":"2018-08-29T05:24:24.709Z","comments":false,"path":"/404.html","permalink":"http://www.wortyby.com//404.html","excerpt":"","text":""},{"title":"datastructure","date":"2018-08-31T09:09:28.000Z","updated":"2018-08-31T09:12:41.723Z","comments":true,"path":"datastructure/index.html","permalink":"http://www.wortyby.com/datastructure/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-31T09:06:48.000Z","updated":"2018-08-31T09:12:59.268Z","comments":true,"path":"categories/index.html","permalink":"http://www.wortyby.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-08-31T09:08:27.000Z","updated":"2018-08-31T09:13:13.095Z","comments":true,"path":"tags/index.html","permalink":"http://www.wortyby.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"深入浅出ML之Factorization家族","slug":"深入浅出ML之Factorization家族","date":"2018-09-21T07:01:51.183Z","updated":"2018-09-21T09:51:49.570Z","comments":true,"path":"2018/09/21/深入浅出ML之Factorization家族/","link":"","permalink":"http://www.wortyby.com/2018/09/21/深入浅出ML之Factorization家族/","excerpt":"","text":"内容列表 写在前面 因子分解机因子分解机（Factorization Machine，简称FM），又称分解机器。是由Konstanz大学（德国康斯坦茨大学）Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？用户在网站上的行为数据会被Server端以日志的形式记录下来，这些数据通常会存放在多台存储机器的硬盘上。以我浪为例，各产品线纪录的用户行为日志会通过flume等日志收集工具交给数据中心托管，它们负责把数据定时上传至HDFS上，或者由数据中心生成Hive表。我们会发现日志中大多数出现的特征是categorical类型的，这种特征类型的取值仅仅是一个标识，本身并没有实际意义，更不能用其取值比较大小。比如日志中记录了用户访问的频道（channel）信息，如”news”, “auto”, “finance”等。假设channel特征有10个取值，分别为{“auto”,“finance”,“ent”,“news”,“sports”,“mil”,“weather”,“house”,“edu”,“games”}。部分训练数据如下：userchanneluser1sportsuser2newsuser3financeuser4houseuser5eduuser6news……特征 ETL 过程中，需要对categorical型特征进行one-hot编码（独热编码），即将categorical型特征转化为数值型特征。channel特征转化后的结果如下：userchn-autochn-financechn-entchn-newschn-sportschn-milchn-weatherchn-housechn-educhn-gamesuser10000100000user20001000000user30100000000user40000000100user50000000010user60001000000可以发现，由 one-hot编码 带来的数据稀疏性会导致特征空间变大。上面的例子中，一维categorical特征在经过one-hot编码后变成了10维数值型特征。真实应用场景中，未编码前特征总维度可能仅有数十维或者到数百维的categorical型特征，经过one-hot编码后，达到数千万、数亿甚至更高维度的数值特征在业内都是常有的。我组广告和推荐业务的点击预估系统，编码前是特征不到100维，编码后（包括feature hashing）的维度达百万维量级。此外也能发现，特征空间增长的维度取决于categorical型特征的取值个数。在数据稀疏性的现实情况下，我们如何去利用这些特征来提升learning performance？ 特征关联以及表征形式或许在学习过程中考虑特征之间的关联信息。针对特征关联，我们需要讨论两个问题：1. 为什么要考虑特征之间的关联信息？2. 如何表达特征之间的关联？ 为什么要考虑特征之间的关联信息？大量的研究和实际数据分析结果表明：某些特征之间的关联信息（相关度）对事件结果的的发生会产生很大的影响。从实际业务线的广告点击数据分析来看，也正式了这样的结论。 如何表达特征之间的关联？表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征：人工特征工程（数据分析＋人工构造）；通过模型做组合特征的学习（深度学习方法、FM/FFM方法）本章主要讨论FM和FFM用来学习特征之间的关联。我们在《第01章：深入浅出ML之Regression家族》看到的多项式回归模型，其中的交叉因子项xixj就是组合特征最直观的例子。xixj表示特征xi和xj的组合，当xi和xj都非零时，组合特征xixj才有意义。这里我们以二阶多项式模型（degree=2时）为例，来分析和探讨FM原理和参数学习过程。 FM模型表达为了更好的介绍FM模型，我们先从多项式回归、交叉组合特征说起，然后自然地过度到FM模型。二阶多项式回归模型我们先看二阶多项式模型的表达式：y&#x005E;(x):=w0+&#x2211;i=1nwixi&#x23DF;&#x7EBF;&#x6027;&#x56DE;&#x5F52;+&#x2211;i=1n&#x2211;j=i+1nwijxixj&#x23DF;&#x4EA4;&#x53C9;&#x9879;&#xFF08;&#x7EC4;&#x5408;&#x7279;&#x5F81;&#xFF09;(n.ml.1.9.1)其中，n表示样本特征维度，截距 w0∈R,w＝{w1,w2,⋯,wn}∈Rn,wij∈Rn×n 为模型参数。从公式(n.ml.1.9.1)可知，交叉项中的组合特征参数总共有n(n−1)2个。在这里，任意两个交叉项参数wij都是独立的。然而，在数据非常稀疏的实际应用场景中，交叉项参数的学习是很困难的。why？因为我们知道，回归模型的参数w的学习结果就是从训练样本中计算充分统计量（凡是符合指数族分布的模型都具有此性质），而在这里交叉项的每一个参数wij的学习过程需要大量的xi、xj同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“xi和xj都非零”的样本数就会更少。训练样本不充分，学到的参数wij就不是充分统计量结果，导致参数wij不准确，而这会严重影响模型预测的效果（performance）和稳定性。How to do it ?那么，如何在降低数据稀疏问题给模型性能带来的重大影响的同时，有效地解决二阶交叉项参数的学习问题呢？矩阵分解方法已经给出了解决思路。这里借用CMU讨论课中提到的FM课件和美团－深入FFM原理与实践中提到的矩阵分解例子（美团技术团队的分享很赞👍）。在基于Model-Based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。如下图所示。上图把每一个user表示成了一个二维向量，同时也把item表示成一个二维向量，两个向量的内积就是矩阵中user对item的打分。根据矩阵分解的启发，如果把多项式模型中二阶交叉项参数wij组成一个对称矩阵W（对角元素设为正实数），那么这个矩阵就可以分解为W=VVT，V∈Rn×k称为系数矩阵，其中第i行对应着第i维特征的隐向量 (这部分在FM公式解读中详细介绍)。将每个交叉项参数wij用隐向量的内积⟨vi,vj⟩表示，是FM模型的核心思想。下面对FM模型表达式和参数求解过程，给出详细解读。 FM模型表达这里我们只讨论二阶FM模型（degree＝2），其表达式为：y&#x005E;(x):=w0+&#x2211;i=1nwixi+&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj(ml.1.9.1)其中，vi表示第i特征的隐向量，⟨⋅,⋅⟩表示两个长度为k的向量的内积，计算公式为：&#x27E8;vi,vj&#x27E9;:=&#x2211;f=1kvi,f&#x22C5;vj,f(ml.1.9.2) 公式解读： 线性模型＋交叉项直观地看FM模型表达式，前两项是线性回归模型的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将两个互异的特征分量之间的关联信息考虑进来。用交叉项表示组合特征，从而建立特征与结果之间的非线性关系。 交叉项系数 → 隐向量内积由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数wij（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积⟨vi,vj⟩表示wij。具体的，FM求解过程中的做法是：对每一个特征分量xi引入隐向量vi＝(vi,1,vi,2,⋯,vi,k)，利用vivTj内积结果对交叉项的系数wij进行估计，公式表示：w&#x005E;ij:=vivjT隐向量的长度k称为超参数(k∈N+,k≪n)，vi=(vi,1,vi,2,⋯,vi,k)的含义是用k个描述特征的因子来表示第i维特征。根据公式(ml.1.9.1)，二阶交叉项的参数由n⋅n个减少到n⋅k个，远少于二阶多项式模型中的参数数量。此外，参数因子化表示后，使得xhxi的参数与xixj的参数不再相互独立。这样我们就可以在样本稀疏情况下相对合理的估计FM模型交叉项的参数。具体地：&#x27E8;vh,vi&#x27E9;:=&#x2211;f=1kvh,f&#x22C5;vi,f(1)&#x27E8;vi,vj&#x27E9;:=&#x2211;f=1kvi,f&#x22C5;vj,f(2)(n.ml.1.9.2)xhxi 与xixj的系数分别为⟨vh,vi⟩和⟨vi,vj⟩，他们之间有共同项vi。也就是说，所有包含xi的非零组合特征（存在某个j≠i，使得xixj≠0）的样本都可以用来学习隐向量vi，这在很大程度上避免了数据稀疏行造成参数估计不准确的影响。在二阶多项式模型中，参数whi和wij的学习过程是相互独立的。论文中还提到FM模型的应用场景，并且说公式(ml.1.9.1)作为一个通用的拟合模型（Generic Model），可以采用不同的损失函数来解决具体问题。比如：FM应用场景损失函数说明回归均方误差（MSE）损失Mean Square Error，与平方误差类似二类分类Hinge/Cross-Entopy损失分类时，结果需要做sigmoid变换排序. FM参数学习 等式变换公式(ml.1.9.1)中直观地看，FM模型的复杂度为O(kn2)，但是通过下面的等价转换，可以将FM的二次项化简，其复杂度可优化到O(kn)。即：&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj=12&#x2211;f=1k&#x27EE;(&#x2211;i=1nvi,fxi)2&#x2212;&#x2211;i=1nvi,f2xi2&#x27EF;(ml.1.9.3)下面给出详细推导：&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj(1)=12&#x2211;i=1n&#x2211;j=1n&#x27E8;vi,vj&#x27E9;xixj&#x2212;12&#x2211;i=1n&#x27E8;vi,vi&#x27E9;xixi(2)=12(&#x2211;i=1n&#x2211;j=1n&#x2211;f=1kvi,fvj,fxixj&#x2212;&#x2211;i=1n&#x2211;f=1kvi,fvi,fxixi)(3)=12&#x2211;f=1k&#x27EE;(&#x2211;i=1nvi,fxi)&#x22C5;(&#x2211;j=1nvj,fxj)&#x2212;&#x2211;i=1nvi,f2xi2&#x27EF;(4)=12&#x2211;f=1k&#x27EE;(&#x2211;i=1nvi,fxi)2&#x2212;&#x2211;i=1nvi,f2xi2&#x27EF;(5)(n.ml.1.9.3) 解读第（1）步到第（2）步，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角与上三角相等，有下式成立： A=12(2A+B)&#x2212;12B.A=&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj&#x005F;;B=12&#x2211;i=1n&#x27E8;vi,vi&#x27E9;xixi&#x005F;(n.ml.1.9.4)如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：&#x2202;&#x2202;&#x03B8;y(x)={1,if&#x03B8;isw0(&#x5E38;&#x6570;&#x9879;)xiif&#x03B8;iswi(&#x7EBF;&#x6027;&#x9879;)xi&#x2211;j=1nvj,fxj&#x2212;vi,fxi2,if&#x03B8;isvi,f(&#x4EA4;&#x53C9;&#x9879;)(ml.1.9.4) 其中， vj,f是隐向量vj的第f个元素。 梯度法训练FM给出伪代码 FM训练复杂度由于∑nj=1vj,fxj只与f有关，在参数迭代过程中，只需要计算第一次所有f的∑nj=1vj,fxj，就能够方便地得到所有vi,f的梯度。显然，计算所有f的∑nj=1vj,fxj的复杂度是O(kn)；已知∑nj=1vj,fxj时，计算每个参数梯度的复杂度是O(n)；得到梯度后，更新每个参数的复杂度是 O(1)；模型参数一共有nk+n+1个。因此，FM参数训练的时间复杂度为O(kn)。综上可知，FM算法可以在线性时间内完成模型训练，以及对新样本做出预测，所以说FM是一个非常高效的模型。 FM总结上面我们主要是从FM模型引入（多项式开始）、模型表达和参数学习的角度介绍的FM模型，这里我把我认为FM最核心的精髓和价值总结出来，与大家讨论。FM模型的核心作用可以概括为以下3个： 1. FM降低了交叉项参数学习不充分的影响one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数wij用对应特征隐向量的内积表示，即⟨vi,vj⟩（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数wij的过程，转变为学习n个单特征对应k维隐向量的过程。很明显，单特征参数（k维隐向量vi）的学习要比交叉项参数wij学习得更充分。示例说明：假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下：共现交叉特征样本数注&lt;女性，汽车&gt;500同时出现&lt;女性，汽车&gt;的样本数&lt;女性，化妆品&gt;1000同时出现&lt;女性，化妆品&gt;的样本数&lt;男性，汽车&gt;1500同时出现&lt;男性，汽车&gt;的样本数&lt;男性，化妆品&gt;0样本中无此特征组合项&lt;女性，汽车&gt; 的含义是 女性看汽车广告。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响。 2. FM提升了模型预估能力依然看上面的示例，样本中没有*&lt;男性，化妆品&gt;交叉特征，即没有男性看化妆品广告的数据。如果用多项式模型来建模，对应的交叉项参数w男性,化妆品是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告*场景给出准确地预估。FM模型是否能得到交叉项参数w男性,化妆品呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为w男性,化妆品=⟨v男性,v化妆品⟩。用男性特征隐向量v男性和化妆品特征隐向量v化妆品的内积表示交叉项参数w男性,化妆品。由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用⟨v男性,v化妆品⟩得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估能力。 3. FM提升了参数学习效率这个显而易见，参数个数由(n2+n+1)变为(nk+n+1)个，模型训练复杂度也由O(mn2)变为O(mnk)。m为训练样本数。对于训练样本和特征数而言，都是线性复杂度。此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。 总结最后一句话总结，FM最大特点和优势：FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力。 场感知分解机场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自于Yu-Chin Juan与其比赛队员，它们借鉴了辣子Michael Jahrer的论文中field概念，提出了FM的升级版模型。通过引入field的概念，FFM吧相同性质的特征归于同一个field。在FM开头one-hot编码中提到用于访问的channel，编码生成了10个数值型特征，这10个特征都是用于说明用户PV时对应的channel类别，因此可以将其放在同一个field中。那么，我们可以把同一个categorical特征经过one-hot编码生成的数值型特征都可以放在同一个field中。同一个categorical特征可以包括用户属性信息（年龄、性别、职业、收入、地域等），用户行为信息（兴趣、偏好、时间等），上下文信息（位置、内容等）以及其它信息（天气、交通等）。在FFM中，每一维特征xi，针对其它特征的每一种”field” fj，都会学习一个隐向量vi,fj。因此，隐向量不仅与特征相关，也与field相关。假设每条样本的n个特征属于f个field，那么FFM的二次项有nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。因此可以吧FM看作是FFM的特例，即把所有的特征都归属到一个field是的FFM模型。根据FFM的field敏感特性，可以导出其模型表达式：\\[ŷ (x):=w0+∑i=1nwixi+∑i=1n∑j=i+1n⟨vi,fj,vj,fi⟩xixj(ml.1.9.5)\\]其中，fj是第j个特征所属的field。如果隐向量的长度为k，那么FFM的二交叉项参数就有nfk个，远多于FM模型的nk个。此外，由于隐向量与field相关，FFM的交叉项并不能够像FM那样做化简，其预测复杂度为O(kn2)。这里以NTU_FFM.pdf和美团－深入FFM原理与实践都提到的例子，给出FFM－Fields特征组合的工作过程。给出一下输入数据：UserMovieGenrePriceYuChin3IdiotsComedy, Drama$9.99Price是数值型特征，实际应用中通常会把价格划分为若干个区间（即连续特征离散化），然后再one-hot编码，这里假设$9.99对应的离散化区间tag为”2”。当然不是所有的连续型特征都要做离散化，比如某广告位、某类广告／商品、抑或某类人群统计的历史CTR（pseudo－CTR）通常无需做离散化。该条记录可以编码为5个数值特征，即User^YuChin, Movie^3Idiots, Genre^Comedy, Genre^Drama, Price2*。其中*GenreComedy, Genre^Drama属于同一个field。为了说明FFM的样本格式，我们把所有的特征和对应的field映射成整数编号。Field NameField IndexFeature NameFeature IndexUser1User^YuChin1Movie2Movie^3Idiots2Genre3Genre^Comedy3－－Genre^Drama4Price4Price^25那么，FFM所有的（二阶）组合特征共有10项（C25=5×42!=10），即为：其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有C2n=n(n−1)2个。 FFM应用场景在我们的广告业务系统、商业推荐以及自媒体－推荐系统中，FFM模型作为点击预估系统中的核心算法之一，用于预估广告、商品、文章的点击率（CTR）和转化率（CVR）。在鄙司广告算法团队，点击预估系统已成为基础设施，支持并服务于不同的业务线和应用场景。预估模型都是离线训练，然后定时更新到线上实时计算，因此预估问题最大的差异就体现在数据场景和特征工程。以广告的点击率为例，特征主要分为如下几类：用户属性与行为特征：广告特征：上下文环境特征：为了使用开源的FFM模型，所以的特征必须转化为field_id:feat_id:value格式，其中field_id表示特征所属field的编号，feat_id表示特征编号，value为特征取值。数值型的特征如果无需离散化，只需分配单独的field编号即可，如历史pseudo-ctr。categorical特征需要经过one-hot编码转化为数值型，编码产生的所有特征同属于一个field，特征value只能是0/1, 如用户年龄区间、性别、兴趣、人群等。开源工具FFM使用时，注意事项（参考新浪广告算法组的实战经验和美团－深入FFM原理与实践）: 样本归一化： 特征归一化： 省略0值特征：回归、分类、排序等。推荐算法，预估模型（如CTR预估等） 参考资料Sina广告点击预估系统实践FM、FFM相关Paper、技术博客美团技术团队原文","categories":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/categories/推荐/"}],"tags":[{"name":"FM","slug":"FM","permalink":"http://www.wortyby.com/tags/FM/"},{"name":"算法","slug":"算法","permalink":"http://www.wortyby.com/tags/算法/"}]},{"title":"FM(因式分解机推荐算法原理)","slug":"FM(因式分解机推荐算法原理）","date":"2018-09-21T06:47:26.049Z","updated":"2018-09-26T02:59:06.691Z","comments":true,"path":"2018/09/21/FM(因式分解机推荐算法原理）/","link":"","permalink":"http://www.wortyby.com/2018/09/21/FM(因式分解机推荐算法原理）/","excerpt":"","text":"背景对于分解机(Factorization Machines，FM)推荐算法原理，本来想自己单独写一篇的。但是看到peghoty写的FM不光简单易懂，而且排版也非常好，因此转载过来，自己就不再单独写FM了。 原理 目录 预测任务 模型方程 回归分类 学习算法原文","categories":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/categories/推荐/"}],"tags":[{"name":"FM","slug":"FM","permalink":"http://www.wortyby.com/tags/FM/"}]},{"title":"腾讯推荐—让您的系统瞬间个性化","slug":"腾讯推荐—让您的系统瞬间个性化","date":"2018-09-21T06:16:13.516Z","updated":"2018-09-26T06:39:48.508Z","comments":true,"path":"2018/09/21/腾讯推荐—让您的系统瞬间个性化/","link":"","permalink":"http://www.wortyby.com/2018/09/21/腾讯推荐—让您的系统瞬间个性化/","excerpt":"","text":"腾讯推荐“腾讯推荐”是腾讯大数据近期大力打造的开放服务平台，旨在集业务接入、数据上报、算法计算、实时推荐和效果监控于一体，对外提供全自动实时精准推荐服务。 腾讯推荐官网: tuijian.qq.com 推荐是什么？文章中提到的推荐均是指在海量的物品中自动为用户选取到感兴趣或合适的信息。就腾讯新闻应用而言，全国各地每天产生的信息总数堪称海量，但屏幕尺寸有限，能给用户展示的新闻也不多，这就涉及到帮助用户在新闻海洋中万里挑一，找到吻合用户兴趣的信息。类似的场景不胜枚举，淘宝天猫，腾讯视频，图片社区，应用宝…互联网下，信息过载已然不可逆转，推荐系统的精确性也因此举足轻重。 腾讯推荐的目标腾讯推荐平台的目标是帮助第三方应用实现个性化推荐功能。我们着力于：将系统资源云服务化，帮助应用节省建设个性化服务的开销。以推荐方式开放用户画像，帮助应用增强个性化服务能力。实现数据接入，算法计算和实时推荐全流程自动化，降低应用构建实时推荐系统的门槛。具体而言，应用开发者仅需上报相应的物品信息和用户行为，平台便会自动结合腾讯用户画像和应用本身的数据对其用户进行个性化推荐。以腾讯新闻应用为例，应用开发者将每天发生的新闻信息以及用户行为包括点击、浏览、评论等数据实时上报到腾讯推荐平台。平台会自动基于腾讯用户画像和行为数据计算出用户与新闻之间的关系。每当用户打开腾讯新闻时，应用便会向推荐平台实时请求当前用户的个性化推荐结果，从而将相关信息展示在用户眼前。目前，腾讯推荐平台支持着公司内20多项核心业务，实践证明个性化推荐服务帮助业务无论在用户体验，产品粘性，还是点击率，转化率等方面均有显著提升，因此是时候将这套实时个性化推荐平台对外开放，以求生态稳定，共生共赢。总的说来,腾讯推荐作为一种云服务，降低了第三方应用构建实时推荐系统的门槛，节省了第三方应用建设个性化服务的系统资源，更将腾讯用户画像以服务的方式对第三方应用开放，增加了应用个性化服务能力。 业务接入按系统指引，用户仅需3步：填写资料、注册业务、添加场景便可完成业务接入。管理员审核通过后，系统会自动为当前业务分配存储和计算资源，同时初始化推荐引擎。在此过程中，系统控制中心会与DSF、TDBank、TDP和TRE各子系统通信，下发当前业务相关配置，所有这一切对用户是透明的。待系统初始化完毕，对用户而言万事俱备，只差数据。 数据上报腾讯推荐基于业务自身上报的ItemInfo（物品信息）和ActionInfo（用户行为），并结合8亿腾讯用户画像提供个性化推荐服务。为方便业务开发者上报ItemInfo和ActionInfo两类数据，目前腾讯推荐提供了C++和Java两种版本的SDK。我们希望业务上报更加完善和准确的行为数据，使得我们提供更加精准推荐服务的同时，也能更好地结合腾讯的用户画像为该产品提供更有价值的属性分析，比如年龄，性别，学历，分布地域，用户偏好，活跃程度等。 用户画像腾讯推荐基于腾讯8亿用户，千亿关系链和众多腾讯系相关产品数据打造了用户覆盖率高且兴趣覆盖度广的统一用户画像。我们基于腾讯众多产品中的行为数据为用户采集丰富的兴趣标签。系统会自动对这些原始标签进行聚类和分类，从而对用户兴趣进行抽象。系统会自动建立标签—&gt;主题（topic）—&gt;类目的映射关系，从而为用户画像进行多粒度、多尺度兴趣刻画。同时系统会根据实时上报的行为，不断更新当前用户的画像兴趣。系统会对上报的物品信息进行文本分析，从而将物品信息抽象并映射到与用户画像相同的主题和类目空间。这样便建立了用户与物品间的泛联系。 推荐策略腾讯推荐平台中，我们提供了5种推荐策略供开发者选用：热度，人群热度，猜你喜欢，猜你喜欢（时间优先）和相关推荐。我们屏蔽策略背后的算法细节与参数，降低用户的使用门槛，同时提供足够的自由度让用户自行选择策略的组合。之所以提供5类推荐策略，除了本身的业务要求外，也建议开发者能基于此在一定程度上解决推荐算法带来的过滤气泡问题，例如将人群热度和猜你喜欢的推荐结果进行穿插后对用户展现。我们会基于上报的点击、曝光行为以及用户画像，采用机器学习方法对当前用户可能点击某物品的概率进行预估。推荐系统内部有多种算法同时ABTest和融合，以求达到推荐效果的最优。综上所述，腾讯推荐平台提供了整套个性化推荐解决方案。过程中，平台将系统资源、用户画像、算法计算以推荐服务的方式对外开放，从而帮助第三方应用节省资源的同时还为其增加了个性化服务能力。平台内部实现了资源初始化、数据处理、算法计算和运维纠错的全流程自动化，因此降低了应用接入门槛，减少了接入时间，增加了服务能力的稳定性。原文","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/tags/推荐/"}]},{"title":"Hive,Hbase,HDFS等之间的关系","slug":"Hive,Hbase,HDFS等之间的关系","date":"2018-09-21T05:17:00.184Z","updated":"2018-09-21T05:17:26.530Z","comments":true,"path":"2018/09/21/Hive,Hbase,HDFS等之间的关系/","link":"","permalink":"http://www.wortyby.com/2018/09/21/Hive,Hbase,HDFS等之间的关系/","excerpt":"","text":"Hive：hive不支持更改数据的操作，Hive基于数据仓库，提供静态数据的动态查询。其使用类SQL语言，底层经过编译转为MapReduce程序，在Hadoop上运行，数据存储在HDFS上。 HDFS:HDFS是GFS的一种实现，他的完整名字是分布式文件系统，类似于FAT32，NTFS，是一种文件格式，是底层的。Hive与Hbase的数据一般都存储在HDFS上。hadoop HDFS为他们提供了高可靠性的底层存储支持。 hbase:Hbase是Hadoop database，即Hadoop数据库。它是一个适合于非结构化数据存储的数据库，HBase基于列的而不是基于行的模式。HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据。Hadoop HDFS为HBase提供了高可靠性的底层存储支持，Hadoop MapReduce为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定服务和failover机制。Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS（关系型数据库）数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。 Pig：Pig的语言层包括一个叫做PigLatin的文本语言,Pig Latin是面向数据流的编程方式。Pig和Hive类似更侧重于数据的查询和分析，底层都是转化成MapReduce程序运行。区别是Hive是类SQL的查询语言，要求数据存储于表中，而Pig是面向数据流的一个程序语言。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://www.wortyby.com/tags/Hive/"},{"name":"HBase","slug":"HBase","permalink":"http://www.wortyby.com/tags/HBase/"},{"name":"HDFS","slug":"HDFS","permalink":"http://www.wortyby.com/tags/HDFS/"}]},{"title":"大数据组件原理总结-Hadoop、Hbase、Kafka、Zookeeper、Spark","slug":"大数据组件原理总结","date":"2018-09-21T05:12:57.410Z","updated":"2018-09-21T05:13:56.757Z","comments":true,"path":"2018/09/21/大数据组件原理总结/","link":"","permalink":"http://www.wortyby.com/2018/09/21/大数据组件原理总结/","excerpt":"","text":"Hadoop原理分为HDFS与Yarn两个部分。HDFS有Namenode和Datanode两个部分。每个节点占用一个电脑。Datanode定时向Namenode发送心跳包，心跳包中包含Datanode的校验等信息，用来监控Datanode。HDFS将数据分为块，默认为64M每个块信息按照配置的参数分别备份在不同的Datanode，而数据块在哪个节点上，这些信息都存储到Namenode上面。Yarn是MapReduce2，可以集成更多的组件，如spark、mpi等。MapReduce包括Jobtraker与Tasktraker两个部分。其中JobTraker是在主节点上，负责整体的调度。Tasktraker在slave节点上，当提交任务后，将其交给Jobtraker进行调度，调度到该任务之后就会将jar包发送到响应的Tasktraker，以实现分布式中移动计算资源而非移动数据。因为这些任务都是并发执行的，所以每个任务需要调用哪个节点的数据必须非常的明确，通常这一部分是通过Jobtraker进行指挥。在MapReduce的开始，每个block作为一个Map输入，Map输入后就进入shuffle阶段。Shuffle阶段主要分为Map和Reduce两个阶段。在Map Shuffle阶段，Map输入按用户的程序进行处理，生成的结果按key排序，然后进入内存，溢出后形成一个spill写入磁盘，这样多个spill在这个节点中进行多路归并排序（胜者树）形成一个排序文件写入磁盘，这期间那些Map文件交给那些节点处理由Jobtraker进行调度，最后生成一个大排序的文件，并且删除spill。之后，再将多个节点上已经排序的文件进行行多路归并排序（一个大文件N分到n个节点，每个节点分为k个spill，一个spill长度s，时间复杂度是N(logn（n个节点多路归并排序） + logk（每个节点内k个spill排序） + logs（每个spill内部进行排序）)，N=nks,所以最后的复杂度还是NlogN）。完成Map Shuffle阶段后通知Jobtraker进入Reduce Shuffle阶段。在这个阶段，因为已经排序，很容易将用户的程序直接作用到相同key的数据上，这些数据作为Reduce的输入进行处理，最终将输出的结果数据写入到HDFS上，并删除磁盘数据。Map一般多，Reduce少，所以通过Hash的方法将Map文件映射到Reduce上，进行处理，这个阶段叫做Patition。为了避免譬如所有数据相加这种操作使得数据负载移动的数量少的Reduce阶段，造成效率低下的结果，我们可以在在Map Shuffle阶段加一个Combine阶段，这个Combine是在每一台节点上将已经排序好的文件进行一次Reduce并将结果作为Reduce Shuffle阶段的输入，这样可以大大减少数据的输入量。通常Reduce的个数通过用户来指定，通常和CPU个数相适应才能使其效率达到最大。 HBase原理Hbase是列存储数据库。其存储的组织结构就是将相同的列族存储在一起，因此得名的。Hbase存储有行键，作为唯一标识，列表示为&lt;列族&gt;:&lt;列&gt;存储信息，如address：city，address：provice，然后是时间戳。Hbase物理模型中，有一个总结点HMaster，通过其自带的zookeeper与客户端相连接。Hbse作为分布式每一个节点作为一个RegionServer，维护Region的状态和管理。Region是数据管理的基本单位。最初只有一个，通过扩充后达到阈值然后分裂，通过Server控制其规模。在RegionServer中，每一个store作为一个列族。当数据插入进来，新数据成为Memstore，写入内存，当Memstore达到阈值后，通过Flashcache进程将数据写入storeFile，也就是当内存数据增多后溢出成一个StoreFile写入磁盘，这里和Hadoop的spill类似，这个过程是在HDFS上进行的操作。所以数据的插入并不是追加的过程，而是积累成大块数据后一并写入。当StoreFile数量过多时，进行合并，将形成一个大的StoreFile并且删除掉原来的StoreFile。再当StoreFile大小超过一定阈值后，分裂成Region。HBase有一个ROOT表和META表。META表记录用户Region的信息，但是随着数据增多，META也会增大，进而分裂成多个Region ，那我们用ROOT表记录下META的信息，是一个二级表，而zookeeper中记录ROOT表的location。当我们需找找到一条信息时，先去zookeeper查找ROOT，从ROOT中查找META找到META位置，在进入META表中寻找该数据所在Region，再读取该Region的信息。HBase适合大量插入又同时读的情况，其瓶颈是硬盘的传输速度而不再是像Oracle一样瓶颈在硬盘的寻道速度。 Zookeeper原理Zookeeper是一个资源管理库，对节点进行协调、通信、失败处理、节点损坏的处理等，是一个无中心设计，主节点通过选举产生。Zookeeper的节点是Znode。每一个节点可以存放1M的数据，client访问服务器时创建一个Znode，可以是短暂的Znode，其上可以放上观察Watcher对node进行监控。Zookeeper有高可用性，每个机器复制一份数据，只要有一般以上的机器可以正常的运行，整个集群就可以工作。比如6台的集群容忍2台断开，超过两台达到一般的数量就不可以，因此集群通常都是奇数来节约资源。Zookeeper使用zab协议，是一个无中心协议，通过选举的方式产生leader，通过每台机器的信息扩散选举最闲的资源利用较少的节点作为主控。同时当配置数据有更改更新时，在每个节点上有配置watcher并触发读取更改，。因此能够保证一致性。每个节点通过leader广播的方式，使所有follower同步。Zookeeper可以实现分布式锁机制。通过watcher监控，对每个Znode的锁都有一个独一的编号，按照序号的大小比较，来分配锁。当一个暂时Znode完结后删除本节点，通知leader完结，之后下一个Znode获取锁进行操作。 Kafka原理Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。它被设计为一个分布式系统，易于向外扩展；它同时为发布和订阅提供高吞吐量；它支持多订阅者，当失败时能自动平衡消费者；它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。broker和生产者、消费者各自都是集群，集群中的各个实例他们之间是对等的，集群扩充节点很方便。Kafka的基本概念包括话题、生产者、消费者、代理或者kafka集群。话题是特定类型的消息流。消息是字节的有效负载，话题是消息的分类名或种子名。生产者是能够发布消息到话题的任何对象。已发布的消息保存在一组服务器中，它们被称为代理或Kafka集群。消费者可以订阅一个或多个话题，并从Broker拉数据，从而消费这些已发布的消息。Kafka的存储布局非常简单。话题的每个分区对应一个逻辑日志。物理上，一个日志为相同大小的一组分段文件。每次生产者发布消息到一个分区，代理就将消息追加到最后一个段文件中。当发布的消息数量达到设定值或者经过一定的时间后，段文件真正写入磁盘中。写入完成后，消息公开给消费者。段文件机制和Hadoop中spill类似。消费者始终从特定分区顺序地获取消息，如果消费者知道特定消息的偏移量，也就说明消费者已经消费了之前的所有消息。消费者向代理发出异步拉请求，准备字节缓冲区用于消费。每个异步拉请求都包含要消费的消息偏移量与其它消息系统不同，Kafka代理是无状态的。这意味着消费者必须维护已消费的状态信息。这些信息由消费者自己维护，代理完全不管。消费者可以故意倒回到老的偏移量再次消费数据。这违反了队列的常见约定，但被证明是许多消费者的基本特征。kafka的broker在配置文件中可以配置最多保存多少小时的数据和分区最大的空间占用，过期的和超量的数据会被broker自动清除掉。Kafka会记录offset到zk，同时又在内存中维护offset，允许快速的checkpoint，如果consumer比partition多是浪费，因为kafka不允许partition上并行consumer读取。同时，consumer比partition少，一个consumer会对应多个partition，有可能导致partition中数据的读取不均匀，也不能保证数据间的顺序性，kafka只有在一个partition读取的时候才能保证时间上是有顺序的。增加partition或者consumer或者broker会导致rebalance，所以rebalance后consumer对应的partition会发生变化。 Spark原理spark 可以很容易和yarn结合，直接调用HDFS、Hbase上面的数据，和hadoop结合。配置很容易。spark发展迅猛，框架比hadoop更加灵活实用。减少了延时处理，提高性能效率实用灵活性。也可以与hadoop切实相互结合。spark核心部分分为RDD。Spark SQL、Spark Streaming、MLlib、GraphX、Spark R等核心组件解决了很多的大数据问题，其完美的框架日受欢迎。其相应的生态环境包括zepplin等可视化方面，正日益壮大。大型公司争相实用spark来代替原有hadoop上相应的功能模块。Spark读写过程不像hadoop溢出写入磁盘，都是基于内存，因此速度很快。另外DAG作业调度系统的宽窄依赖让Spark速度提高。RDD是弹性分布式数据也是spark的核心，完全弹性的，如果数据丢失一部分还可以重建。有自动容错、位置感知调度和可伸缩性，通过数据检查点和记录数据更新金象容错性检查。通过SparkContext.textFile()加载文件变成RDD，然后通过transformation构建新的RDD，通过action将RDD存储到外部系统。RDD使用延迟加载，也就是懒加载，只有当用到的时候才加载数据。如果加载存储所有的中间过程会浪费空间。因此要延迟加载。一旦spark看到整个变换链，他可以计算仅需的结果数据，如果下面的函数不需要数据那么数据也不会再加载。转换RDD是惰性的，只有在动作中才可以使用它们。Spark分为driver和executor，driver提交作业，executor是application早worknode上的进程，运行task，driver对应为sparkcontext。Spark的RDD操作有transformation、action。Transformation对RDD进行依赖包装，RDD所对应的依赖都进行DAG的构建并保存，在worknode挂掉之后除了通过备份恢复还可以通过元数据对其保存的依赖再计算一次得到。当作业提交也就是调用runJob时，spark会根据RDD构建DAG图，提交给DAGScheduler，这个DAGScheduler是在SparkContext创建时一同初始化的，他会对作业进行调度处理。当依赖图构建好以后，从action开始进行解析，每一个操作作为一个task，每遇到shuffle就切割成为一个taskSet，并把数据输出到磁盘，如果不是shuffle数据还在内存中存储。就这样再往前推进，直到没有算子，然后运行从前面开始，如果没有action的算子在这里不会执行，直到遇到action为止才开始运行，这就形成了spark的懒加载，taskset提交给TaskSheduler生成TaskSetManager并且提交给Executor运行，运行结束后反馈给DAGScheduler完成一个taskSet，之后再提交下一个，当TaskSet运行失败时就返回DAGScheduler并重新再次创建。一个job里面可能有多个TaskSet，一个application可能包含多个job。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://www.wortyby.com/tags/HBASE/"},{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"HADOOP","slug":"HADOOP","permalink":"http://www.wortyby.com/tags/HADOOP/"},{"name":"KAFKA","slug":"KAFKA","permalink":"http://www.wortyby.com/tags/KAFKA/"},{"name":"ZOOKEEPER","slug":"ZOOKEEPER","permalink":"http://www.wortyby.com/tags/ZOOKEEPER/"}]},{"title":"数据项目该做的事情","slug":"数据计划","date":"2018-09-20T02:53:29.778Z","updated":"2018-09-26T07:37:53.749Z","comments":true,"path":"2018/09/20/数据计划/","link":"","permalink":"http://www.wortyby.com/2018/09/20/数据计划/","excerpt":"","text":"autherdateversionseven2018-09-20version 1.0 现状 数据同步: 数据同步器应该统一到Hbase ，hive 查询使用 外部表创建 临时表的方式去做 ETL 或其他的hive 运算。将计算结果再存入HBASE 供实时查询，或是写入 redis 给接口调用。实时数据计算，由行为记录统一提前写入HBase数据迁移，统一由任务调度终端，使用sqoop 倒入Hbase，避免使用python 包装代码。 现状是1.所有数据都放Hive;2.数据迁移，采用定时脚本去同步数据库。 数据检索: 数据字典: 数据字典重新构建与否？现状评估是否能够相对准确的计算相似度。 展望 数据预测:依赖特征工程以及数据挖掘的结果进行数据预测 内容画像;没什么概念 用户画像; 用户帖子偏好 用户音频/视频/节目/专辑偏好 用户图片偏好 预测效果展示平台; 特征工程:利用 hive 结合 kylin 和 superset 可以实时构建用户的不同特征展示 特征提取; 数值特征; 类别特征; 时间特征; 空间特征; 文本特征; 常用模型理解各个模型的区别以及使用场景 逻辑回归 因式分解机 梯度提升 问题数学建模 建立评估指标 分类指标 回归指标 排序指标 样本选择 原数据与训练数据选择 交叉验证 构建pipeline 执行 数据分组，按照指标，进行交叉验证数据预测的概率大小 以及确认是否过拟合？ 数据挖掘 用户画像 用户标识域外平台(微信，微博，知乎…)平台内(设备和UID) 用户特征数据收听发帖评论点赞年龄(平台内外用户，看各个平台用户群分布)收听时间点(平台内外用户，看各个平台用户时间分配) 样本数据随机抽取其他平台的用户特征来匹配收听我们平台内容的可能性大小，相应的去其他平台拉用户做推广？随机的从我们的平台用户特征去匹配其他平台内容，看合作机会做推广？ 标签建模可以采用kylin 与 superset 做实时的标签构建与BI 报表展示,来让运营人员进行对应的运营策略制定。 用户画像实时查询系统方便观测平台内用户状态与热点内容追踪 人群画像分析系统方便观测用户群体分布以及意见领袖产生影响力的领域。 评论挖掘 评论标签提取 文字评论的情感分析 音频评论的情感分析 图片评论的情感分析 标签的实时查询系统 音频挖掘 音频标签提取 音频的音文转写 音频的情感分析 音频挖掘信息的实时查询系统 帖子挖掘 帖子标签提取 帖子的情感分析 帖子的实时查询系统实时跟踪平台内帖子的情绪，防止负面内容传播，在平台内可以先遏制掉负面内容传播的可能性，设定一个警戒值 搜索与推荐根据用户的搜索更新用户的偏好进而去推荐内容; 搜索 构建数据的图关系得到类目的权重以及重要程度 构建图边关系节点向量利用用户的搜索次，生成用户可能感兴趣的推荐搜索词 类目相关性实时查询系统实时的显示平台内用户的搜索；平台也可以利用人群属性和用户属性，动态的推荐用户搜索关键词，引导用户去查看平台希望他看的内容。或是他可能想看的内容 推荐 以域外热点数据来推平台内数据知乎微博。。。 平台内的特征数据推荐根据协同过滤算法推荐用户可能要看的可能性内容。 引导用户完成搜索 用户引导的产品定义与衡量标准 搜索前的引导推荐 查询词 搜索中的引导查询词 补全 搜索后的引导相关性搜索 搜索的实时反馈平台展示平台实时搜索平台外的热门搜索与平台的热门搜索比较，确定平台的定位差异还是归一到平台外的搜索。 数据召回数据消费反馈回炉进行更新模型与特征数据召回有以下几种方式 基于协同过滤的召回 基于位置的召回 基于搜索查询的召回 基于用户关系图的召回 基于用户实时行为的召回 A/B Test 通过abtest 方式来进行效果的测试与比较 数据推送 app端 微信小程序端 举例开车的时候，推荐收听的节目应该以交通主题相关的音频节目相关；休息的时候，应该推荐实时性高的热点帖子内容。睡觉的时候，应该推荐轻松的故事广播为主等等。。。。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"架构项目","slug":"架构项目","permalink":"http://www.wortyby.com/tags/架构项目/"}]},{"title":"Happybase 进行 过滤查询","slug":"Happybase 进行 过滤查询","date":"2018-09-18T07:46:26.219Z","updated":"2018-09-18T08:19:47.399Z","comments":true,"path":"2018/09/18/Happybase 进行 过滤查询/","link":"","permalink":"http://www.wortyby.com/2018/09/18/Happybase 进行 过滤查询/","excerpt":"","text":"背景HBASE 中的数据 ，当利用 scan 进行数据查询的时候，如果不给过滤条件，他会将所有数据都输出。如果我们仅仅是想得到某些列的数据，那么我们需要祭出filter 神器。或是 columns 来指定我们需要哪些列的数据。下面我们上代码来看看 filter 和 column 来进行 筛选数据的用法。 初始化代码123import happybaseconnection = happybase.Connection(&apos;localhost&apos;, autoconnect=False)table = connection.table(&apos;live_gift&apos;) columns 来指定列名称通过columns 来确定 当前需要哪些列的数据，一般会加上一个 limit 参数不废话，先上代码看看它为何方神圣。1data = table.scan(columns=[b&apos;g:user_id&apos;, b&apos;g:gift_price&apos;], limit=20)得到的结果就是以user_id，gift_price 2列的数据一般情况下，这种方式都是简单的查看数据，但更实际的场景可能需要综合各种条件进行查询，那么就需要祭出 filter 神器了 filter 来过滤符合条件的数据进行输出通过filter 来确定当前需要符合这些条件的数据进行输出。filter 是以字符串形式存在，如果是多个条件查询，就综合拼接成一个字符串 单一条件查询拿出开始时间大于 这个时间的数据1data = table.scan(columns=[b&apos;s:live_id&apos;, b&apos;s:start_time&apos;], filter=&quot;SingleColumnValueFilter(&apos;s&apos;, &apos;start_time&apos;, &gt;=, &apos;binary:1490976000&apos;)&quot;) 多个条件查询拿出 live_id = 15909的数据，gift_price=20 的数据1data = table.scan(columns=[b&apos;g:user_id&apos;, b&apos;g:gift_price&apos;], filter=&quot;SingleColumnValueFilter(&apos;g&apos;, &apos;live_id&apos;, =, &apos;binary:15909&apos;)&quot; and &quot;SingleColumnValueFilter(&apos;g&apos;, &apos;gift_price&apos;, =, &apos;binary:20&apos;)&quot;, limit=20)happyhbase官方文档","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://www.wortyby.com/tags/HBASE/"}]},{"title":"python redis之连接池的原理","slug":"python redis之连接池的原理","date":"2018-09-14T05:58:58.211Z","updated":"2018-09-14T06:14:30.832Z","comments":true,"path":"2018/09/14/python redis之连接池的原理/","link":"","permalink":"http://www.wortyby.com/2018/09/14/python redis之连接池的原理/","excerpt":"","text":"我们使用redis连接池, 却不怎么了解这个连接池的原理, 今天我们来研究一下 什么是连接池通常情况下, 当我们需要做redis操作时, 会创建一个连接, 并基于这个连接进行redis操作, 操作完成后, 释放连接,一般情况下, 这是没问题的, 但当并发量比较高的时候, 频繁的连接创建和释放对性能会有较高的影响于是, 连接池就发挥作用了连接池的原理是, 通过预先创建多个连接, 当进行redis操作时, 直接获取已经创建的连接进行操作, 而且操作完成后, 不会释放, 用于后续的其他redis操作这样就达到了避免频繁的redis连接创建和释放的目的, 从而提高性能了 原理那么, 在redis-py中, 他是怎么进行连接池管理的呢连接池使用首先看下如何进行连接池操作的1234rdp = redis.ConnectionPool(host=&apos;127.0.0.1&apos;, port=6379, password=&apos;xxxxx&apos;)rdc = redis.StrictRedis(connection_pool=rdp)rdc.set(&apos;name&apos;, &apos;Yi_Zhi_Yu&apos;)rdc.get(&apos;name&apos;) 原理解析当redis.ConnectionPool 实例化的时候, 做了什么123456789def __init__(self, connection_class=Connection, max_connections=None, **connection_kwargs): max_connections = max_connections or 2 ** 31 if not isinstance(max_connections, (int, long)) or max_connections &lt; 0: raise ValueError(&apos;&quot;max_connections&quot; must be a positive integer&apos;) self.connection_class = connection_class self.connection_kwargs = connection_kwargs self.max_connections = max_connections这个连接池的实例化其实未做任何真实的redis连接, 仅仅是设置最大连接数, 连接参数和连接类StrictRedis 实例化的时候, 又做了什么12345def __init__(self, ...connection_pool=None...): if not connection_pool: ... connection_pool = ConnectionPool(**kwargs) self.connection_pool = connection_pool以上仅保留了关键部分代码可以看出, 使用StrictRedis 即使不创建连接池, 他也会自己创建到这里, 我们还没有看到什么redis连接真实发生继续下一步就是set 操作了, 很明显, 这个时候一定会发生redis连接(要不然怎么set)123def set(self, name, value, ex=None, px=None, nx=False, xx=False): ... return self.execute_command(&apos;SET&apos;, *pieces)我们继续看看execute_command12345678910111213141516def execute_command(self, *args, **options): &quot;Execute a command and return a parsed response&quot; pool = self.connection_pool command_name = args[0] connection = pool.get_connection(command_name, **options) try: connection.send_command(*args) return self.parse_response(connection, command_name, **options) except (ConnectionError, TimeoutError) as e: connection.disconnect() if not connection.retry_on_timeout and isinstance(e, TimeoutError): raise connection.send_command(*args) return self.parse_response(connection, command_name, **options) finally: pool.release(connection)终于, 在这我们看到到了连接创建123456789101112connection = pool.get_connection(command_name, **options)这里调用的是ConnectionPool的get_connectiondef get_connection(self, command_name, *keys, **options): &quot;Get a connection from the pool&quot; self._checkpid() try: connection = self._available_connections.pop() except IndexError: connection = self.make_connection() self._in_use_connections.add(connection) return connection如果有可用的连接, 获取可用的链接, 如果没有, 创建一个123456def make_connection(self): &quot;Create a new connection&quot; if self._created_connections &gt;= self.max_connections: raise ConnectionError(&quot;Too many connections&quot;) self._created_connections += 1 return self.connection_class(**self.connection_kwargs)终于, 我们看到了, 在这里创建了连接在ConnectionPool的实例中, 有两个list, 依次是 _available_connections, _in_use_connections,分别表示可用的连接集合和正在使用的连接集合, 在上面的get_connection中, 我们可以看到获取连接的过程是从可用连接集合尝试获取连接,如果获取不到, 重新创建连接将获取到的连接添加到正在使用的连接集合上面是往**_in_use_connections**里添加连接的, 这种连接表示正在使用中, 那是什么时候将正在使用的连接放回到可用连接列表中的呢这个还是在execute_command里, 我们可以看到在执行redis操作时, 在finally部分, 会执行一下1pool.release(connection)连接池对象调用release方法, 将连接从_in_use_connections 放回 _available_connectionsÅ, 这样后续的连接获取就能再次使用这个连接了release 方法如下1234567def release(self, connection): &quot;Releases the connection back to the pool&quot; self._checkpid() if connection.pid != self.pid: return self._in_use_connections.remove(connection) self._available_connections.append(connection) 总结至此, 我们把连接池的管理流程走了一遍, ConnectionPool通过管理可用连接列表(_available_connections) 和 正在使用的连接列表 从而实现连接池管理","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.wortyby.com/tags/redis/"},{"name":"连接池","slug":"连接池","permalink":"http://www.wortyby.com/tags/连接池/"}]},{"title":"HBase中CloumnFamily的设计规则","slug":"HBase中CloumnFamily的设计规则","date":"2018-09-14T05:45:09.944Z","updated":"2018-09-14T05:56:39.958Z","comments":true,"path":"2018/09/14/HBase中CloumnFamily的设计规则/","link":"","permalink":"http://www.wortyby.com/2018/09/14/HBase中CloumnFamily的设计规则/","excerpt":"","text":"HBase本身的设计目标是 支持稀疏表，而 稀疏表通常会有很多列，但是每一行有值的列又比较少。 替代方案如果不使用Column Family的概念，那么有两种设计方案： 方案一把所有列的数据放在一个文件中（也就是传统的按行存储）。那么当我们想要访问少数几个列的数据时，需要遍历每一行，读取整个表的数据，这样子是很低效的。 方案二把每个列的数据单独分开存在一个文件中（按列存储）。那么当我们想要访问少数几个列的数据时，只需要读取对应的文件，不用读取整个表的数据，读取效率很高。然而，由于稀疏表通常会有很多列，这会导致文件数量特别多，这本身会影响文件系统的效率。而Column Family的提出就是为了在上面两种方案中做一个折中。HBase中 将一个Column Family中的列存在一起，而不同Column Family的数据则分开。由于在HBase中Column Family的数量通常很小，同时HBase建议把经常一起访问的比较类似的列放在同一个Column Family中，这样就可以在访问少数几个列时，只读取尽量少的数据。 优化：因为一直在做hbase的应用层面的开发，所以体会的比较深的一点是hbase的表结构设计会对系统的性能以及开销上造成很大的区别，本篇文章先按照hbase表中的rowkey、columnfamily、column、timestamp几个方面进行一些分析。最后结合分析如何设计一种适合应用的高效表结构。 表的属性 最大版本数：通常是3，如果对于更新比较频繁的应用完全可以设置为1，能够快速的淘汰无用数据，对于节省存储空间和提高查询速度有效果。不过这类需求在海量数据领域比较小众。 压缩算法：可以尝试一下最新出炉的snappy算法，相对lzo来说，压缩率接近，压缩效率稍高，解压效率高很多。 inmemory：表在内存中存放，一直会被忽略的属性。如果完全将数据存放在内存中，那么hbase和现在流行的内存数据库memorycached和redis性能差距有多少，尚待实测。 bloomfilter：根据应用来定，看需要精确到rowkey还是column。不过这里需要理解一下原理，bloomfilter的作用是对一个region下查找记录所在的hfile有用。即如果一个region下的hfile数量很多，bloomfilter的作用越明显。适合那种compaction赶不上flush速度的应用。 rowkeyrowkey是hbase的key-value存储中的key，通常使用用户要查询的字段作为rowkey，查询结果作为value。可以通过设计满足几种不同的查询需求。 数字rowkey的从大到小排序：原生hbase只支持从小到大的排序，这样就对于排行榜一类的查询需求很尴尬。那么采用rowkey = Integer.MAX_VALUE-rowkey的方式将rowkey进行转换，最大的变最小，最小的变最大。在应用层再转回来即可完成排序需求。 rowkey的散列原则：如果rowkey是类似时间戳的方式递增的生成，建议不要使用正序直接写入rowkey，而是采用reverse的方式反转rowkey，使得rowkey大致均衡分布，这样设计有个好处是能将regionserver的负载均衡，否则容易产生所有新数据都在一个regionserver上堆积的现象，这一点还可以结合table的预切分一起设计。 columnfamilycolumnfamily尽量少，原因是过多的columnfamily之间会互相影响。 column对于column需要扩展的应用，column可以按普通的方式设计，但是对于列相对固定的应用，最好采用将一行记录封装到一个column中的方式，这样能够节省存储空间。封装的方式推荐protocolbuffer。 摸索与讨论以下会分场景介绍一些特殊的表结构设计方法，只是一些摸索，欢迎讨论： value数目过多场景下的表结构设计：目前我碰到了一种key-value的数据结构，某一个key下面包含的column很多，以致于客户端查询的时候oom，bulkload写入的时候oom，regionsplit的时候失败这三种后果。通常来讲，hbase的column数目不要超过百万这个数量级。在官方的说明和我实际的测试中都验证了这一点。有两种思路可以参考，第一种是单独处理这些特殊的rowkey，第二种如下：可以考虑将column设计到rowkey的方法解决。例如原来的rowkey是uid1,，column是uid2，uid3…。重新设计之后rowkey为&lt;uid2&gt;，&lt;uid1&gt;…当然大家会有疑问，这种方式如何查询，如果要查询uid1下面的所有uid怎么办。这里说明一下hbase并不是只有get一种随机读取的方法。而是含有scan(startkey,endkey)的扫描方法，而这种方法和get的效率相当。需要取得uid1下的记录只需要new Scan(“uid1&quot;,&quot;uid1~”)即可。这里的设计灵感来自于hadoop world大会上的一篇文章，这篇文章本身也很棒，推荐大家看一下http://www.cloudera.com/resource/hadoop-world-2011-presentation-slides-advanced-hbase-schema-design/ 其他参考资料： 表设计优化总结参考HBase性能优化方法总结（一）：表的设计关于cloumn family的描述：不要在一张表里定义太多的****column family。目前Hbase并不能很好的处理超过2~3个column family的表。因为某个column family在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://www.wortyby.com/tags/HBASE/"}]},{"title":"Python 进程管理工具 Supervisor 使用教程","slug":"Python 进程管理工具 Supervisor 使用教程","date":"2018-09-06T11:16:21.880Z","updated":"2018-09-06T11:57:32.700Z","comments":true,"path":"2018/09/06/Python 进程管理工具 Supervisor 使用教程/","link":"","permalink":"http://www.wortyby.com/2018/09/06/Python 进程管理工具 Supervisor 使用教程/","excerpt":"","text":"Supervisor 是基于 Python 的进程管理工具，可以帮助我们更简单的启动、重启和停止服务器上的后台进程，是 Linux 服务器管理的效率工具。什么情况下我们需要进程管理呢？就是执行一些需要以守护进程方式启动的程序，比如一个后台任务、一组 Web 服务的进程（说是一组，是因为经常用 Nginx 来做负载均衡），这些很可能是一些网站、REST API 的服务、消息推送的后台服务、日志数据的处理分析服务等等。需要注意的是 Supervisor 是通用的进程管理工具，可以用来启动任意进程，不仅仅是用来管理 Python 进程。Supervisor 经常被用来管理由 gunicorn 启动的 Django 或 Flask 等 Web 服务的进程。我最常用的是用来管理和启动一组 Tornado 进程来实现负载均衡。除此之外，Supervisor 还能很友好的管理程序在命令行上输出的日志，可以将日志重定向到自定义的日志文件中，还能按文件大小对日志进行分割。目前 Supervisor 只能运行在 Unix-Like 的系统上，也就是无法运行在 Windows 上。Supervisor 官方版目前只能运行在 Python 2.4 以上版本，但是还无法运行在 Python 3 上，不过已经有一个 Python 3 的移植版 supervisor-py3k。Supervisor 有两个主要的组成部分：supervisord，运行 Supervisor 时会启动一个进程 supervisord，它负责启动所管理的进程，并将所管理的进程作为自己的子进程来启动，而且可以在所管理的进程出现崩溃时自动重启。supervisorctl，是命令行管理工具，可以用来执行 stop、start、restart 等命令，来对这些子进程进行管理。 安装sudo pip install supervisor 创建配置文件echo_supervisord_conf &gt; /etc/supervisord.conf 如果出现没有权限的问题，可以使用这条命令sudo su - root -c &quot;echo_supervisord_conf &gt; /etc/supervisord.conf&quot; 配置文件说明想要了解怎么配置需要管理的进程，只要打开 supervisord.conf 就可以了，里面有很详细的注释信息。打开配置文件vim /etc/supervisord.conf 默认的配置文件是下面这样的，但是这里有个坑需要注意，supervisord.pid 以及 supervisor.sock 是放在 /tmp 目录下，但是 /tmp 目录是存放临时文件，里面的文件是会被 Linux 系统删除的，一旦这些文件丢失，就无法再通过 supervisorctl 来执行 restart 和 stop 命令了，将只会得到 unix:///tmp/supervisor.sock 不存在的错误 。因此可以单独建一个文件夹，来存放这些文件，比如放在 /home/supervisor/创建文件夹mkdir /home/supervisor mkdir /var/log/supervisor mkdir /etc/supervisor.d 然后对一些配置进行修改1234567891011121314151617181920212223242526272829303132333435363738394041[unix_http_server] ;file=/tmp/supervisor.sock ; (the path to the socket file) ;修改为 /home/supervisor 目录，避免被系统删除 file=/home/supervisor/supervisor.sock ; (the path to the socket file) ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for ;all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ... [supervisord] ;logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log) ;修改为 /var/log 目录，避免被系统删除 logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) ; 日志文件多大时进行分割 logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) ; 最多保留多少份日志文件 logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) ;pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ;修改为 /home/supervisor 目录，避免被系统删除 pidfile=/home/supervisor/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ... ;设置启动supervisord的用户，一般情况下不要轻易用root用户来启动，除非你真的确定要这么做 ;user=chrism ; (default is current user, required if root) ... [supervisorctl] ; 必须和&apos;unix\\_http\\_server&apos;里面的设定匹配 ;serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socket ;修改为 /home/supervisor 目录，避免被系统删除 serverurl=unix:///home/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set ...默认情况下，进程的日志文件达到50MB时，将进行分割，最多保留10个文件，当然这些配置也可以对每个进程单独配置。 权限问题设置好配置文件后，应先创建上述配置文件中新增的文件夹。如果指定了启动用户 user，这里以 oxygen 为例，那么应注意相关文件的权限问题，包括日志文件，否则会出现没有权限的错误。例如设置了启动用户 oxygen，然后启动 supervisord 出现错误Error: Cannot open an HTTP server: socket.error reported errno.EACCES (13) 就是由于上面的配置文件中 /home/supervisor 文件夹，没有授予启动 supervisord 的用户 oxygen 的写权限，可以将这个文件夹的拥有者设置该该账号sudo chown oxygen /home/supervisor 一般情况下，我们可以用 root 用户启动 supervisord 进程，然后在其所管理的进程中，再具体指定需要以那个用户启动这些进程。 使用浏览器来管理supervisor 同时提供了通过浏览器来管理进程的方法，只需要注释掉如下几行就可以了。12345678910;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for ;all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) [supervisorctl] ... ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set 使用 include在配置文件的最后，有一个 [include] 的配置项，跟 Nginx 一样，可以 include 某个文件夹下的所有配置文件，这样我们就可以为每个进程或相关的几个进程的配置单独写成一个文件。12[include]files = /etc/supervisor.d/*.ini 进程的配置样例sudo pip install supervisor 一个简单的例子如下; 设置进程的名称，使用 supervisorctl 来管理进程时需要使用该进程名 [program:your_program_name] command=python server.py --port=9000 ;numprocs=1 ; 默认为1 ;process_name=%(program_name)s ; 默认为 %(program_name)s，即 [program:x] 中的 x directory=/home/python/tornado_server ; 执行 command 之前，先切换到工作目录 user=oxygen ; 使用 oxygen 用户来启动该进程 ; 程序崩溃时自动重启，重启次数是有限制的，默认为3次 autorestart=true redirect_stderr=true ; 重定向输出的日志 stdout_logfile = /var/log/supervisor/tornado_server.log loglevel=info 设置日志级别loglevel 指定了日志的级别，用 Python 的 print 语句输出的日志是不会被记录到日志文件中的，需要搭配 Python 的 logging 模块来输出有指定级别的日志。 多个进程按照官方文档的定义，一个 [program:x] 实际上是表示一组相同特征或同类的进程组，也就是说一个 [program:x] 可以启动多个进程。这组进程的成员是通过 numprocs 和 process_name 这两个参数来确定的，这句话什么意思呢，我们来看这个例子。12345678910111213; 设置进程的名称，使用 supervisorctl 来管理进程时需要使用该进程名 [program:foo] ; 可以在 command 这里用 python 表达式传递不同的参数给每个进程 command=python server.py --port=90%(process_num)02d directory=/home/python/tornado_server ; 执行 command 之前，先切换到工作目录 ; 若 numprocs 不为1，process\\_name 的表达式中一定要包含 process_num 来区分不同的进程 numprocs=2 process_name=%(program_name)s_%(process_num)02d; user=oxygen ; 使用 oxygen 用户来启动该进程 autorestart=true ; 程序崩溃时自动重启 redirect_stderr=true ; 重定向输出的日志 stdout_logfile = /var/log/supervisor/tornado_server.log loglevel=info上面这个例子会启动两个进程，process_name 分别为 foo:foo_01 和 foo:foo_02。通过这样一种方式，就可以用一个 [program:x] 配置项，来启动一组非常类似的进程。再介绍两个配置项 stopasgroup 和 killasgroup12345; 默认为 false，如果设置为 true，当进程收到 stop 信号时，会自动将该信号发给该进程的子进程。如果这个配置项为 true，那么也隐含 killasgroup 为 true。例如在 Debug 模式使用 Flask 时，Flask 不会将接收到的 stop 信号也传递给它的子进程，因此就需要设置这个配置项。 stopasgroup=false ; send stop signal to the UNIX process ; 默认为 false，如果设置为 true，当进程收到 kill 信号时，会自动将该信号发给该进程的子进程。如果这个程序使用了 python 的 multiprocessing 时，就能自动停止它的子线程。 killasgroup=false ; SIGKILL the UNIX process group (def false)更详细的配置例子，可以参考如下，官方文档在这里123456789101112131415161718192021222324252627282930;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;autorestart=unexpected ; whether/when to restart (default: unexpected) ;startsecs=1 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; &apos;expected&apos; exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout\\_logfile\\_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_capture\\_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=&quot;1&quot;,B=&quot;2&quot; ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) 将多个进程按组管理Supervisor 同时还提供了另外一种进程组的管理方式，通过这种方式，可以使用 supervisorctl 命令来管理一组进程。跟 [program:x] 的进程组不同的是，这里的进程是一个个的 [program:x] 。123[group:thegroupname] programs=progname1,progname2 ; each refers to &apos;x&apos; in \\[program:x\\] definitions priority=999 ; the relative start priority (default 999)当添加了上述配置后，progname1 和 progname2 的进程名就会变成 thegroupname:progname1 和 thegroupname:progname2 以后就要用这个名字来管理进程了，而不是之前的 progname1。以后执行 supervisorctl stop thegroupname: 就能同时结束 progname1 和 progname2，执行 supervisorctl stop thegroupname:progname1 就能结束 progname1。supervisorctl 的命令我们稍后介绍。 启动 supervisord执行 supervisord 命令，将会启动 supervisord 进程，同时我们在配置文件中设置的进程也会相应启动。123456# 使用默认的配置文件 /etc/supervisord.conf supervisord # 明确指定配置文件 supervisord -c /etc/supervisord.conf # 使用 user 用户启动 supervisord supervisord -u user更多参数请参考文档 supervisorctl 命令介绍12345678910111213141516# 停止某一个进程，program_name 为 [program:x] 里的 x supervisorctl stop program_name # 启动某个进程 supervisorctl start program_name # 重启某个进程 supervisorctl restart program_name # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理) supervisorctl stop groupworker: # 结束 groupworker:name1 这个进程 (start，restart 同理) supervisorctl stop groupworker:name1 # 停止全部进程，注：start、restart、stop 都不会载入最新的配置文件 supervisorctl stop all # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程 supervisorctl reload # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启 supervisorctl update注意：显示用 stop 停止掉的进程，用 reload 或者 update 都不会自动重启。也可以参考这里 开机自动启动 Supervisord 方法1有一个简单的方法，因为 Linux 在启动的时候会执行 /etc/rc.local 里面的脚本，所以只要在这里添加执行命令就可以12# 如果是 Ubuntu 添加以下内容 /usr/local/bin/supervisord -c /etc/supervisord.conf12# 如果是 Centos 添加以下内容 /usr/bin/supervisord -c /etc/supervisord.conf以上内容需要添加在 exit 命令前，而且由于在执行 rc.local 脚本时，PATH 环境变量未全部初始化，因此命令需要使用绝对路径。可以用 which supervisord 查看一下 supervisord 所在的路径。在添加前，先在终端测试一下命令是否能正常执行，如果找不到 supervisord，可以用如下命令找到sudo find / -name supervisord 如果是 Ubuntu 16.04 以上，rc.local 被当成了服务，而且默认是不会启动，需要手动启用一下服务。https://askubuntu.com/questions/765120/after-upgrade-to-16-04-lts-rc-local-not-executing-command启用 rc.local 服务sudo systemctl enable rc-local.service 方法2Supervisord 默认情况下并没有被安装成服务，它本身也是一个进程。官方已经给出了脚本可以将 Supervisord 安装成服务，可以参考这里查看各种操作系统的安装脚本，但是我用官方这里给的 Ubuntu 脚本却无法运行。安装方法可以参考 serverfault 上的回答。比如我是 Ubuntu 系统，可以这么安装，这里选择了另外一个脚本123456789# 下载脚本 sudo su - root -c &quot;sudo curl https://gist.githubusercontent.com/howthebodyworks/176149/raw/d60b505a585dda836fadecca8f6b03884153196b/supervisord.sh &gt; /etc/init.d/supervisord&quot; # 设置该脚本为可以执行 sudo chmod +x /etc/init.d/supervisord # 设置为开机自动运行 sudo update-rc.d supervisord defaults # 试一下，是否工作正常 service supervisord stop service supervisord start注意：这个脚本下载下来后，还需检查一下与我们的配置是否相符合，比如默认的配置文件路径，pid 文件路径等，如果存在不同则需要进行一些修改。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"python","slug":"python","permalink":"http://www.wortyby.com/tags/python/"}]},{"title":"马尔可夫链的扩展 贝叶斯网络","slug":"马尔可夫链的扩展 贝叶斯网络 ","date":"2018-09-05T06:24:26.034Z","updated":"2018-09-05T06:29:01.637Z","comments":true,"path":"2018/09/05/马尔可夫链的扩展 贝叶斯网络 /","link":"","permalink":"http://www.wortyby.com/2018/09/05/马尔可夫链的扩展 贝叶斯网络 /","excerpt":"","text":"我们在前面的系列中多次提到马尔可夫链 (MarkovChain)，它描述了一种状态序列，其每个状态值取决于前面有限个状态。这种模型，对很多实际问题来讲是一种很粗略的简化。在现实生活中，很多事物相互的关系并不能用一条链来串起来。它们之间的关系可能是交叉的、错综复杂的。比如在下图中可以看到，心血管疾病和它的成因之间的关系是错综复杂的。显然无法用一个链来表示。 信念网络我们可以把上述的有向图看成一个网络，它就是**贝叶斯网络。其中每个圆圈表示一个状态。状态之间的连线表示它们的因果关系。比如从心血管疾病出发到吸烟的弧线表示心血管疾病可能和吸烟有关。当然，这些关系可以有一个量化的可信度 (belief)，用一个概率描述。我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名。由于网络的每个弧有一个可信度，贝叶斯网络也被称作信念网络** (belief networks)。和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述事件之间的相关性。可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。 贝叶斯网络与马尔科夫链的比较使用贝叶斯网络必须知道各个状态之间相关的概率。得到这些参数的过程叫做训练。和训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据。比如在训练上面的网络，需要知道一些心血管疾病和吸烟、家族病史等有关的情况。相比马尔可夫链，贝叶斯网络的训练比较复杂，从理论上讲，它是一个 NP-complete 问题，也就是说，对于现在的计算机是不可计算的。但是，对于某些应用，这个训练过程可以简化，并在计算上实现。 贝叶斯网络的工具与应用值得一提的是 IBM Watson 研究所的茨威格博士 (Geoffrey Zweig) 和西雅图华盛顿大学的比尔默 (Jeff Bilmes) 教授完成了一个通用的贝叶斯网络的工具包，提供给对贝叶斯网络有兴趣的研究者。贝叶斯网络在图像处理、文字处理、支持决策等方面有很多应用。在文字处理方面，语义相近的词之间的关系可以用一个贝叶斯网络来描述。我们利用贝叶斯网络，可以找出近义词和相关的词，在 Google 搜索和 Google 广告中都有直接的应用。原文链接","categories":[{"name":"数学","slug":"数学","permalink":"http://www.wortyby.com/categories/数学/"}],"tags":[{"name":"分类","slug":"分类","permalink":"http://www.wortyby.com/tags/分类/"},{"name":"图","slug":"图","permalink":"http://www.wortyby.com/tags/图/"}]},{"title":"矩阵运算和文本处理中的分类问题","slug":"矩阵运算和文本处理中的分类问题","date":"2018-09-05T06:10:34.388Z","updated":"2018-09-05T06:57:58.038Z","comments":true,"path":"2018/09/05/矩阵运算和文本处理中的分类问题/","link":"","permalink":"http://www.wortyby.com/2018/09/05/矩阵运算和文本处理中的分类问题/","excerpt":"","text":"背景我在大学学习线性代数时，实在想不出它除了告诉我们如何解线性方程外，还能有什么别的用途。关于矩阵的许多概念，比如特征值等等，更是脱离日常生活。后来在数值分析中又学了很多矩阵的近似算法，还是看不到可以应用的地方。当时选这些课，完全是为了混学分的学位。我想，很多同学都多多少少有过类似的经历。直到后来长期做自然语言处理的研究，我才发现数学家们提出那些矩阵的概念和算法，是有实际应用的意义的。 语音分类问题在自然语言处理中，最常见的两类的分类问题分别是，将文本按主题归类（比如将所有介绍亚运会的新闻归到体育类）和将词汇表中的字词按意思归类（比如将各种体育运动的名称个归成一类）。这两种分类问题都可用通过矩阵运算来圆满地、同时解决。为了说明如何用矩阵这个工具类解决这两个问题的，让我们先来来回顾一下我们在余弦定理和新闻分类中介绍的方法。分类的关键是计算相关性。我们首先对两个文本计算出它们的内容词，或者说实词的向量，然后求这两个向量的夹角。当这两个向量夹角为零时，新闻就相关；当它们垂直或者说正交时，新闻则无关。当然，夹角的余弦等同于向量的内积。从理论上讲，这种算法非常好。但是计算时间特别长。通常，我们要处理的文章的数量都很大，至少在百万篇以上，二次回标有非常长，比如说有五十万个词（包括人名地名产品名称等等）。如果想通过对一百万篇文章两篇两篇地成对比较，来找出所有共同主题的文章，就要比较五千亿对文章。现在的计算机一秒钟最多可以比较一千对文章，完成这一百万篇文章相关性比较就需要十五年时间。注意，要真正完成文章的分类还要反复重复上述计算。 矩阵处理在文本分类中，另一种办法是利用矩阵运算中的奇异值分解（Singular Value Decomposition，简称 SVD)。现在让我们来看看奇异值分解是怎么回事。首先，我们可以用一个大矩阵A来描述这一百万篇文章和五十万词的关联性。这个矩阵中，每一行对应一篇文章，每一列对应一个词。在上面的图中，M=1,000,000，N=500,000。第 i 行，第 j 列的元素，是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF)。读者可能已经注意到了，这个矩阵非常大，有一百万乘以五十万，即五千亿个元素。奇异值分解就是把上面这样一个大矩阵，分解成三个小矩阵相乘，如下图所示。比如把上面的例子中的矩阵分解成一个一百万乘以一百的矩阵X，一个一百乘以一百的矩阵B，和一个一百乘以五十万的矩阵Y。这三个矩阵的元素总数加起来也不过1.5亿，仅仅是原来的三千分之一。相应的存储量和计算量都会小三个数量级以上。三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次**奇异值分解**，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。现在剩下的唯一问题，就是如何用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值等等，以及数值分析的各种算法就统统用上了。在很长时间内，奇异值分解都无法并行处理。（虽然 Google 早就有了MapReduce 等并行计算的工具，但是由于奇异值分解很难拆成不相关子运算，即使在 Google 内部以前也无法利用并行计算的优势来分解矩阵。）最近，Google 中国的张智威博士和几个中国的工程师及实习生已经实现了奇异值分解的并行算法，我认为这是 Google 中国对世界的一个贡献。利用相对熵，我们可以到处信息检索中最重要的一个概念：词频率-逆向文档频率（TF/IDF)。我们下回会介绍如何根据相关性对搜索出的网页进行排序，就要用的餐TF/IDF 的概念。另外，在新闻的分类中也要用到相对熵和 TF/IDF。原文链接信息熵","categories":[{"name":"数学","slug":"数学","permalink":"http://www.wortyby.com/categories/数学/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"http://www.wortyby.com/tags/线性代数/"},{"name":"分类","slug":"分类","permalink":"http://www.wortyby.com/tags/分类/"}]},{"title":"PySpark 初体验之DataFrame","slug":"PySpark 初体验之DataFrame","date":"2018-09-04T11:06:16.164Z","updated":"2018-09-04T11:47:24.031Z","comments":true,"path":"2018/09/04/PySpark 初体验之DataFrame/","link":"","permalink":"http://www.wortyby.com/2018/09/04/PySpark 初体验之DataFrame/","excerpt":"","text":"##背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 鉴于这篇文章介绍过RDD，本篇文章重点介绍DataFrame。DataFrame 是一个组织成命名列的数据集。它在概念上等同于关系数据库中的表或是Python 中的里的数据框架(pandas)，但其经过了优化。DataFrames 可以从各种各样的源构建，例如结构化数据文件,Hive 中的表,外部数据库,RDD 转换DataFrame API 可以被Scala，Java，Python和R调用。Python 中 DataFrame 由 Row 的数据集表示 。由于我们现在是在PySpark 的环境中使用，所以我们就谈谈 DataFrame 在 Python 中的使用。或许之前了解过 pandas 里的dataframe 操作，也可以对比着了解，同时 也支持 PySpark DataFrame 与 pandas 的 dataframe 之间的转换。该文将以 Spark初始化，DataFrame 初始化，DataFrame 运算，RDD 与 DataFrame 转化，DataFrame 与 pandas dataframe 转化 四个部分展开 Spark 初始化：在 进行 DataFrame 操作 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkSession 与 SparkContext 初始化123456789from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(&quot;RDD_and_DataFrame&quot;) \\ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \\ .getOrCreate() sc = spark.sparkContextsparkContext 初始化之后，需要进行数据读取来初始化DataFrame DataFrame 初始化我们以从 txt 文档读取数据的方式来初始化DataFrame 的例子来说明 DataFrame 的相关运算方式以及相关性质 创建DataFrame 以及 DataFrame 与 RDD 的转换123456789101112131415161718192021 lines = sc.textFile(&quot;employee.txt&quot;)parts = lines.map(lambda l: l.split(&quot;,&quot;))employee = parts.map(lambda p: Row(name=p[0], salary=int(p[1]))) #RDD转换成DataFrameemployee_temp = spark.createDataFrame(employee) #显示DataFrame数据employee_temp.show() #创建视图employee_temp.createOrReplaceTempView(&quot;employee&quot;)#过滤数据employee_result = spark.sql(&quot;SELECT name,salary FROM employee WHERE salary &gt;= 14000 AND salary &lt;= 20000&quot;) # DataFrame转换成RDDresult = employee_result.rdd.map(lambda p: &quot;name: &quot; + p.name + &quot; salary: &quot; + str(p.salary)).collect() #打印RDD数据for n in result:","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"DataFrame","slug":"DataFrame","permalink":"http://www.wortyby.com/tags/DataFrame/"}]},{"title":"PySpark 初体验之RDD","slug":"PySpark 初体验之运算规则","date":"2018-09-03T08:24:27.468Z","updated":"2018-09-04T11:43:21.196Z","comments":true,"path":"2018/09/03/PySpark 初体验之运算规则/","link":"","permalink":"http://www.wortyby.com/2018/09/03/PySpark 初体验之运算规则/","excerpt":"","text":"背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 本文重点介绍RDD，下篇文章重点介绍DataFrame。RDD 可以导入外部存储系统的数据集，例如：HDFS，HBASE 或者 本地文件 *.JSON 或者*.XML 等数据源。该文将以 SparkContext初始化，RDD 初始化，运算，持久化 四个部分展开 SparkContext 初始化：在 进行RDD 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkContext123from pyspark import SparkConf, SparkContextconf = *** #spark运行环境的配置信息初始化sc = SparkContext(conf)sparkContext 初始化之后，需要进行数据读取来初始化RDD RDD 初始化我们以初始化数字和字符串的例子来说明 RDD的相关运算方式以及相关性质 创建RDD1kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3)]) RDD 转化为 Python 数据类型RDD 类型的数据可以使用 collect 方法转化为 Python 的数据类型:1print (kvRDD.collect())输出为1[(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;, 3)] RDD 基本运算 map 运算map运算可以通过传入的函数，将每一个元素经过函数运算产生另外一个RDD。下面这个例子将 value 值统一加 1之后进行返回 关于 value 的list 。1print ((kvRDD.map(lambda x:x[1]+1).collect()))输出为1[5, 7, 7, 3, 4] filter 运算filter可以用于对RDD内每一个元素进行筛选，并产生另外一个RDD。下面的例子中，我们筛选kvRDD中数字小于3的元素。1print(kvRDD.filter(lambda a:a[0]&gt;3).collect())输出为1[(5, 6), (&apos;sd&apos;, 3)] distinct 运算distinct运算会删除重复的元素：下面的例子中，我们去除kvRDD中重复的的元素('sd',3)。12kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])print(kvRDD. distinct().collect())输出为1[(&apos;sd&apos;, 3), (5, 6), (1, 2), (3, 4), (3, 6)] randomSplit 运算randomSplit 运算将整个集合以随机数的方式按照比例分为多个RDD，比如按照0.4和0.6的比例将kvRDD分为两个RDD，并输出：1234567kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])sRDD = kvRDD.randomSplit([0.4,0.6])print (len(sRDD))print (sRDD[0].collect())print (sRDD[1].collect())输出为1232[(5, 6), (1, 2)][(3, 4), (3, 6), (&apos;sd&apos;, 3), (&apos;sd&apos;, 3)] groupBy 运算groupBy运算可以按照传入匿名函数的规则，将数据分为多个Array。123kvRDD = sc.parallelize([1,2,3,4,5])result = kvRDD.groupBy(lambda x: x % 2).collect()print(sorted([(x, sorted(y)) for (x, y) in result]))输出为1[(0, [2, 4]), (1, [1, 3, 5])] RDD 复合运算RDD 也支持执行多个RDD的运算,即支持 复合运算RDD 复合运算主要包含 并集运算、交集运算、差集运算、笛卡尔积运算先准备三个DEMO RDD123RDD1 = sc.parallelize([3,1,2,5,5])RDD2 = sc.parallelize([5,6])RDD3 = sc.parallelize([2,7]) 并集运算通过使用 union 函数进行并集运算:输入1print(RDD1.union(RDD2).union(RDD3).collect())输出1[3, 1, 2, 5, 5, 5, 6, 2, 7] 交集运算可以使用intersection进行交集运算：输入1print(RDD1.intersection(RDD3).collect())输出1[2] 差集运算可以使用subtract函数进行差集运算:输入1print (RDD1.subtract(RDD2).collect())由于两个RDD的重复部分为5，所以输出为[1,2,3]:输出1[1,2,3] 笛卡尔积运算可以使用cartesian函数进行笛卡尔乘积运算:输入1print (RDD1.cartesian(RDD2).collect())由于两个RDD分别有5个元素和2个元素，所以返回结果有10各元素：输出1[(3, 5), (3, 6), (1, 5), (1, 6), (2, 5), (2, 6), (5, 5), (5, 5), (5, 6), (5, 6)] RDD 基本 “动作” 运算 读取元素可以使用下列命令读取RDD内的元素，这是Actions运算，所以会马上执行：12345678#取第一条数据print (RDD1.first())#取前两条数据print (RDD1.take(2))#升序排列，并取前3条数据print (RDD1.takeOrdered(10))#降序排列，并取前3条数据print (RDD1.takeOrdered(3,lambda x:-x))输出为:12343[3, 1][1, 2, 3, 5, 5][5, 5, 3] 统计功能可以将RDD内的元素进行统计运算：1234567891011121314#统计print (RDD1.stats())#最小值print (RDD1.min())#最大值print (RDD1.max())#标准差print (RDD1.stdev())#计数print (RDD1.count())#求和print (RDD1.sum())#平均print (RDD1.mean())输出为:1234567(count: 5, mean: 3.2, stdev: 1.6, max: 5.0, min: 1.0)151.65163.2 RDD key-Value 基本 “转换” 运算Spark RDD支持键值对运算，Key-Value运算是 MR*(mapreduce)*运算的基础，本节介绍RDD键值的基本“转换”运算。 初始化我们用元素类型为tuple元组的数组初始化我们的RDD，这里，每个tuple的第一个值将作为键，而第二个元素将作为值。1kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)]) 得到key和value值可以使用keys和values函数分别得到RDD的键数组和值数组：12print (kvRDD1.keys().collect())print (kvRDD1.values().collect())输出为：12[3, 3, 5, 1][4, 6, 6, 2] 筛选元素可以按照键进行元素筛选，也可以通过值进行元素筛选，和之前的一样，使用filter函数，这里要注意的是，虽然RDD中是以键值对形式存在，但是本质上还是一个二元组，二元组的第一个值代表键，第二个值代表值，所以按照如下的代码既可以按照键进行筛选，我们筛选键值小于5的数据：1print (kvRDD1.filter(lambda x:x[0] &lt; 5).collect())输出为：1[(3, 4), (3, 6), (1, 2)]同样，将x[0]替换为x[1]就是按照值进行筛选，我们筛选值小于5的数据：1print (kvRDD1.filter(lambda x:x[1] &lt; 5).collect())输出为：1[(3, 4), (1, 2)] 值运算我们可以使用mapValues方法处理value值，下面的代码将value值进行了平方处理：1print (kvRDD1.mapValues(lambda x:x**2).collect())输出为：1[(3, 16), (3, 36), (5, 36), (1, 4)] 按照key排序可以使用sortByKey按照key进行排序，传入参数的默认值为true，是按照从小到大排序，也可以传入参数false，表示从大到小排序：123print (kvRDD1.sortByKey().collect())print (kvRDD1.sortByKey(True).collect())print (kvRDD1.sortByKey(False).collect())输出为：123[(1, 2), (3, 4), (3, 6), (5, 6)][(1, 2), (3, 4), (3, 6), (5, 6)][(5, 6), (3, 4), (3, 6), (1, 2)] 合并相同key值的数据使用reduceByKey函数可以对具有相同key值的数据进行合并。比如下面的代码，由于RDD中存在（3,4）和（3,6）两条key值均为3的数据，他们将被合为一条数据：1print (kvRDD1.reduceByKey(lambda x,y:x+y).collect())输出为1[(1, 2), (3, 10), (5, 6)] 多个RDD Key-Value“转换”运算 初始化首先我们初始化两个k-v的RDD：12kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])kvRDD2 = sc.parallelize([(3,8)]) 内连接运算join运算可以实现类似数据库的内连接，将两个RDD按照相同的key值join起来，kvRDD1与kvRDD2的key值唯一相同的是3，kvRDD1中有两条key值为3的数据（3,4）和（3,6），而kvRDD2中只有一条key值为3的数据（3,8），所以join的结果是（3，（4,8）） 和（3，（6，8））：1print (kvRDD1.join(kvRDD2).collect())输出为:1[(3, (4, 8)), (3, (6, 8))] 左外连接使用leftOuterJoin可以实现类似数据库的左外连接，如果kvRDD1的key值对应不到kvRDD2，就会显示None1print (kvRDD1.leftOuterJoin(kvRDD2).collect())输出为:1[(1, (2, None)), (3, (4, 8)), (3, (6, 8)), (5, (6, None))] 右外连接使用rightOuterJoin可以实现类似数据库的右外连接，如果kvRDD2的key值对应不到kvRDD1，就会显示None1print (kvRDD1.rightOuterJoin(kvRDD2).collect())输出为：1[(3, (4, 8)), (3, (6, 8))] 删除相同key值数据使用subtractByKey运算会删除相同key值得数据：1print (kvRDD1.subtractByKey(kvRDD2).collect())结果为：1[(1, 2), (5, 6)] Key-Value“动作”运算 读取数据可以使用下面的几种方式读取RDD的数据：1234567891011121314#读取第一条数据print (kvRDD1.first())#读取前两条数据print (kvRDD1.take(2))#读取第一条数据的key值print (kvRDD1.first()[0])#读取第一条数据的value值print (kvRDD1.first()[1])输出为:(3, 4)[(3, 4), (3, 6)]34 按key值统计：使用countByKey函数可以统计各个key值对应的数据的条数：1print (kvRDD1.countByKey().collect())输出为：1defaultdict(&lt;type &apos;int&apos;&gt;, &#123;1: 1, 3: 2, 5: 1&#125;) lookup查找运算使用lookup函数可以根据输入的key值来查找对应的Value值：1print (kvRDD1.lookup(3))输出为：1[4, 6] 持久化操作spark RDD的持久化机制，可以将需要重复运算的RDD存储在内存中，以便大幅提升运算效率，有两个主要的函数：持久化使用persist函数对RDD进行持久化：1kvRDD1.persist()在持久化的同时我们可以指定持久化存储等级：等级说明MEMORY_ONLY以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将不会被缓存， 这样当再次需要这些分区的时候，将会重新计算。这是默认的级别。MEMORY_AND_DISK以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将将会缓存在磁盘上，再次需要的时候从磁盘读取。MEMORY_ONLY_SER以序列化JAVA对象的方式存储 (每个分区一个字节数组). 相比于反序列化的方式,这样更高效的利用空间， 尤其是使用快速序列化时。但是读取是CPU操作很密集。MEMORY_AND_DISK_SER与MEMORY_ONLY_SER相似, 区别是但内存不足时，存储在磁盘上而不是每次重新计算。DISK_ONLY只存储RDD在磁盘MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.与上面的级别相同，只不过每个分区的副本只存储在两个集群节点上。OFF_HEAP (experimental)将RDD以序列化的方式存储在 Tachyon. 与 MEMORY_ONLY_SER相比, OFF_HEAP减少了垃圾回收。允许执行体更小通过共享一个内存池。因此对于拥有较大堆内存和高并发的环境有较大的吸引力。更重要的是，因为RDD存储在Tachyon上，执行体的崩溃不会造成缓存的丢失。在这种模式下.Tachyon中的内存是可丢弃的，这样 Tachyon 对于从内存中挤出的块不会试图重建它。如果你打算使用Tachyon作为堆缓存，Spark提供了与Tachyon相兼容的版本。首先我们导入相关函数：1from pyspark.storagelevel import StorageLevel在scala中可以直接使用上述的持久化等级关键词，但是在pyspark中封装为了一个类，StorageLevel类，并在初始化时指定一些参数，通过不同的参数组合，可以实现上面的不同存储等级。StorageLevel类的初始化函数如下：123456def __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1): self.useDisk = useDisk self.useMemory = useMemory self.useOffHeap = useOffHeap self.deserialized = deserialized self.replication = replication那么不同的存储等级对应的参数为:1234567891011121314151617181920StorageLevel.DISK_ONLY = StorageLevel(True, False, False, False)StorageLevel.DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)StorageLevel.MEMORY_ONLY = StorageLevel(False, True, False, False)StorageLevel.MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)StorageLevel.MEMORY_AND_DISK = StorageLevel(True, True, False, False)StorageLevel.MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)StorageLevel.OFF_HEAP = StorageLevel(True, True, True, False, 1)&quot;&quot;&quot;.. note:: The following four storage level constants are deprecated in 2.0, since the records \\will always be serialized in Python.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER = StorageLevel.MEMORY_ONLY&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER_2 = StorageLevel.MEMORY_ONLY_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY_2`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER = StorageLevel.MEMORY_AND_DISK&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER_2 = StorageLevel.MEMORY_AND_DISK_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK_2`` instead.&quot;&quot;&quot; 取消持久化使用unpersist函数对RDD进行持久化：1kvRDD1.unpersist() 综上所述:哇，有关pyspark的RDD的基本操作就是上面这些啦，想要了解更多的盆友们可以参照官网给出的官方文档今天主要介绍了两种RDD，基本的RDD和Key-Value形式的RDD，介绍了他们的几种“转换”运算和“动作”运算，整理如下：RDD运算说明基本RDD“转换”运算map（对各数据进行转换），filter（过滤符合条件的数据），distinct（去重运算），randomSplit（根据指定的比例随机分为N各RDD），groupBy（根据条件对数据进行分组），union（两个RDD取并集），intersection（两个RDD取交集），subtract（两个RDD取差集）。cartesian（两个RDD进行笛卡尔积运算）基本RDD“动作”运算first（取第一条数据），take（取前几条数据），takeOrdered（排序后取前N条数据），统计函数Key-Value形式 RDD“转换”运算filter（过滤符合条件的数据），mapValues（对value值进行转换），sortByKey（根据key值进行排序），reduceByKey（合并相同key值的数据），join（内连接两个KDD），leftOuterJoin（左外连接两个KDD），rightOuterJoin（右外连接两个RDD），subtractByKey（相当于key值得差集运算）Key-Value形式 RDD“动作”运算first（取第一条数据），take（取前几条数据），countByKey（根据key值分组统计），lookup（根据key值查找value值）RDD持久化persist用于对RDD进行持久化，unpersist取消RDD的持久化，注意持久化的存储等级","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"RDD","slug":"RDD","permalink":"http://www.wortyby.com/tags/RDD/"}]},{"title":"tmpwatch 工具使用说明","slug":"tmpwatch 工具使用说明","date":"2018-08-28T11:47:00.266Z","updated":"2018-08-31T09:14:56.837Z","comments":true,"path":"2018/08/28/tmpwatch 工具使用说明/","link":"","permalink":"http://www.wortyby.com/2018/08/28/tmpwatch 工具使用说明/","excerpt":"","text":"背景tmp 目录原来只有在启动的时候才会被清理…… 不同的 Linux 发行版其实对 /tmp 目录的清理方式有所不同：在 Debian-like 的系统，启动的时候才会清理 (规则定义在 /etc/default/rcS)在 RedHat-like 的系统，按文件存在时间定时清理 (RHEL6 规则定义在 /etc/cron.daily/tmpwatch; RHEL7 以及 RedHat-like with systemd 规则定义在 /usr/lib/tmpfiles.d/tmp.conf, 通过 systemd-tmpfiles-clean.service 服务调用)在 CentOS 里，也是按文件存在时间清理的 (通过 crontab 的配置 /etc/cron.daily 定时执行 tmpwatch 来实现)在 Gentoo 里也是启动清理，规则定义在 /etc/conf.d/bootmisc，大 Gentoo 就是不走寻常路对于那些只能开机清理临时文件的发行版，如果作为服务器，这种不重启就对临时文件目录的垃圾不问不管的做事风格实在是很不靠谱。不过从上面其他发行版大家估计也会发现，解决此问题的关键就在于 tmpwatch 和定时任务的配合使用。 服务器除了调用用户的计划任务外，还会执行系统自己的，比如：12345/etc/cron.hourly/etc/cron.daily/etc/cron.daily目的能够自动删除不经常使用的临时文件为了保证tmp目录不爆满，系统默认情况下每日会处理一次tmp目录文件。安装1yum install tmpwatch -y功能说明删除暂存文件(默认是240小时，10天)tmpwatch 是专门用于解决“删除 xxx 天没有被访问/修改过的文件”这样需求的命令。使用方式也极其简单：除了删除tmp 文件夹之外也可以删除任何其他指定文件夹语法tmpwatch [-afqv][–test][超期时间][目录…]执行tmpwatch指令可删除不必要的暂存文件，您可以设置文件超期时间，单位以小时计算。 tmpwatch 参数说明123456789101112131415-u, --atime 基于访问时间来删除文件，默认的。-m, --mtime 基于修改时间来删除文件。-c, --ctime 基于创建时间来删除文件，对于目录，基于mtime。-M, --dirmtime 删除目录基于目录的修改时间而不是访问时间。-a, --all 删除所有的文件类型，不只是普通文件，符号链接和目录。-d, --nodirs 不尝试删除目录，即使是空目录。-d, --nosymlinks 不尝试删除符号链接。-f, --force 强制删除。-q, --quiet 只报告错误信息。-s, --fuser 如果文件已经是打开状态在删除前，尝试使用“定影”命令。默认不启用。-t, --test 仅作测试，并不真的删除文件或目录。-U, --exclude-user=user 不删除属于谁的文件。-v, --verbose 打印详细信息。-x, --exclude=path 排除路径，如果路径是一个目录，它包含的所有文件被排除了。如果路径不存在，它必须是一个绝对路径不包含符号链接。-X, --exclude-pattern=pattern 排除某规则下的路径。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"清理","slug":"清理","permalink":"http://www.wortyby.com/tags/清理/"}]}]}