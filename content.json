{"meta":{"title":"值得荐","subtitle":"值得推荐的原创文章，技术文章或是科普文章","description":"同乐科技","author":"1One's Dad","url":"http://www.wortyby.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-08-29T02:08:14.000Z","updated":"2018-08-29T05:24:24.000Z","comments":false,"path":"/404.html","permalink":"http://www.wortyby.com//404.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-31T09:06:48.000Z","updated":"2018-08-31T09:12:59.000Z","comments":true,"path":"categories/index.html","permalink":"http://www.wortyby.com/categories/index.html","excerpt":"","text":""},{"title":"datastructure","date":"2018-08-31T09:09:28.000Z","updated":"2018-08-31T09:12:41.000Z","comments":true,"path":"datastructure/index.html","permalink":"http://www.wortyby.com/datastructure/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-08-31T09:08:27.000Z","updated":"2018-08-31T09:13:13.000Z","comments":true,"path":"tags/index.html","permalink":"http://www.wortyby.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"内容爬取小试牛刀","slug":"内容爬取小试牛刀","date":"2019-06-30T14:32:28.041Z","updated":"2019-06-30T15:01:15.616Z","comments":true,"path":"2019/06/30/内容爬取小试牛刀/","link":"","permalink":"http://www.wortyby.com/2019/06/30/内容爬取小试牛刀/","excerpt":"","text":"内容爬取小试牛刀通常 request-html 可以直接爬取静态网站的内容,但是日常开发过程中,爬取完全静态的网站是很少存在的需求,那么我们今天来看看,爬取一个动态网站的过程是怎么样的呢？下面我们看看从准备工作到实践上机操作, 需要付出哪些准备工作。 工具链requests-htmljson 解析html解析re从html过滤出 url 参数chrome获取页面元素的 xpath / css-selectorpython写脚本sublime-text 运行python脚本 示例代码: 先获得你想爬取的页面的link-url直接贴入chrome地址栏,Enter 键 浏览器右键选择 检查选择Element 这个 tab找到你想取的内容，鼠标点上去，右键选择 copy selector 或者 copy xpath在python脚本里写入这个 selector or xpathExample:12345678910111213141516171819from requests_html import HTMLSessionfrom http_request import parse_jiuzhenshijian_resultsession = HTMLSession()url = &apos;domain/Article/index/3/catid/5&apos;r=session.get(url)# 获取 class = ui-container 的element l = r.html.find(&apos;.ui-container&apos;) # 获取页面的文本信息text = r.html.text# 获取 请求的html 内容html = r.html.html # 也就是下一步 (获取页面HTML) 获取页面HTML1234567891011121314&lt;!DOCTYPE html&gt;&lt;html&gt;...function getlist(startnum)&#123; var kw=$(&apos;#kw&apos;).val(); var url = &apos;.../Article/getArticlelist?hosID=3&amp;catid=162&amp;kw=&apos;+kw+&apos;&amp;sign=9c62f0ab416fbda5ec8cd359e827ac9e&amp;pagesize=&apos;+pagesize+&apos;&amp;page=&apos;+startnum+&apos;&amp;usepage=1&apos;; xmlHttp2(url,getliststatus);&#125; ...&lt;/html&gt; 获取页面的sign供request 请求url 拼接使用1234567891011121314151617#parse.pydef get_sign_flag(r): html = r.html.html pattern = re.compile(r&apos;sign=\\w+&apos;) m =pattern.findall(html) print(m) return m[0] if len(m) else None get_sign_flag(r)&gt;&gt;[&apos;sign=9c62f0ab416fbda5ec8cd359e827ac9e&apos;]即可得 sign这个参数 异步 URL 请求参数 拼接通过上面几个步骤的执行我们获取到了想要爬取的页面的页面内容、请求的 base_url, 请求的 parameter拼接成一个新的 url 供 request-html 发起醒的网路请求按照如此循环的步骤，可以进行爬取某个完整的网站内容1234567url = &apos;domainArticle/getArticlelist?catid=&#123;0&#125;&amp;kw=&amp;sign=&#123;1&#125;&amp;pagesize=10&amp;page=0&amp;usepage=1&apos;.format(idx,sign) r=session.get(url) dic = r.json() # dic 即为我们通过拼接的请求 **url** 获取到的结果内容","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://www.wortyby.com/categories/爬虫/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://www.wortyby.com/tags/爬虫/"},{"name":"requests-html","slug":"requests-html","permalink":"http://www.wortyby.com/tags/requests-html/"},{"name":"re","slug":"re","permalink":"http://www.wortyby.com/tags/re/"}]},{"title":"微信小程序-页面之间传递值","slug":"微信小程序页面传参数","date":"2019-06-30T06:05:56.846Z","updated":"2019-06-30T06:49:59.928Z","comments":true,"path":"2019/06/30/微信小程序页面传参数/","link":"","permalink":"http://www.wortyby.com/2019/06/30/微信小程序页面传参数/","excerpt":"","text":"微信小程序-页面之间传递值 小程序页面传值的方式正向传值: 上一页面 --&gt; 下一页面URL 传值本地存储全局的app对象反向传值: 下一页面 --&gt; 上一页面本地存储全局的app对象 先说一下正向传值: URL 传值通过 URL 传值的需要通过 option 来获取参数值。更多详情可以访问小程序-NavigateTo章节A页面123wx.navigateTo(&#123; url: &apos;test?id=1&apos;&#125;)B页面12345678910Page(&#123; data:&#123; id:&apos;&apos;, &#125;, onLoad: function(option)&#123; this.setData(&#123; id:option.id &#125;) &#125;&#125;) 本地存储关于缓存，可以先访问小程序-数据缓存稍作了解。A页面1wx.setStorageSync(&apos;username&apos;, &apos;ddd&apos;)B页面1234567891011Page(&#123; data:&#123; username:&apos;&apos;, &#125;, onLoad: function()&#123; var username = wx.getStorageSync(&apos;username&apos;) this.setData(&#123; username: username &#125;) &#125;&#125;) 全局的app对象关于app对象，可以访问小程序-注册程序了解相关信息。。A页面12var app = getApp();app.username=&apos;ddd&apos;;B页面12var app = getApp();var username = app.username; 反向传值: 本地存储：B页面：123wx.setStorageSync(&apos;username&apos;, &apos;ddd&apos;);//返回上一页wx.navigateBack();A页面：1234567891011Page(&#123; data:&#123; username:&apos;&apos;, &#125;, onShow: function()&#123; var username = wx.getStorageSync(&apos;username&apos;) this.setData(&#123; username: username &#125;) &#125;&#125;) 全局的app对象B页面：12var app = getApp();app.username=&apos;ddd&apos;;A页面：12345678910111213var app = getApp(); Page(&#123; data:&#123; username:&apos;&apos;, &#125;, onShow: function()&#123; var username = app.username; this.setData(&#123; username: username &#125;) &#125;&#125;) 在当前页通过获取前一个 page 实例，再赋值1234567var pages = getCurrentPages();var currPage = pages[pages.length - 1]; //当前页面var prevPage = pages[pages.length - 2]; //上一个页面//直接调用上一个页面的 setData() 方法，把数据存到上一个页面中去prevPage.setData(&#123; mdata:1 &#125;)这种方法的弊端：因为进入 B 页面的入口可能是很多个。这样做，可能会导致获取到的页面实例不正确。 通过onfire.js为来实现这个效果onfire.js使用思路：A 页面先订阅一个事件，并定义处理方法；从 B 页面返回时，发送消息；A 页面卸载时，解除订阅。使用方法如下：A页面代码如下：12345678910111213141516171819var onfire = require(&quot;../utils/onfire.js&quot;);var that;var eventObj = onfire.on(&apos;key&apos;, function () &#123; // 当消息被传递时，做具体的事&#125;);Page(&#123; data: &#123; &#125;, onLoad: function(options) &#123; // Do some initialize when page load. &#125;, onReady: function() &#123; // Do something when page ready. &#125;, onUnload: function (e) &#123; onfire.un(&apos;key&apos;); onfire.un(eventObj);//移除 &#125;&#125;)我们可以在 A 页面直接调用 onfire.on 方法，订阅一个名字为 key 的消息。在上面的代码中，消息附带的参数无传参。如果需要传参的话，直接在 function 里增加参数即可，例如：123var eventObj = onfire.on(&apos;key&apos;, function (data)&#123; // 执行操作&#125;)需要注意的是，一定要在 onUnload 里（在页面被关闭时）取消订阅消息，并取消绑定 eventObj。B 页面里代码在回调的地方加入以下代码：123onfire.fire(&apos;key&apos;);//key 为上文中订阅的消息// 有参数时onfire.fire(&apos;key&apos;,&apos;test&apos;) dva最终可能参考 dva 的事件订阅机制，demo 暂且先用页面传参方式吧","categories":[{"name":"小程序","slug":"小程序","permalink":"http://www.wortyby.com/categories/小程序/"}],"tags":[{"name":"页面传参数","slug":"页面传参数","permalink":"http://www.wortyby.com/tags/页面传参数/"}]},{"title":"指标体系","slug":"metric_info","date":"2019-03-21T13:36:02.875Z","updated":"2019-03-21T14:47:06.521Z","comments":true,"path":"2019/03/21/metric_info/","link":"","permalink":"http://www.wortyby.com/2019/03/21/metric_info/","excerpt":"","text":"用户 会员类指标* 注册会员数 (一定统计周期内的注册会员数量) * 活跃会员数 (一定统计周期内的有消费或登录行为的会员总数) * 活跃会员率 (活跃会员占注册会员总数的比重) * 会员复购率 (统计周期内，产生二次及二次以上购买的会员占购买会员的总数) * 会员平均购买次数 (指统计周期内,每个会员平均购买的次数,会员订单总数/购买的会员用户总数) * 会员回购率 (上一期未活跃用户在下一期时间内有购买行为的会员比例) * 会员留存率 (访问之后，过一段时间周期后，还会继续访问活动,活动周期一般是日、周、月、半年度) * 新会员留存率(可以按活跃度计算) * 新会员留存率(可以按消费纬度计算) * 活跃用户留存率 订单 订单生产率指标* 总订单数量(订单数之和，活动ID) * 总订单数量(订单数之和，活动周期全平台) * 访问到下单转化率(下单次数与访问次数之比,活动ID) * 访问到下单转化率(下单次数与访问次数之比，活动周期内平台总的) 总体销售业绩指标* GMV(成交额，只统计订单号金额) * 销售金额(出售的金额总额) * 客单价(订单金额/订单数量 比值) 统计类指标* 退款订单数(依据支付状态) * 活动ID * 活动周期全平台 * 会员/非会员订单比例(参与活动) * 活动ID * 活动周期全平台 * 订单分布状态 * 城市(订单总数,取消订单数,URN,SRN,OCC) * 活动ID * 活动周期全平台 * 渠道(订单总数,取消订单数,URN,SRN,OCC) * 活动ID * 活动周期全平台 * 酒店(订单总数，取消订单数，支付总资金,URN,SRN,OCC)) * 活动ID * 活动周期全平台 * 风控统计对象 * 人员信息 * 订单数 * 用户历史完成订单数 * 用户历史取消订单数 * 当日入住退房总订单数 * 入住时长小于5小时总订单数 * 入住时间在23:00-05:00 总订单数(半夜退房) * 用户历史当日退房订单数 * 用户历史半夜退房订单数 * 用户入住时长小于5小时订单数 * 用户入住时间在23:00-05:00 总订单数(半夜退房) * 存量客户各项预订间隔订单数(当天，提前一至三天，提前3-10天，提前10天以上) * 客户订单数(周一至周日) * 用户在当日下单总数里所占的次数贡献(比如 订单总数 100次，用户数就2次) * 金额 * 用户历史完成总金额 * 用户统计分布 * 当日注册用户总数 * 城市当日注册用户总数 * 当日预订用户总数 * 城市当日预订用户总数 * 当日入住用户总数 * 城市当日入住用户总数 * 当日注册且入住用户总数 * 当日推荐注册用户总数 * 活动日重复用户数(计算参加历史活动日的重复客户的数量) * 历史注册用户数手机品牌分布 * 历史注册用户数手机号归属地分布 * 历史注册用户数手机运营商分布 * 僵尸用户数 * 日活跃用户数(DAU) * 独立访客(UV) * 日访问页面数 * 日停留时长 * 日访问行为数量 * 历史预定不同酒店数的数量 * 预定不同间夜的用户数 * 历史预定城市的数量 * 预定不同酒店数的用户数量 * 渠道 * 订单数 * 各渠道当日预订单数 * 各渠道历史预订单数 * 金额 * 各渠道历史完成总金额 * 各渠道当日完成总金额 * 商品 * 金额 房间最高价格历史波动(计算一周内的最高价/平均价) * 营销 * 金额分布 * 首次预订折扣金额 * 各次预定的折扣金额 * 订单数 * 首次预订有折扣的订单数 * 预订折扣低于95%的订单数 * 营销活动期间的订单数 * 营销活动期间各城市的订单数 * 优惠券 * 各券历史已使用总量 * 各券已失效数量 * 各券存量数量 * 用户数 * 营销活动期间新增用户数 营销 渠道* 外部渠道 * 渠道新增用户 * 渠道新增订单 活动* 效果评估 * 新增用户数 * 新增注册人数 * 总访问次数 * 订单数量 * 下单转化率 * ROI * 活动期间用户活跃度 * 活动展现渠道 * 活动周期 * 不同活动整体环比增长率 * 优惠券 * 领取用户数 * 领取并消费用户数 * 优惠券过期仍未消费用户数 * 实时监测类指标 * 当前活动总访问用户数 * 近一周期的最低访问数 * 近一周期的最高访问数 * 今日访问用户概况 * 访问用户总数 * 登录老用户总数 * 登录新用户总数 * 下单用户总数 * 注册用户总数 * 用户访问数分时图 * 用户搜索活动数分时图 * 实时用户行为 12user_id action page timestamp&apos;张三&apos; &apos;访问了&apos; &apos;活动首页&apos; &apos;2019-03-18&apos; * 实时活动页面被访问次数排名 * 活动首页次数 * 活动详情页次数 * 活动下单页次数 * 库存变化 * 库存酒店变化排名 * 库存房型变化排名 * 库存房型消费价格变化排名 流量类指标 规模类指标* UV(独立访客数) * PV(页面访问数) * PV/UV(人均页面访问数,反应访问粘性变化) 流量质量类指标* 跳出率(浏览单页即退出的次数/该页访问次数,主要是来判断推广渠道目标人群和推广活动内容匹配程度) * 页面访问时长(单个页面被访问的时间)，结合转化率看，评判页面体验设计 * 人均页面浏览量(统计周期内，平均每个访客所浏览的页面量) 商品(酒店) 转化类指标* 收藏率 * 下单率 * 支付率 * 评价率 库存类指标* 库存消费的酒店分布 * 酒店位置(城市,具体地段可能需要使用外部接口数据) * 酒店总房间数 * 参与活动房间数 * 消费前用户的酒店浏览记录(主要看用户是第一选择还是备选) * 库存消费的房型分布 * 房型配置 * 房型价位 * 城市位置 * 消费前用户的酒店浏览记录(主要看用户是第一选择还是备选) * 库存消费的活动价位分布 * 城市位置 * 酒店类型 * 酒店规模 * 消费前用户的酒店浏览记录(主要看用户是第一选择还是备选)","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://www.wortyby.com/categories/数据分析/"}],"tags":[{"name":"订单分析","slug":"订单分析","permalink":"http://www.wortyby.com/tags/订单分析/"},{"name":"指标","slug":"指标","permalink":"http://www.wortyby.com/tags/指标/"}]},{"title":"离线数据分析平台实战-订单分析","slug":"离线数据分析平台实战-订单分析","date":"2019-02-20T09:02:55.754Z","updated":"2019-02-21T13:01:53.731Z","comments":true,"path":"2019/02/20/离线数据分析平台实战-订单分析/","link":"","permalink":"http://www.wortyby.com/2019/02/20/离线数据分析平台实战-订单分析/","excerpt":"","text":"现在电商行业由于业务发达，可能会涉及到具体分析，是哪里的哪些用户，会对该电商平台的产品感兴趣，该用户从哪里进来，该用户访问的深度信息等，下面简单描述一下，要分析这些内容，该从哪些方面入手，具体的可以从以下信息看到。 项目 wbs 步骤模块名称状态检查语言用户基本信息分析√spark浏览器信息分析(UA)√spark地域信息分析√spark外链信息分析√spark用户浏览深度分析√spark订单分析具体要写代码的spark事件分析具体要写代码的spark通过一个airflow 任务调度模块，形成一个依赖的dags 关系图，进而形成一个固定工作流程。 模块介绍订单分析 分别分析 订单的数量、订单的金额、以及将订单分为 总订单 、支付成功订单 以及 退款订单 三种类型的数据，通过这六个分析指标的数据我们可以指定网站的订单情况。 计算规则和统计stats_event&amp;stats_view_depth表的数据不太一样，我们采用每个统计指标写一个hql语句+sqoop语句的方法进行数据的插入操作。也就是说分别统计订单数量和订单金额，而不是使用一张hive表同时保存多个指标的数据，而是采用多个表分别保存不同指标的数据或者采用一张表非同时的保存多个指标的数据。分别统计oid的去重数量作为订单数量，使用去重后的订单的支付金额作为订单金额。最终数据保存：stats_order。涉及到所有列。涉及到其他表有dimension_platform、dimension_date、dimension_currency_type、dimension_payment_type. 代码步骤hive 中创建 hbase 对应的外部表;订单数量&amp;订单金额的 hive &amp; sqoop 分析;实现自定义 UDF &amp; 自定义函数创建hive + sqoop 脚本成功支付订单数量 &amp; 金额 &amp; 总金额的 hive &amp; sqoop 分析订单数据保存mysql;实现自定义 udf &amp; 自定义函数创建hive + sqoop 脚本退款订单数量 &amp; 金额 &amp; 总金额的 hive &amp; sqoop 分析shell 脚本编写以及测试","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://www.wortyby.com/categories/数据分析/"}],"tags":[{"name":"订单分析","slug":"订单分析","permalink":"http://www.wortyby.com/tags/订单分析/"},{"name":"分析方法","slug":"分析方法","permalink":"http://www.wortyby.com/tags/分析方法/"}]},{"title":"数据分析常见指标","slug":"数据分析常见指标","date":"2019-02-20T08:20:58.160Z","updated":"2019-02-20T08:41:19.025Z","comments":true,"path":"2019/02/20/数据分析常见指标/","link":"","permalink":"http://www.wortyby.com/2019/02/20/数据分析常见指标/","excerpt":"","text":"通常对一个事情的评价，会有一个量化的指标，即一个量化标准，这里我们讲下通常用来评价一个网站的好坏，会涉及到以下内容。 什么是展现量、点击量、点击率在百度推广后台可以看到消费、平均价格、点击、展现、点击率、千次展现费用等数据，这些数据是你全面评估推广效果、深入开展推广优化的基础。在网民搜索查询时，如果您账户内符合网民搜索需求的关键词被触发，该关键词所对应的创意将出现在搜索结果页，称之为关键词和创意的一次展现。一段时间内您获得的展现次数称之为 “展现量”。展现量体现了你的关键词质量度和创意的好坏（如果你的创意展现方式是优先展现的话）。在您的推广结果展现时，如果网民对您的推广结果感兴趣，希望进一步地了解您的产品/服务，可能将会点击访问您的网站。一段时间内您获得的点击次数称之为“点击量”。简单的说，点击量指你的创意被点击的次数。 点击量/展现量=点击率。点击率体现你的创意的吸引力。 如何用好展现量、点击率让推广更高效？展现量有助于您了解推广结果覆盖了多少网民，是一个数量上的概念。通过统计报告提供的展现量数据，您可以发现哪些关键词创意的展现机会较大，每天能够给您带来多少次的曝光机会，从而估算出您的推广活动能够覆盖到多少数量的潜在客户。如果您的所有关键词每天合计的展现量一直比较少，无法让您的推广结果得到充分的曝光，那么我们建议您可以考虑提交更多的关键词，并采用广泛匹配，从而达到覆盖更多潜在客户的目的。 什么是访客数(UV)访客数就是指一天之内到底有多少不同的用户访问了你的网站。访客数要比IP数更能真实准确地反映用户数量。百度统计完全抛弃了IP这个指标，而启用了访客数这一指标，是因为IP往往不能反映真实的用户数量。尤其对于一些流量较少的企业站来说，IP数和访客数会有一定的差别。访客数主要是以cookie为依据来进行判断的，而每台电脑的cookie也是不一样的。有些情况下IP数会大于真实的访客数。有时候访客数也会大于IP数。访客数要比IP数更能真实准确地反映用户数量。 什么是访问次数访问次数是指访客完整打开了网站页面进行访问的次数。访问次数是网站的访问速度的衡量标准。如果访问次数明显少于访客数，就说明很多用户在没有完全打开网页时就将网页关闭了。如果是这样的情况，我们就要好好检查一下网站的访问速度了，看看到底是网站空间出了问题还是网站程序出了问题。访问次数一般会大于访客数。 什么是浏览量(PV)浏览量和访问次数是呼应的。用户访问网站时每打开一个页面，就记为1个PV。同一个页面被访问多次，浏览量也会累积。一个网站的浏览量越高，说明这个网站的知名度越高，内容越受用户喜欢。一味地重视PV也是没有太大意义的（PV跟点击量差不多吧）。PV是一个重要的指标，反映了网站内容是否对用户有足够的吸引力。对于竞价而言，只能是侧面反映，因为我们设置了访问URL。很多用户需求也非常明确，来到网站之后，往往只会寻找自己需求的产品，所以一味地重视PV也是没有太大意义的。应该把重点内容展示给目标客户就可以了，就没必要一味地追求PV值，追求那些 转化率、跳出率、UV、转化次数等那才是重点。 什么是转化次数（重要，但是对于竞价，一般是把点击商务通作为转化页面的，所以很多是无意点击）潜在用户在我们的网站上完成一次我们期望的行为，就叫做一次转化。我们可以在百度统计的后台设置相应的转化页面，用户访问这个页面1次，就记为1次转化。 什么是转化率：转化率=转化次数/访问次数。对竞价而言，是关键词和访问页面的精准的指标。转化率可以用来衡量网络营销的效果。如果我们在A、B两个网站同时投放了广告，A网站每天能带来100次用户访问，但是只有1个转化，B网站每天能带来10次用户访问，但是却有5个转化。这就说明B网站带来的转化率更高，用户更加精准，网络营销效果更好。 什么是平均访问时长：平均访问时长是衡量网站用户体验的一个重要指标平均访问时长是用户访问网站的平均停留时间。平均访问时长=总访问时长/访问次数。如果用户不喜欢网站的内容，可能稍微看一眼就关闭网页了，那么平均访问时长就很短;如果用户对网站的内容很感兴趣，一连看了很多内容，或者在网站停留了很长时间，平均访问时长就很长。 什么叫平均访问页数：平均访问页数也是衡量网站的用户体验的指标平均访问页数是用户访问网站的平均浏览页数。平均访问页数=浏览量/访问次数。平均访问页数很少，说明访客进入你的网站后访问少数几个页面就离开了。 什么是跳出率：跳出率是反映网站流量质量的重要指标跳出率是指访客来到网站后，只访问了一个页面就离开网站的访问次数占总访问次数的百分比。跳出率=只访问一个页面就离开网站的访问次数/总访问次数，跳出率越低说明流量质量越好，用户对网站的内容越感兴趣。原文链接","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://www.wortyby.com/categories/数据分析/"}],"tags":[{"name":"评价指标","slug":"评价指标","permalink":"http://www.wortyby.com/tags/评价指标/"},{"name":"pv","slug":"pv","permalink":"http://www.wortyby.com/tags/pv/"},{"name":"uv","slug":"uv","permalink":"http://www.wortyby.com/tags/uv/"},{"name":"访问次数","slug":"访问次数","permalink":"http://www.wortyby.com/tags/访问次数/"},{"name":"跳出率","slug":"跳出率","permalink":"http://www.wortyby.com/tags/跳出率/"},{"name":"转化率","slug":"转化率","permalink":"http://www.wortyby.com/tags/转化率/"},{"name":"平均访问时长","slug":"平均访问时长","permalink":"http://www.wortyby.com/tags/平均访问时长/"}]},{"title":"集群管理相关文档","slug":"集群管理相关文档","date":"2019-02-02T03:01:02.000Z","updated":"2019-02-02T03:01:02.000Z","comments":true,"path":"2019/02/02/集群管理相关文档/","link":"","permalink":"http://www.wortyby.com/2019/02/02/集群管理相关文档/","excerpt":"","text":"结合目前公司使用的hdp 平台搭建的 ambari 管理平台，找到的金山云相关的文档中心，整理如下 金山云 ambari金山ambari金山Kafka","categories":[{"name":"集群管理相关","slug":"集群管理相关","permalink":"http://www.wortyby.com/categories/集群管理相关/"}],"tags":[{"name":"文档","slug":"文档","permalink":"http://www.wortyby.com/tags/文档/"}]},{"title":"pyppeteer 使用总结","slug":"pyppeteer","date":"2019-01-22T07:20:30.000Z","updated":"2019-01-22T07:20:30.000Z","comments":true,"path":"2019/01/22/pyppeteer/","link":"","permalink":"http://www.wortyby.com/2019/01/22/pyppeteer/","excerpt":"","text":"最近有个爬虫项目网站太多，绝大部分接口都是json，有些还有参数加密，一个个分析接口就太麻烦，就想用浏览器全部渲染出来，就可以省掉这些步骤。最近流行 headless 就用它了。puppeteer 是 nodejs 的，它的 python绑定比较好的是 pyppeteer。 基本用法中文资料非常少，接口看文档，例子看 tests 下面的测试用例。还可以看看 puppeteer 的教程。 启动参数browser = await pyppeteer.launch({ 'devtools': False, 'args': ['--no-sandbox', '--user-agent=&quot;' + UserAgent + '&quot;'] }) devtools 控制界面的显示，用来调试。args 是浏览器启动的命令行参数，可以设置浏览器头部，不然会标示为无头浏览器。--no-sandbox 在 docker 里使用时需要加入的参数，不然会报错。 请求钩子为了加快渲染速度，可以禁止加载图片之类的。123456789async def request_check(req): &apos;&apos;&apos;请求过滤&apos;&apos;&apos; if req.resourceType in [&apos;image&apos;, &apos;media&apos;, &apos;eventsource&apos;, &apos;websocket&apos;]: await req.abort() else: await req.continue_()await page.setRequestInterception(True)page.on(&apos;request&apos;, request_check) 网络问题请求加载是否完成，无网都需要处理123456789101112131415async def goto(page, url): while True: try: await page.goto(url, &#123; &apos;timeout&apos;: 0, &apos;waitUntil&apos;: &apos;networkidle0&apos; &#125;) break except (pyppeteer.errors.NetworkError, pyppeteer.errors.PageError) as ex: # 无网络 &apos;net::ERR_INTERNET_DISCONNECTED&apos;,&apos;net::ERR_TUNNEL_CONNECTION_FAILED&apos; if &apos;net::&apos; in str(ex): await asyncio.sleep(10) else: raise 注入 js 文件比如一些js库，我使用 Ajax-hook 来统计 ajax 的请求完成情况，需要在网页头部注入js文件，一些自己的库，比较大，也这样注入。123456from pathlib import PathCURDIR = Path(__file__).parentJS_AJAX_HOOK_LIB = str(CURDIR / &apos;static&apos; / &apos;ajaxhook.min.js&apos;)await page.addScriptTag(path=JS_AJAX_HOOK_LIB)这样注入的js文件不能有中文，因为 pyppeteer 里面打开文件用的是默认编码，可以 hook 住open 函数来解决。12# 因为 pyppeteer 库里面 addScriptTag 用的是系统默认编码,导致js文件里面不能有中文pyppeteer.frame_manager.open = lambda file: open(file, encoding=&apos;utf8&apos;) 在docker里使用在 window10 里开发很流程，部署到 windows server 上，可能由于配置比较差或其他原因，网站渲染很慢。可以放在容器里，效果明显。注意点是上面提到了的关闭沙盒模式，需要下一些浏览器的依赖，还有就是最好先把浏览器下好，做到镜像里，这样就不会在容器里一个一个下了。123456789101112131415FROM python:slimWORKDIR /usr/src/appRUN apt-get update &amp;&amp; apt-get install -y gconf-service libasound2 libatk1.0-0 libc6 libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 libglib2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxss1 libxtst6 ca-certificates fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils wgetRUN apt-get install -y vimCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtRUN python -c &quot;import pyppeteer;pyppeteer.chromium_downloader.download_chromium();&quot;COPY . .VOLUME /data","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://www.wortyby.com/categories/爬虫/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://www.wortyby.com/tags/爬虫/"}]},{"title":"经典案例","slug":"经典案例","date":"2019-01-15T08:36:13.000Z","updated":"2019-01-15T08:36:13.000Z","comments":true,"path":"2019/01/15/经典案例/","link":"","permalink":"http://www.wortyby.com/2019/01/15/经典案例/","excerpt":"","text":"算法背景CTR之FFM情感分析的现代方法（包含word2vec Doc2Vec）基于TextCNN的情感二分类新闻上的文本分类：机器学习大乱斗深入浅出排序学习：写给程序员的算法系统开发实践干货｜个性化推荐系统五大研究热点之用户画像（四）干货 | 个性化推荐系统五大研究热点之深度学习（一）多图万字文 | 从神经元到 CNN、RNN、GAN… 神经网络看本文绝对够了机器学习(03)朴素贝叶斯构建敏感词过滤系 具体的案例分析微博用户行为特征分析微博媒体特性以及用户使用状况研究大数据环境下微博舆情热点话题挖掘方法研究微博传播与微博营销之媒体传播特性北邮复杂网络与在线社交网络概论 论文 在线社交网络信息传播研究——以微博为例微博话题研究微博信息传播半衰期研究(修正版) 推荐相关一位算法师工程师的Spark机器学习笔记：构建一个简单的推荐系统推荐系统那点事 —— 基于Spark MLlib的特征选择spark机器学习笔记：（三）用Spark Python构建推荐系统推荐系统遇上深度学习(一)–FM模型理论和实践推荐系统遇上深度学习(十八)–探秘阿里之深度兴趣网络(DIN)浅析及实现 文本分类CNN 和 word2Vec 的文本分类基于 word2vec 和 CNN 的文本分类 ：综述 &amp; 实践文本分类(下)-卷积神经网络(CNN)在文本分类上的应用python文本相似度计算NLP之文本相似度漫谈：机器学习中距离和相似性度量方法 精华BLOGDanifree’s Blog我爱自然语言处理Mark Chang’s Blog 客户端可视化显示Seaborn使用Seaborn 教程 曾经的疑惑计算中文文本相似度有哪些好用的算法？日策略机器学习模型分析（小组内部讨论） Spark2.2.0 中文文档spark2.2.2 文档spark 英文文档","categories":[{"name":"学习计划","slug":"学习计划","permalink":"http://www.wortyby.com/categories/学习计划/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://www.wortyby.com/tags/算法/"},{"name":"案例","slug":"案例","permalink":"http://www.wortyby.com/tags/案例/"}]},{"title":"Protobuf","slug":"Protobuf 原理剖析","date":"2018-12-19T07:26:26.000Z","updated":"2018-12-19T07:26:26.000Z","comments":true,"path":"2018/12/19/Protobuf 原理剖析/","link":"","permalink":"http://www.wortyby.com/2018/12/19/Protobuf 原理剖析/","excerpt":"","text":"Protobuf 原理剖析AuthorDateVersionSevenFeng2017.01.11Ver1.0简介Protocol Buffers是Google开发一种数据描述语言，能够将结构化数据序列化，可用于数据存储、通信协议等方面。不了解Protocol Buffers的同学可以把它理解为更快、更简单、更小的JSON或者XML，区别在于Protocol Buffers是二进制格式，而JSON和XML是文本格式。protobuf Github地址据Google官方文档介绍，现在Google内部已经有48,162个消息类型定义在12,183个proto文件中，因此使用Protocol 文件的时候，我也能也需要定义一些 xxx.proto。本文会从快速入门、语言规范、编码协议、性能评估等几个方面对Prototol Buffers进行介绍。变长编码wiki变长编码广泛使用在对内存大小要求较高的场景.protobuf 采用的是自己开发的 Base 128 Varints编码12345678910111213&gt;&gt; 在理解Protocol Buffer的编码规则之前，你首先需要了解varints。&gt;&gt;varints是一种使用一个或多个字节表示整型数据&gt;&gt;的方法。其中数值本身越小，其所占用的字节数越少。&gt;&gt; &gt;&gt; 在varint中，除了最后一个字节之外的每个字节中都包含一个msb(most significant bit)设置(使用最高位)，这意味着其后的字节是否和当前字节一起来表示同一个整型数值。而字节中的其余七位将用于存储数据本身。由此我们可以简单的解释一下Base 128，通常而言，整数数值都是由字节表示，其中每个字节为8位，即Base 256。然而在Protocol Buffer的编码中，最高位成为了msb，只有后面的7位存储实际的数据，因此我们称其为Base 128（2的7次方）。&gt;&gt;字节序:小端模式 将低序字节存储在起始地址;在网络程序开发时 或是跨平台开发时 也应该注意保证只用一种字节序 不然两方的解释不一样就会产生bug. 因此不同的平台开发网络通信数据的时候需要保证 Host to Network htons 以及Network to Host ntohs 的数据字节序保证一致，所幸protobuf 库已经帮我们做好了这个转换.示例解析decode1234567891011121314&gt;&gt;10101011 01100001&gt;&gt;&gt;&gt;上述是通过编码了之后的二进制数据，总共16位，2个字节。。。&gt;&gt;&gt;&gt;(1) 从前面的一个字节说起吧：10101011 最开始的位是1，&gt;&gt; 那么它表示这个字节以后还有后续的字节。。。&gt;&gt; 然后将最前面的1去掉，还剩下 0101011&gt;&gt;&gt;&gt;(2) 接下来看后面的一个字节：01100001 最开始的位是0，那么表示这个字节是最后一个字节了，&gt;&gt;以后没有更多的字节，那么去掉这个0之后还剩下 1100001&gt;&gt;(3) 那么最后上面的二进制变成了 0101011 1100001 然后首位顺序还需要颠倒一下，那么变成了1100001 0101011，用10进制来表示的话就是 12459&gt;&gt;消息结构保证消息新旧版本兼容性原因123456789&gt;&gt;Protocol Buffer中的消息都是由一系列的键值对构成的。&gt;&gt;每个消息的二进制版本都是使用标签号作为key，&gt;&gt;而每一个字段的名字和类型均是在解码的过程中根据目标类型&gt;&gt;（反序列化后的对象类型）进行配对的。在进行消息编码时，key/value被连接成字节流。&gt;&gt;在解码时，解析器可以直接跳过不识别的字段，&gt;&gt;这样就可以保证新老版本消息定义在新老程序之间的兼容性，&gt;&gt;从而有效的避免了使用older消息格式的older程序在解析newer程序发来的newer消息时，&gt;&gt;一旦遇到未知（新添加的）字段时而引发的解析和对象初始化的错误。&gt;&gt;Protobuf 支持的 字段 类型Type|Meaning|Used For--------------------|------------------------|-----|0|Varint|int32, int64, uint32, uint64, sint32, sint64, bool, enum1|64-bit|fixed64,sfixed64,double2|Length-delimited|string, bytes, embedded messages, packed repeated fields3|32-bit|fixed32, sfixed32, float字段解析原理由于在编码后每一个字段的key都是varint类型，key的值是由字段标号和字段类型合成编码所得，其公式如下：12&gt;&gt;field_number &lt;&lt; 3 | field_type&gt;&gt;由此看出，key的最后3个bits用于存储字段的类型信息。那么在使用该编码时，Protocol Buffer所支持的字段类型将不会超过8种。这里我们可以进一步计算出Protocol Buffer在一个消息中可以支持的字段数量为2的29次方减一。举例 int32 来说1234&gt;&gt;message Test1 &#123;&gt;&gt;int32 a = 1;&gt;&gt;&#125;&gt;&gt;假设我们在应用程序中将字段a的值设置为150（十进制），此后再将该对象序列化到Binary文件中，你可以看到文件的数据为：08 96 01现在我们再来回顾一下之前给出的Test消息被序列化后的第一个字节08的由来。123&gt;&gt;0000 1000-&gt; 000 1000 //drop掉msb（最高位）&gt;&gt;最低的3位表示字段类型，即0为varint。我们再将结果右移3位( &gt;&gt; 3)，此时得到的结果为1，即字段a在消息Test1中的标签号。通过这样的结果，Protocol Buffer的解码器可以获悉当前字段的标签号是1，其后所跟随数据的类型为varint。现在我们可以继续利用上面讲到的知识分析出后两个字节(96 01)的由来。123456&gt;&gt;96 01 = 1001 0110 0000 0001 //一个字节数字96，一个字节01 -&gt; 001 0110 000 0001 //drop两个字节的msb -&gt; 000 0001 001 0110 //翻转高低字节 -&gt; 10010110 //去掉最高位中没用的0 -&gt; 128 + 16 + 4 + 2 = 150&gt;&gt;更多类型有符号类型如前所述，类型0表示varint，其中包含int32/int64/uint32/uint64/sint32/sint64/bool/enum。在实际使用中，如果当前字段可以表示为负数，那么对于int32/int64和sint32/sint64而言，它们在进行编码时将存在着较大的差别。如果使用int32/int64表示一个负数，该字段的值无论是-1还是-2147483648，其编码后长度将始终为10个字节，就如同对待一个很大的无符号整型一样。如果使用的是sint32/sint64 编码负数，Protocol Buffer将会采用ZigZag编码方式，其编码后的结果将会更加高效。下面是 ZigZag 对照表：Signed Original|Encoded As—|---|0|0-1|11|2-2|32147483647|4294967294-2147483648|4294967295其公式如下：123(n &lt;&lt; 1) ^ (n &gt;&gt; 31) //sint32(n &lt;&lt; 1&gt; ^ (n &gt;&gt; 63) //sint64&gt;&gt;需要补充说明的是，Protocol Buffer在实现上述位移操作时均采用的算术位移，因此对于(n &gt;&gt; 31)和(n &gt;&gt; 63)而言，如果n为负值位移后的结果就是-1，否则就是0。注：简单解释一下C语言中的算术位移和逻辑位移。他们的左移操作都是相同的，即低位补0，高位直接移除。不同的是右移操作，逻辑位移比较简单，高位全部补0。而算术位移则需要视当前值的符号位而定，补进的位和符号位相同，即正数全补0，负数全补1。换句话说，算术位移右移时要保证符号位的一致性。在C语言中，如果使用 int变量位移时就是算术位移，uint变量位移时是逻辑位移。Non-varint 数值型double/fixed64始终都占用8个字节，float/fixed32始终占用4个字节。Strings 类型其类型值为2，key信息之后是字节数组的长度信息，最后在紧随指定长度的实际数据值信息。12345&gt;&gt;&gt;message Test2 &gt;&gt;&gt;&#123; required string b = 2;//标号为2&#125;&gt;&gt;&gt;现在我们设置b的值为&quot;testing&quot;。其编码后数据如下：12&gt;&gt;&gt;12 07 74 65 73 74 69 6E 67&gt;&gt;&gt;第一个字节 0x12(0001 0010) 表示key，通过解码**(12==&gt;0x12==&gt;0b(0001 0010)&gt;18&gt;2&lt;&lt;3|2)**可以得到字段类型2和字段标号2。第二个字节 07 表示testing的长度。查阅Ascii 码表74==&gt;(0x74)==t65==&gt;(0x65)==e73==&gt;(0x73)==e74==&gt;(0x74)==t69==&gt;(0x69)==i6E==&gt;(0x6E)==n67==&gt;(0x67)==g所以 后面 7个字节则表示testing。嵌入消息先定义一个嵌套的消息类型12345&gt;&gt;&gt;message Test3 &gt;&gt;&gt;&#123; Test1 c = 3;//标号为3&#125;&gt;&gt;&gt;此时我们先将Test1的a字段值设置为150，其编码结果如下：1A 03 08 96 01 从上面的结果可以看出08 96 01和之前直接编码Test1时是完全一致的，只是在前面增加了key(字段类型 + 标号)和长度信息。新增信息的解码方式和含义与前面的Strings完全相同，这里不再重复解释了。1A==&gt;0x1A==&gt;0b(0001 1010)&gt;26&gt;3&lt;&lt;3|2所以 type =2;标号得3repeated 消息说明12345&gt;&gt;&gt;数组中的所有元素会被编码成一个单一的key/value形式。&gt;&gt;&gt;毕竟数组中的每一个元素都具有相同的字段类型和标号。该编码形式，&gt;&gt;&gt;对包含较小值的整型元素而言，&gt;&gt;&gt;优化后的编码结果可以节省更多的空间。&gt;&gt;&gt;先定义一个 repeated 的消息类型12345&gt;&gt;&gt;message Test4 &gt;&gt;&gt;&#123; repeat int32 d = 4;//标号为4&#125;&gt;&gt;&gt;这里我们假设d字段包含3个元素，值分别为3,270,86942。其编码结果如下：22 // key (字段标号4，类型为2) 06 // 数据中所有元素所占用的字节数量 (3占1 Byte; 270 占 2Byte ;86942 占 3字节) 03 // 第一个元素(varint 3) 8E 02 // 第二个元素(varint 270) 9E A7 05 // 第三个元素(varint 86942) 1234&gt;&gt;270==&gt;0b 1 0000 1110 ==字节翻转去除msb==&gt; 0b 000 1110 000 0010==补充截止符号==&gt; 0b 1000 1110 0000 0010 ===&gt;0x8E 0x02==&gt;8E 02&gt;&gt;&gt;字段顺序在.proto文件中定义消息的字段标号时，可以是不连续的，但是如果将其定义为连续递增的数值，将获得更好的编码和解码性能。 Protobuf 性能指标序列化速度报文大小评估准备使用统一的消息体格式使用的类库语言|类库----------|---------|Java|Java SerializeJava|XML;JAXBJava| Json使用gson 库Java |Protobuf 官方标准库 3.1.0C#|.Net SerializeC# |Protobuf 官方标准库 3.1.0C++|protobuf 官方标准库 3.1.0Objective-C|protobuf 官方标准库 3.1.0评估结果类报文大小(bytes)序列化耗时(ms)反序列化耗时(ms)Java Serialize|1610|997|1037Java XML|4141|2468|4001Java Json|3153|11437|17351Java Protobuf|933|776|422C# .Net Serialize|2726|4573|3185C# Protobuf|933|488|673C++ Protobuf|933|282|297Objective-C|933|310|308 proto 文件定义小结syntax = “proto3”; //声明proto 版本option java_package = “com.ajmd.ajpolling.proto”; //声明生成的Java 类包名option java_outer_classname = “AJMD_LC”; //声明生成的Java 类前缀option optimize_for = SPEED; //声明优化类型(SPEED/CODE_SIZE/LITE_RUNTIME)SPEED表示生成的代码运行效率高，但是由此生成的代码编译后会占用更多的空间CODE_SIZE和SPEED恰恰相反，代码运行效率较低，但是由此生成的代码编译后会占用更少的空间，通常用于资源有限的平台，如Mobile。LITE_RUNTIME生成的代码执行效率高，同时生成代码编译后的所占用的空间也是非常少。这是以牺牲Protocol Buffer提供的反射功能为代价的。因此我们在C++中链接Protocol Buffer库时仅需链接libprotobuf-lite，而非libprotobuf。在Java中仅需包含protobuf-java-2.4.1-lite.jar，而非protobuf-java-2.4.1.jar。 protobuf 与 json 共通据说 protobuf V3 与 json 已经支持(后面去研究)","categories":[{"name":"Protobuf","slug":"Protobuf","permalink":"http://www.wortyby.com/categories/Protobuf/"}],"tags":[{"name":"Protobuf","slug":"Protobuf","permalink":"http://www.wortyby.com/tags/Protobuf/"}]},{"title":"mqtt","slug":"mqttLib 设计文档","date":"2018-12-19T07:26:14.000Z","updated":"2018-12-19T07:26:14.000Z","comments":true,"path":"2018/12/19/mqttLib 设计文档/","link":"","permalink":"http://www.wortyby.com/2018/12/19/mqttLib 设计文档/","excerpt":"","text":"mqttLib(TLS/SSL)mqtt 建立连接 (详情请查阅 demo 代码)所需要的clientId,userName/Password其中所有的timestamp 使用的是year+month+day+hour+minute+second 比如[20170110130506]salt 会依据app的 version 动态变化，所以需要客户端维护一个 map(version,salt)**比如 {@“2.0.1”😡“JLEYcOiE45gTm6T2dUc7cgRC4DY8WYAt”} **Password 需要大写cleansession=False &amp;&amp; subscribe qos=1 or 2 来保障offline message receive.sub topictopic 规则**(ajmd/program/music/7204442/v1)**;consume message建立消息队列处理接受到的信息;连接失败，断开连接，重连机制，订阅成功处理，订阅失败处理;android 需要服务器提供自签名证书.参数说明clientID1clientId = ajmd_devicetoken_[iosandroid]_version#_timestampe.g1ajmd_ffffffff-9ce0-40a7-0033-c5870033c587_android_2.0.1_20170110130506userName1username=ajmd_devicetokene.g1ajmd_ffffffff-9ce0-40a7-0033-c5870033c587password1password=md5(devicetoken+version#+timestamp+salt)e.g12md5String = [md5(6ac33d90-79fb-4be4-865c-bb65cb912af62.0.120170110130506JLEYcOiE45gTm6T2dUc7cgRC4DY8WYAt) uppercaseString]password =52EBCE000A3588686CBA5464DC946974 动态库静态库(.so/.a)编译编译环境搭建CMake/Android Studio/Xcode编译选项设定支持的cpu 架构iOS(armv7 i386 x86_64 arm64)[Android(‘x86’, ‘armeabi’, ‘armeabi-v7a’)SSL/TLS 加密编译以及设置打开编译选项 增加OPENSSL 宏 ;下载openssl 源码到本地电脑目录 ;Xcode 或 CMake 设定openssl 源码文件目录 ;Xcode build .aandroid studio build .so protobuf 库编译iOS 直接使用 cocoaPods 管理;android 使用protobuf 官方的3.1.0 版本。 MQTTLib Arch server、iOS、Android 数据交换格式定义文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104syntax = &quot;proto3&quot;;option java_package = &quot;com.ajmd.ajpolling.proto&quot;;option java_outer_classname = &quot;AJMD_LC&quot;;option optimize_for = SPEED;message MessageReqResp&#123; string code =1;//消息ID string msg = 2; message PollData &#123; enum Msg_Type &#123; Msg_Type_Chat = 0;//聊天 Msg_Type_Gift =1;//礼物 Msg_Type_Plug =2;//插件 Msg_Type_Command =3;//命令 Msg_Type_Music =4;//音乐 Msg_Type_Pack =5;//礼包 Msg_Type_Prize =6;//中奖 &#125; message Chat&#123; int64 msgid = 1; string c = 2; string m = 3; int32 i = 4; string username = 5; string uimgPath = 6; string attach = 7; &#125; message Gift&#123; int64 msgid = 1; string c = 2; string m = 3; int32 i = 4; string username = 5; string uimgPath = 6; int32 giftType = 7; int32 giftNum = 8; string giftimgPath = 9; string giftUnit = 10; &#125; message Plug&#123; int64 msgid = 1; string c = 2; string m = 3; int32 i = 4; string username = 5; string uimgPath = 6; message TopicAttach&#123; string link=1; string name = 2; int64 programId = 3; string subject = 4; string thread_imgpath = 5; string topic_id = 6; string type = 7; string uimgPath = 8; string username=9; &#125; TopicAttach topicAttach = 7; &#125; message Command&#123; string date=1; string command=2; &#125; message Music &#123; int32 duration = 1;//时长 int32 musicOnTime = 2;//音乐所在流中的时间 string ablum = 3;//专辑名称 string artist = 4;//演唱者 string title = 5;//标题名称 &#125; message Pack &#123; int64 packageId=1; string presenterName=2; string presenterImgPath=3; string packageImg=4; &#125; message Prize &#123; int64 msgid=1; string c = 2; string m=3; int32 i = 4; string giftImg=5; string username=6; string uimgPath=7; &#125; Msg_Type type = 1; Chat chat =2; Gift gift = 3; Plug plug =4; Command cmd=5; Music music = 6; Pack pack =7; Prize prize =8; &#125; repeated PollData list = 3;&#125;&gt;创建平台代码&gt;&gt;protoc --java_out=. --objc_out=. --php_out=. ajmideMessage.proto","categories":[{"name":"mqtt","slug":"mqtt","permalink":"http://www.wortyby.com/categories/mqtt/"}],"tags":[{"name":"mqtt","slug":"mqtt","permalink":"http://www.wortyby.com/tags/mqtt/"}]},{"title":"Data Format","slug":"binary_data_format","date":"2018-12-19T07:25:59.000Z","updated":"2018-12-19T07:25:59.000Z","comments":true,"path":"2018/12/19/binary_data_format/","link":"","permalink":"http://www.wortyby.com/2018/12/19/binary_data_format/","excerpt":"","text":"Data Format We use protobuf to pack transfer Info message in version 3.1.0Since Protobuf Version 3.1.* begin to support more program language such as c/c++ ,objective-c ,python ,php,java etc to produce proper RPC Code for Android,iOS,Server to communicate each other. Transfer Binary Data FormatThe following is a sample which Ajmd Client communicate with Server:protobuf DataType Data (fixed 4Bytes)CMDRequest0001ChatMessage0002…………GameMessage1001…………GiftMessage1111parameter explain12- protobuf Data segment pack the Objective-C object data to send.- type Data segment pack the type enum that points out what response parser to parse the protobuf Data segment,this segment represent pack Transfer Binary Datastore the send data into a binary array which size is large 4bytes.store the needed parse type into the up binary array’s last 4bytessend the new packed binary data to the other.android exam:123456789101112131415161718192021222324252627282930CMDRequest.Builder builder = CMDRequest.newBuilder(); builder.setAction(CMD_TYPE.CMD_TYPE_ENDMUTELIVE); CMDRequest request = builder.build(); byte[] reqBytes = request.toByteArray(); final int len = reqBytes.length; final int type =5 ; final byte[] sendData = new byte[len+4]; final byte[] typeData = intToByteArray(type); System.arraycopy(reqBytes,0,sendData,0,len); System.arraycopy(typeData,0,sendData,len,4); final MqttMessage message = new MqttMessage(sendData); if(mqttClient==null || !mqttClient.isConnected())&#123; Toast.makeText(getApplicationContext(),&quot;MQTT未连接&quot;,Toast.LENGTH_LONG).show(); return; &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; try&#123; mqttClient.publish(targetID, message); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125;).start();ios exam:123456789101112131415161718192021222324252627 CMDRequest* req = [[CMDRequest alloc]init];[req setAction:CMD_TYPE_CmdTypeNeedendlive];NSData* sendData = [req data];int type = 5;NSData *data = [NSData dataWithBytes: &amp;type length: sizeof(type)];NSMutableData* sd = [NSMutableData data];[sd appendData:sendData];[sd appendData:data];MQTTAsync_message message = MQTTAsync_message_initializer;message.payload = (void*)[sd bytes];message.payloadlen = (int)[sd length];_publishButton.enabled = NO;_publishField.enabled = NO;MQTTAsync_responseOptions pubOptions = MQTTAsync_responseOptions_initializer;pubOptions.onSuccess = mqttPublishSucceeded;pubOptions.onFailure = mqttPublishFailed;pubOptions.context = (__bridge void*)weakSelf;int status = MQTTAsync_sendMessage(_mqttClient, _publishField.text.UTF8String, &amp;message, &amp;pubOptions);if (!status != MQTTASYNC_SUCCESS) &#123; _publishField.enabled = YES; _publishButton.enabled = YES; &#125; upack Transfer Binary DataReceive the entire binary data to a binary array.Extract last fixed 4bytes data to get the parse response type;According the type’s value ,client can parse the protobuf to proper client language object.android exam:12345678910111213141516171819202122232425@Override public void messageArrived(String topic, MqttMessage message) throws Exception &#123; final byte[] rdata = message.getPayload(); final int len = rdata.length; final int type ; final byte[] realData = new byte[len-4]; final byte[] typeData = new byte[4]; System.arraycopy(rdata,0,realData,0,realData.length); System.arraycopy(rdata,len-4,typeData,0,4); type = byteArrayToInt(typeData); CMDResponse response = CMDResponse.parseFrom(realData); final String msg = new String(message.getPayload()); this.runOnUiThread(new Runnable() &#123; public void run() &#123; Toast.makeText(getApplicationContext(), &quot;收到:&quot;+msg, Toast.LENGTH_LONG).show(); &#125; &#125;); &#125;ios exam:123456789101112131415161718192021int mqttMessageArrived(void* context, char* topicName, int topicLen, MQTTAsync_message* message)&#123; dispatch_async(dispatch_get_main_queue(), ^&#123; NSError* error = nil; NSData* data = [NSData dataWithBytes:message-&gt;payload length:message-&gt;payloadlen]; int type; NSMutableData* rd = [NSMutableData dataWithData:data]; NSData* typeData = [rd subdataWithRange:NSMakeRange(message-&gt;payloadlen-sizeof(type), sizeof(type))]; [typeData getBytes: &amp;type length: sizeof(type)]; NSData* RealData = [rd subdataWithRange:NSMakeRange(0, message-&gt;payloadlen-sizeof(type))]; CMDResponse* response = [CMDResponse parseFromData:RealData error:&amp;error]; printf(&quot;MQTT message arrived from topic: %s with body: %zi\\n&quot;, topicName, response.action); return true;&#125; how to use protobufdescribe the web service in a xxx.proto file likedefine the proto version by syntax= “proto3”;define the proto java package by option java_package = “com.ajmide.proto”;define the proto java class name by option java_outer_classname = “AJMD_LC”;define the compile option by option optimize_for = SPEED;example for definetion named ajmideMessage.proto123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170syntax = &quot;proto3&quot;;option java_package = &quot;com.ajmide.proto&quot;;option java_outer_classname = &quot;AJMD_LC&quot;;option optimize_for = SPEED;message ChatMessageRequest&#123; enum Msg_Type&#123; Msg_Type_Label = 0;//文字 Msg_Type_Image =1;//图片 &#125; Msg_Type type = 1 ;//消息类型 string msg = 2; string msgColor = 3; string img = 4; int32 time = 5; int32 insertTime = 6;&#125;message ChatMessageResponse&#123; message Result &#123; string msgId =1;//消息ID enum Msg_Type&#123; Msg_Type_Label = 0;//文字 Msg_Type_Image =1;//图片 &#125; Msg_Type type = 2 ;//消息类型 string msg = 3; string msgColor = 4; string img = 6; int32 time = 7; int32 insertTime = 8; string usernick = 9; enum Role_Type &#123; Role_Type_Usual = 0;//普通用户 Role_Type_Host = 1;//主播 &#125; Role_Type userrole = 10 ;//用户角色 string portrait = 11; &#125; repeated Result result = 1;&#125;message GiftMessageRequest&#123; enum Gift_Type &#123; Gift_Type_Gift = 0;//礼物 Gift_Type_Money =1;//打赏 &#125; Gift_Type type = 1 ;//礼物类型 string presenterId = 2;//主持人ID string giftId = 3;//礼物ID int32 count = 4;//礼物图片 int32 giftCode = 5;//礼物code int32 time = 6;//发送时间点 秒&#125;message GiftMessageResponse&#123; message Result &#123; string msgId =1;//消息ID enum Gift_Type&#123; Gift_Type_Gift = 0;//礼物 Gift_Type_Money =1;//打赏 &#125; Gift_Type type = 2 ;//礼物类型 int32 giftNum = 3;//礼物数量 string giftUnit = 4;//礼物单位 string giftImg = 5;//礼物图片 int32 time = 6;//发送时间点 秒 string usernick = 7;//用户昵称 string portrait = 8;//用户头像 &#125; repeated Result result = 1;&#125;message PluginMessageRequest&#123; enum Plugin_Type&#123; Plugin_Type_Topic = 0;//帖子 Plugin_Type_Vote =1;//投票 Plugin_Type_Sec = 2;//秒杀 &#125; Plugin_Type type = 1 ;//插件类型 string name = 2;//插件名称 string subject = 3;//插件标题 string topicId = 4;//帖子ID string programId = 5;//节目ID string plugImg = 6;//插件图片 string link = 7;//插件链接地址?这个发送的时候会不会走一个http发送callback 成URL？ int32 time = 8;//用户昵称&#125;message PluginMessageResponse&#123; message Result&#123; string msgId =1; enum Plugin_Type&#123; Plugin_Type_Topic = 0;//帖子 Plugin_Type_Vote =1;//投票 Plugin_Type_Sec = 2;//秒杀 &#125; Plugin_Type type = 2 ;//插件类型 string name = 3;//插件名称 string subject = 4;//插件标题 string topicId = 5;//帖子ID string programId = 6;//节目ID string plugImg = 7;//插件图片 string link = 8;//插件链接地址 string usernick = 9;//用户昵称 string portrait = 10;//用户头像 &#125; repeated Result result = 1;&#125;message DistinguishMusicMessageResponse&#123; message Result&#123; int32 duration = 1;//时长 int32 musicOnTime = 2;//音乐所在流中的时间 string ablum = 3;//专辑名称 string artist = 4;//演唱者 string giftImg = 5;//礼物图片 &#125; repeated Result result = 1;&#125;enum CMD_TYPE&#123; CMD_TYPE_STARTLIVE =0;//开始直播 CMD_TYPE_ENDLIVE=1;//结束直播 CMD_TYPE_NEEDENDLIVE=2;//即将结束直播 CMD_TYPE_STARTMUTELIVE=3;//静音 CMD_TYPE_ENDMUTELIVE=4;//结束静音&#125;message CMDRequest&#123; CMD_TYPE action =1;&#125;message CMDResponse&#123; CMD_TYPE action=1;&#125;//当前只有以下4种信息需要解析处理，后期如果有添加，需要更新以下枚举文件enum ResponseParseType&#123; ResponseParseType_ChatMessageResponse=0;//聊天信息解析 ResponseParseType_GiftMessageResponse=1;//礼物信息解析 ResponseParseType_PluginMessageResponse=2;//插件信息解析 ResponseParseType_DistinguishMusicMessageResponse=3;//音乐识别信息解析&#125;generate RPC code by the following command1protoc --java_out=. --objc_out=. --php_out=. ajmideMessage.proto","categories":[{"name":"Data Format","slug":"Data-Format","permalink":"http://www.wortyby.com/categories/Data-Format/"}],"tags":[{"name":"Data Format","slug":"Data-Format","permalink":"http://www.wortyby.com/tags/Data-Format/"}]},{"title":"AJMD Content Workflows (AJCW)","slug":"tornado 再体验","date":"2018-12-19T07:24:45.000Z","updated":"2018-12-19T07:24:45.000Z","comments":true,"path":"2018/12/19/tornado 再体验/","link":"","permalink":"http://www.wortyby.com/2018/12/19/tornado 再体验/","excerpt":"","text":"AJMD Content Workflows (AJCW)AJCW is a library designed to bring a simple appraoch to workflows in Content Processing Mornitor .It provides :workflow definition;Running code when performing transitions;Hooks for running extra code before/after transition;A hook for logging performed transitions;GlossaryLSFPlatform LSF is a commercial DRMSGESun Grid Engine is a commercial DRMGEGrid Engine is an open source version of SGEDRMDistributed Resource Management System. This is the underlying queuing software that manages jobs on a cluster. Examples include LSF, and SGEDRMAADistributed Resorce Management Application API. A standard library that is an abstraction built on top of DRM so that the same application code can seamlessly run on any DRM that supports DRMAADAGDirected Acyclic Graph. A DAG is used byCosmos to describe a workflow’s jobs and their dependencies on each other.FlaskA Python web framework Cosmos uses for its web-interfaceSqlalchemyA popular Python ORM that Cosmos uses","categories":[{"name":"workFlow","slug":"workFlow","permalink":"http://www.wortyby.com/categories/workFlow/"}],"tags":[{"name":"workflow","slug":"workflow","permalink":"http://www.wortyby.com/tags/workflow/"}]},{"title":"weUI 初体验","slug":"weUI 初体验","date":"2018-12-19T07:23:39.000Z","updated":"2018-12-19T07:23:39.000Z","comments":true,"path":"2018/12/19/weUI 初体验/","link":"","permalink":"http://www.wortyby.com/2018/12/19/weUI 初体验/","excerpt":"","text":"weUI 初体验1. Header Placeholder1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-cmn-Hans&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1,user-scalable=0&quot; /&gt; &lt;title&gt;WeUI&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;../style/weui.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;./example.css&quot; /&gt; &lt;/head&gt; &lt;body ontouchstart=&quot;&quot;&gt; &lt;div class=&quot;weui-toptips weui-toptips_warn js_tooltips&quot;&gt; 错误提示 &lt;/div&gt; &lt;div class=&quot;container&quot; id=&quot;container&quot;&gt;&lt;/div&gt;2. Page Content123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259&lt;script type=&quot;text/html&quot; id=&quot;tpl_home&quot;&gt;&lt;div class=&quot;page&quot;&gt; &lt;div class=&quot;page__hd&quot;&gt; &lt;h1 class=&quot;page__title&quot;&gt; &lt;img src=&quot;./images/logo.png&quot; alt=&quot;WeUI&quot; height=&quot;21px&quot; /&gt; &lt;/h1&gt; &lt;p class=&quot;page__desc&quot;&gt;WeUI 是一套同微信原生视觉体验一致的基础样式库，由微信官方设计团队为微信内网页和微信小程序量身设计，令用户的使用感知更加统一。&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;page__bd page__bd_spacing&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;div class=&quot;weui-flex js_category&quot;&gt; &lt;p class=&quot;weui-flex__item&quot;&gt;表单&lt;/p&gt; &lt;img src=&quot;./images/icon_nav_form.png&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;page__category js_categoryInner&quot;&gt; &lt;div class=&quot;weui-cells page__category-content&quot;&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;button&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Button&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;input&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Input&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;list&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;List&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;slider&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Slider&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;uploader&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Uploader&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;div class=&quot;weui-flex js_category&quot;&gt; &lt;p class=&quot;weui-flex__item&quot;&gt;基础组件&lt;/p&gt; &lt;img src=&quot;./images/icon_nav_layout.png&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;page__category js_categoryInner&quot;&gt; &lt;div class=&quot;weui-cells page__category-content&quot;&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;articl43&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Article&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;badge&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Badge&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;flex&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Flex&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;footer&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Footer&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;gallery&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Gallery&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;grid&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Grid&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;icons&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Icons&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;loadmore&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Loadmore&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;panel&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Panel&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;preview&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Preview&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;progress&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Progress&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;div class=&quot;weui-flex js_category&quot;&gt; &lt;p class=&quot;weui-flex__item&quot;&gt;操作反馈&lt;/p&gt; &lt;img src=&quot;./images/icon_nav_feedback.png&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;page__category js_categoryInner&quot;&gt; &lt;div class=&quot;weui-cells page__category-content&quot;&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;actionsheet&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Actionsheet&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;dialog&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Dialog&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;msg&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Msg&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;picker&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Picker&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;toast&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Toast&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;div class=&quot;weui-flex js_category&quot;&gt; &lt;p class=&quot;weui-flex__item&quot;&gt;导航相关&lt;/p&gt; &lt;img src=&quot;./images/icon_nav_nav.png&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;page__category js_categoryInner&quot;&gt; &lt;div class=&quot;weui-cells page__category-content&quot;&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;navbar&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Navbar&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;tabbar&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Tabbar&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;div class=&quot;weui-flex js_category&quot;&gt; &lt;p class=&quot;weui-flex__item&quot;&gt;搜索相关&lt;/p&gt; &lt;img src=&quot;./images/icon_nav_search.png&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;page__category js_categoryInner&quot;&gt; &lt;div class=&quot;weui-cells page__category-content&quot;&gt; &lt;a class=&quot;weui-cell weui-cell_access js_item&quot; data-id=&quot;searchbar&quot; href=&quot;javascript:;&quot;&gt; &lt;div class=&quot;weui-cell__bd&quot;&gt; &lt;p&gt;Search Bar&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;weui-cell__ft&quot;&gt;&lt;/div&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;div class=&quot;weui-flex js_item&quot; data-id=&quot;layers&quot;&gt; &lt;p class=&quot;weui-flex__item&quot;&gt;层级规范&lt;/p&gt; &lt;img src=&quot;./images/icon_nav_z-index.png&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;page__ft&quot;&gt; &lt;a href=&quot;javascript:home()&quot;&gt;&lt;img src=&quot;./images/icon_footer.png&quot; /&gt;&lt;/a&gt; &lt;/div&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt; $(function()&#123; var winH = $(window).height(); var categorySpace = 10; $(&apos;.js_item&apos;).on(&apos;click&apos;, function()&#123; var id = $(this).data(&apos;id&apos;); window.pageManager.go(id); &#125;); $(&apos;.js_category&apos;).on(&apos;click&apos;, function()&#123; var $this = $(this), $inner = $this.next(&apos;.js_categoryInner&apos;), $page = $this.parents(&apos;.page&apos;), $parent = $(this).parent(&apos;li&apos;); var innerH = $inner.data(&apos;height&apos;); bear = $page; if(!innerH)&#123; $inner.css(&apos;height&apos;, &apos;auto&apos;); innerH = $inner.height(); $inner.removeAttr(&apos;style&apos;); $inner.data(&apos;height&apos;, innerH); &#125; if($parent.hasClass(&apos;js_show&apos;))&#123; $parent.removeClass(&apos;js_show&apos;); &#125;else&#123; $parent.siblings().removeClass(&apos;js_show&apos;); $parent.addClass(&apos;js_show&apos;); if(this.offsetTop + this.offsetHeight + innerH &gt; $page.scrollTop() + winH)&#123; var scrollTop = this.offsetTop + this.offsetHeight + innerH - winH + categorySpace; if(scrollTop &gt; this.offsetTop)&#123; scrollTop = this.offsetTop - categorySpace; &#125; $page.scrollTop(scrollTop); &#125; &#125; &#125;); &#125;);&lt;/script&gt;3 Content Link Distinguish.1234567891011121314151617181920212223242526272829303132333435363738394041&lt;script type=&quot;text/html&quot; id=&quot;tpl_articl43&quot;&gt;&lt;div class=&quot;page&quot;&gt; &lt;div class=&quot;page__hd&quot;&gt; &lt;h1 class=&quot;page__title&quot;&gt;Article&lt;/h1&gt; &lt;p class=&quot;page__desc&quot;&gt;文章&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;page__bd&quot;&gt; &lt;article class=&quot;weui-article&quot;&gt; &lt;h1&gt;大标题&lt;/h1&gt; &lt;section&gt; &lt;h2 class=&quot;title&quot;&gt;章标题&lt;/h2&gt; &lt;section&gt; &lt;h3&gt;1.1 节标题&lt;/h3&gt; &lt;p&gt; Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. &lt;/p&gt; &lt;p&gt; &lt;img src=&quot;./images/pic_article.png&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;./images/pic_article.png&quot; alt=&quot;&quot;&gt; &lt;/p&gt; &lt;/section&gt; &lt;section&gt; &lt;h3&gt;1.2 节标题&lt;/h3&gt; &lt;p&gt; Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. &lt;/p&gt; &lt;/section&gt; &lt;/section&gt; &lt;/article&gt; &lt;/div&gt; &lt;div class=&quot;page__ft&quot;&gt; &lt;a href=&quot;javascript:home()&quot;&gt;&lt;img src=&quot;./images/icon_footer_link.png&quot; /&gt;&lt;/a&gt; &lt;/div&gt;&lt;/div&gt;&lt;/script&gt;4 footer123456 &lt;script src=&quot;./zepto.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;https://res.wx.qq.com/open/js/jweixin-1.0.0.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;./weui.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;./example.js&quot;&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt;","categories":[{"name":"weUI","slug":"weUI","permalink":"http://www.wortyby.com/categories/weUI/"}],"tags":[{"name":"weUI","slug":"weUI","permalink":"http://www.wortyby.com/tags/weUI/"}]},{"title":"Spark 提交 Python 那点事","slug":"Spark 提交PYTHON 那点事","date":"2018-12-18T02:58:15.000Z","updated":"2018-12-18T02:58:15.000Z","comments":true,"path":"2018/12/18/Spark 提交PYTHON 那点事/","link":"","permalink":"http://www.wortyby.com/2018/12/18/Spark 提交PYTHON 那点事/","excerpt":"","text":"背景通过 shell 脚本提交 spark-submit 任务的时候，发现一个问题，是前一秒可以正常运行，后一秒发现找不到相关自定义的module，刚开始以为是引用作用域的问题。后续确认Python在本地测试没有问题，运行正常，部署到server 就出现 ImportError: No module named * ，有点莫名的奇怪。 排错 确认pythonpath通过 sys.path 查看 module 所在路径都在 sys.path 路径列表里.所以可以排除 module 不在 server 的状况。 确认Python安装版本通过 which python 查得 Python 的版本数与本地开发环境的Python 版本一致、所以排除server 由于安装多个Python 版本导致的问题。目前 server 上没有安装虚拟环境，毕竟业务单一。 期待由于spark 是以集群资源来运行，但是发现单纯的通过spark-submit 提交的任务，通过增加 --verbose 查看运行信息，显示的都是local 运行模式，通过指定 --master yarn 才能指定集群以 yarn 运行，充分利用分布式集群的 yarn 任务调度。此时，你会发现 deployMode 还是 以 client 方式运行，通过文档可知，deployMode 还有一个 cluster 方式，所以我通过增加 一个参数 --deployMode cluster 来达到一个，让任务以集群方式正常运行。OOPS ，发现居然也是提示 ImportError: No module named *好了，知道怎么上 Google 利用关键词搜索了，输入 spark python ImportError: No module named ，霍霍，居然有好多类似答案，最终通过选择 这里 给出的方案，进行尝试，everything 都解决了。 总结12345spark-submit --verbose audio_als.py --py-files audio_als_dep.zip--master yarn --deployMode cluster &gt;&gt;/data/log/audio_als.log通过 --py-files 将 Python 依赖的 module 或 Python文件 打包成一个zip 统一提交到集群里，从而实现集群资源的一个统一调度以及资源规划。归根结底，还是要多研读官方文档。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"PySpark","slug":"PySpark","permalink":"http://www.wortyby.com/tags/PySpark/"},{"name":"Module","slug":"Module","permalink":"http://www.wortyby.com/tags/Module/"}]},{"title":"An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers — Part 1)","slug":"An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers — Part 1)","date":"2018-12-14T05:10:53.000Z","updated":"2018-12-14T05:10:53.000Z","comments":true,"path":"2018/12/14/An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers — Part 1)/","link":"","permalink":"http://www.wortyby.com/2018/12/14/An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers — Part 1)/","excerpt":"","text":"Programming is a crucial prerequisite for anyone wanting to learn machine learning. Sure quite a few autoML tools are out there, but most are still at a very nascent stage and well beyond an individual’s budget. The sweet spot for a data scientist lies in combining programming with machine learning algorithms.Fast.ai led by the amazing partnership of Jeremy Howard and Rachel Thomas. So when they released their machine learning course, I couldn’t wait to get started.What I personally liked about this course is the top-down approach to teaching. You first learn how to code an algorithm in Python, and then move to the theory aspect. While not a unique approach, it certainly has it’s advantages.While going these videos, I decided to curate my learning in the form of a series of articles for our awesome community! So in this first post, I have provided a comprehensive summary (including the code) of the first two videos where Jeremy Howard teaches us how to build a random forest model using the fastai library, and how tuning the different hyperparameters can significantly alter our model’s accuracy.You need to have a bit of experience in Python to follow along with the code. So if you’re a beginner in machine learning and have not used Python and Jupyter Notebooks before, I recommend checking out the below two resources first:Introduction to Data Science (covers the basics of Python, Statistics and Predictive Modeling)Beginner’s Guide to Jupyter NotebooksTable of contentsCourse Structure and MaterialsIntroduction to Machine Learning: Lesson 12.1 Importing Necessary Libraries2.2 Downloading the Dataset2.3 Introduction to Random Forest2.4 Preprocessing2.5 Model BuildingIntroduction to Machine Learning: Lesson 23.1 Creating a validation set3.2 Creating a single treeAdditional TopicsCourse Structure and MaterialsThe video lectures are available on YouTube and the course has been divided into twelve lectures as per the below structure:Lesson 1 — Introduction to Random ForestsLesson 2 — Random Forest Deep DiveLesson 3 — Performance, Validation and Model InterpretationLesson 4 — Feature Importance, Tree InterpreterLesson 5 — Extrapolation and RF from ScratchLesson 6 — Data Products and Live CodingLesson 7 — RF from Scratch and Gradient DescentLesson 8 — Gradient Descent and Logistic RegressionLesson 9 — Regularization, Learning Rates, and NLPLesson 10 — More NLP and Columnar DataLesson 11 — EmbeddingsLesson 12 — Complete Rossman, Ethical IssuesThis course assumes that you have Jupyter Notebook installed on your machine. In case you don’t (and don’t prefer installing it either), you can choose any of the following (these have a nominal fee attached to them):CrestlePaperspaceAll the Notebooks associated with each lecture are available on fast.ai’s GitHub repository. You can clone or download the entire repository in one go. You can locate the full installation steps under the to-install section.Introduction to Machine Learning: Lesson 1Ready to get started? Then check out the Jupyter Notebook and the below video for the first lesson.In this lecture, we will learn how to build a random forest model in Python. Since a top-down approach is followed in this course, we will go ahead and code first while simultaneously understanding how the code work. We’ll then look into the inner workings of the random forest algorithm.Let’s deep dive into what this lecture covers.Importing necessary libraries%load ext_autoreload%autoreload 2The above two commands will automatically modify the notebook when the source code is updated. Thus, using ext_autoreload will automatically and dynamically make the changes in your notebook.%matplotlib inlineUsing %matplotlib inline, we can visualize the plots inside the notebook.from fastai.imports import*from fastai.structured import *from pandas_summary import DataFrameSummaryfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifierfrom IPython.display import displayfrom sklearn import metricsUsing import* will import everything in the fastai library. Other necessary libraries have also been imported for reading the dataframe summary, creating random forest models and metrics for calculating the RMSE (evaluation metric).Downloading the DatasetThe dataset we’ll be using is the ‘Blue Book for Bulldozers’. The problem statement for this challenge is described below:The goal is to predict the sale price of a particular piece of heavy equipment at an auction, based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations. Fast Iron is creating a “blue book for bulldozers”, for customers to value what their heavy equipment fleet is worth at an auction.The evaluation metric is RMSLE (root mean squared log error). Don’t worry if you haven’t heard of it before, we’ll understand and deal with it during the code walk-through. Assuming you have successfully downloaded the dataset, let’s move on to coding!PATH = “data/bulldozers/”This command is used to set the location of our dataset. We currently have the downloaded dataset stored in a folder named bulldozers within the data folder. To check what are the files inside the PATH, you can type:!ls data/bulldozers/Or,!ls {PATH}Reading the filesThe dataset provided is in a .csv format. This is a structured dataset, with columns representing a range of things, such as ID, Date, state, product group, etc. For dealing with structured data, pandas is the most important library. We already imported pandas as pd when we used the import* command earlier. We will now use the read_csv function of pandas to read the data :df_raw = pd.read_csv(f’{PATH}Train.csv’, low_memory=False, parse_dates=[“saledate”])Let us look at the first few rows of the data:df_raw.head()Since the dataset is large, this command does not show us the complete column-wise data. Instead, we will see some dots for the data that isn’t being displayed (as shown in the screenshot):To fix this, we will define the following function, where we set max.rows and max.columns to 1000.def display_all(df):with pd.option_context(“display.max_rows”, 1000, display.max_columns&quot;, 1000):display(df)We can now print the head of the dataset using this newly minted function. We have taken the transpose to make it visually appealing (we see column names as the index).display_all(df_raw.head().transpose())Remember the evaluation metric is RMSLE — which is basically the RMSE between the log values of the result. So we will transform the target variable by taking it’s log values. This is where the popular library numpy comes to the rescue.df_raw.SalePrice = np.log(df_raw.SalePrice)Introduction to Random ForestThe concept of how a Random Forest model works from scratch will be discussed in detail in the later sections of the course, but here is a brief introduction in Jeremy Howard’s words:Random forest is a kind of universal machine learning techniqueIt can be used for both regression (target is a continuous variable) or classification (target is a categorical variable) problemsIt also works with columns of any kinds, like pixel values, zip codes, revenue, etc.In general, random forest does not overfit (it’s very easy to stop it from overfitting)You do not need a separate validation set in general. It can tell you how well it generalizes even if you only have one datasetIt has few (if any) statistical assumptions (it doesn’t assume that data is normally distributed, data is linear, or that you need to specify the interactions)Requires very few feature engineering tactics, so it’s a great place to start. For many different types of situations, you do not have to take the log of the data or multiply interactions togetherSounds like a smashing technique, right?RandomForestRegressor and RandomForestClassifier functions are used in Python for regression and classification problems respectively. Since we’re dealing with a regression challenge, we will stick to the RandomForestRegressor.m = RandomForestRegressor(n_jobs=-1) m.fit(df_raw.drop(‘SalePrice’, axis=1), df_raw.SalePrice)The m.fit function takes two inputs:Independent variablesDependent (target) variableThe target variable here is df_raw.SalePrice. The independent variables are all the variables except SalePrice. Here, we are using df_raw.drop to drop the SalePrice column (axis = 1 represents column). This would throw up an error like the one below:ValueError: could not convert string to float: ‘Conventional’This suggests that the model could not deal with the value ‘Conventional’. Most machine learning models (including random forest) cannot directly use categorical columns. We need to convert these columns into numbers first. So naturally the next step is to convert all the categorical columns into continuous variables.Data PreprocessingLet’s take each categorical column individually. First, consider the saledate column which is of datetime format. From the date column, we can extract numerical values such as — year, month, day of month, day of the week, holiday or not, weekend or weekday, was it raining?, etc.We’ll leverage the add_datepart function from the fastai library to create these features for us. The function creates the following features:‘Year’, ‘Month’, ‘Week’, ‘Day’, ‘Dayofweek’, ‘Dayofyear’, ‘Is_month_end’, ‘Is_month_start’, ‘Is_quarter_end’, ‘Is_quarter_start’, ‘Is_year_end’, ‘Is_year_start’Let’s run the function and check the columns:add_datepart(df_raw, ‘saledate’)df_raw.columnsoutput:Index([‘SalesID’, ‘SalePrice’, ‘MachineID’, ‘ModelID’, ‘datasource’, ‘auctioneerID’, ‘YearMade’, ‘MachineHoursCurrentMeter’, ‘UsageBand’, ‘fiModelDesc’, ‘fiBaseModel’, ‘fiSecondaryDesc’, ‘fiModelSeries’, ‘fiModelDescriptor’, ‘ProductSize’, ‘fiProductClassDesc’, ‘state’, ‘ProductGroup’, ‘ProductGroupDesc’, ‘Drive_System’, ‘Enclosure’, ‘Forks’, ‘Pad_Type’, ‘Ride_Control’, ‘Stick’, ‘Transmission’, ‘Turbocharged’, ‘Blade_Extension’, ‘Blade_Width’, ‘Enclosure_Type’, ‘Engine_Horsepower’, ‘Hydraulics’, ‘Pushblock’, ‘Ripper’, ‘Scarifier’, ‘Tip_Control’, ‘Tire_Size’, ‘Coupler’, ‘Coupler_System’, ‘Grouser_Tracks’, ‘Hydraulics_Flow’, ‘Track_Type’, ‘Undercarriage_Pad_Width’, ‘Stick_Length’, ‘Thumb’, ‘Pattern_Changer’, ‘Grouser_Type’, ‘Backhoe_Mounting’, ‘Blade_Type’, ‘Travel_Controls’, ‘Differential_Type’, ‘Steering_Controls’, ‘saleYear’, ‘saleMonth’, ‘saleWeek’, ‘saleDay’, ‘saleDayofweek’, ‘saleDayofyear’, ‘saleIs_month_end’, ‘saleIs_month_start’, ‘saleIs_quarter_end’, ‘saleIs_quarter_start’, ‘saleIs_year_end’, ‘saleIs_year_start’, ‘saleElapsed’], dtype=‘object’)The next step is to convert the categorical variables into numbers. We can use the train_cats function from fastai for this:train_cats(df_raw)While converting categorical to numeric columns, we have to take the following two issues into consideration:Some categorical variables can have an order among them (for example — High&gt;Medium&gt;Low). We can use set_categories to set the order.If a category gets a particular number in the train data, it should have the same value in the test data. For instance, if the train data has 3 for high and test data has 2, then it will have two different meanings. We can use apply_cats for validation and test sets to make sure that the mappings are the same throughout the different setsAlthough this won’t make much of a difference in our current case since random forest works on splitting the dataset (we will understand how random forest works in detail in the shortly), it’s still good to know this for other algorithms.Missing Value TreatmentThe next step is to look at the number of missing values in the dataset and understand how to deal with them. This is a pretty widespread challenge in both machine learning competitions and real-life industry problems.display_all(df_raw.isnull().sum().sort_index()/len(df_raw))We use .isnull().sum() to get the total number of missing values. This is divided by the length of the dataset to determine the ratio of missing values.The dataset is now ready to be used for creating a model. Data cleaning is always a tedious and time consuming process. Hence, ensure to save the transformed dataset so that the next time we load the data, we will not have to perform the above tasks again.We will save it in a feather format, as this let’s us access the data efficiently:#to saveos.makedirs(‘tmp’, exist_ok=True)df.to_feather(‘tmp/bulldozers-raw’)#to readdf_raw = pd.read_feather(‘tmp/bulldozers-raw’)We have to impute the missing values and store the data as dependent and independent part. This is done by using the fastai function proc_df. The function performs the following tasks:For continuous variables, it checks whether a column has missing values or notIf the column has missing values, it creates another column called columnname_na, which has 1 for missing and 0 for not missingSimultaneously, the missing values are replaced with the median of the columnFor categorical variables, pandas replaces missing values with -1. So proc_df adds 1 to all the values for categorical variables. Thus, we have 0 for missing while all other values are incremented by 1df, y, nas = proc_df(df_raw, ‘SalePrice’)Model BuildingWe have dealt with the categorical columns and the date values. We have also taken care of the missing values. Now we can finally power up and build the random forest model we have been inching towards.m = RandomForestRegressor(n_jobs=-1)m.fit(df, y)m.score(df,y)The n_jobs is set to -1 to use all the available cores on the machine. This gives us a score (r²) of 0.98, which is excellent. The caveat here is that we have trained the model on the training set, and checked the result on the same. There’s a high chance that this model might not perform as well on unseen data (test set, in our case).The only way to find out is to create a validation set and check the performance of the model on it. So let’s create a validation set that contains 12,000 data points (and the train set will contain the rest).def split_vals(a,n):return a[:n].copy(), a[n:].copy()n_valid = 12000 # same as Kaggle’s test set sizen_trn = len(df)-n_validraw_train, raw_valid = split_vals(df_raw, n_trn)X_train, X_valid = split_vals(df, n_trn)y_train, y_valid = split_vals(y, n_trn)X_train.shape, y_train.shape, X_valid.shapeOutput:((389125, 66), (389125,), (12000, 66))Here, we will train the model on our new set (which is a sample of the original set) and check the performance across both — train and validation sets.#define a function to check rmse valuedef rmse(x,y):return math.sqrt(((x-y)**2).mean())In order to compare the score against the train and test sets, the below function returns the RMSE value and score for both datasets.def print_score(m):res = [rmse(m.predict(X_train), y_train),rmse(m.predict(X_valid), y_valid),m.score(X_train, y_train), m.score(X_valid, y_valid)]if hasattr(m, ‘oob_score_’): res.append(m.oob_score_)print(res)m = RandomForestRegressor(n_jobs=-1)%time m.fit(X_train, y_train)print_score(m)The result of the above code is shown below. The train set has a score of 0.98, while the validation set has a score of 0.88. A bit of a drop-off, but the model still performed well overall.CPU times: user 1min 3s, sys: 356 ms, total: 1min 3sWall time: 8.46 s[0.09044244804386327, 0.2508166961122146, 0.98290459302099709, 0.88765316048270615]Introduction to Machine Learning: Lesson 2Now that you know how to code a random forest model in Python, it’s equally important to understand how it actually works underneath all that code. Random forest is often cited as a black box model, and it’s time to put that misconception to bed.We observed in the first lesson that the model performs extremely well on the training data (the points it has seen before) but dips when tested on the validation set (the data points model was not trained on). Let us first understand how we created the validation set and why it’s so crucial.Creating a Validation setCreating a good validation set that closely resembles the test set is one of the most important tasks in machine learning. The validation score is representative of how our model performs on real-world data, or on the test data.Keep in mind that if there’s a time component involved, then the most recent rows should be included in the validation set. So, our validation set will be of the same size as the test set (last 12,000 rows from the training data).def split_vals(a,n):return a[:n].copy(), a[n:].copy()n_valid = 12000n_trn = len(df)-n_validraw_train, raw_valid = split_vals(df_raw, n_trn)X_train, X_valid = split_vals(df, n_trn)y_train, y_valid = split_vals(y, n_trn)The data points from 0 to (length — 12000) are stored as the train set (x_train, y_train). A model is built using the train set and its performance is measured on both the train and validation sets as before.m = RandomForestRegressor(n_jobs=-1)%time m.fit(X_train, y_train)print_score(m)Result :CPU times: user 1min 3s, sys: 356 ms, total: 1min 3sWall time: 8.46 s[0.09044244804386327, 0.2508166961122146, 0.98290459302099709, 0.88765316048270615]From the above code, we get the results:RMSE on the training setRMSE on the validation setR-square on the training setR-square on validation setIt’s clear that the model is overfitting on the training set. Also, it takes a smidge over 1 minute to train. Can we reduce the training time? Yes, we can! To do this, we will further take a subset of the original dataset:df_trn, y_trn, nas = proc_df(df, ‘SalePrice’, subset=30000)X_train, _ = split_vals(df_trn, 20000)y_train, _ = split_vals(y_trn, 20000)A subset of 30,000 samples has been created from which we take 20,000 for training the Random Forest model.Building a single treeRandom Forest is a group of trees which are called estimators. The number of trees in a random forest model is defined by the parameter n_estimator. We will first look at a single tree (set n_estimator = 1) with a maximum depth of 3.m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)m.fit(X_train, y_train)print_score(m)Result:[0.4965829795739235, 0.5246832258551836, 0.50149617735615859, 0.5083655198087873]Plotting the tree:draw_tree(m.estimators_[0], df_trn, precision=3)The tree is a set of binary decisions. Looking at the first box, the first split is on coupler-system value: less than/equal to 0.5 or greater than 0.5. After the split, we get 3,185 rows with coupler_system&gt;0.5 and remaining 16,815 with &lt;0.5. Similarly, next split is on enclosure and Year_made.For the first box, a model is created using only the average value (10.189). This means that all the rows have a predicted value of 10.189 and the MSE (Mean Squared Error) for these predictions is 0.459. Instead, if we make a split and separate the rows based on coupler_system &lt;0.5, the MSE is reduced to 0.414 for samples satisfying the condition (true) and 0.109 for the remaining samples.So how do we decide which variable to split on? The idea is to split the data into two groups which are as different from each other as possible. This can be done by checking each possible split point for each variable, and then figuring out which one gives the lower MSE. To do this, we can take a weighted average of the two MSE values after the split. The splitting stops when it either reaches the pre-specified max_depth value or when each leaf node has only one value.We have a basic model — a single tree, but this is not a very good model. We need something a bit more complex that builds upon this structure. For creating a forest, we will use a statistical technique called bagging.Introduction to BaggingIn the bagging technique, we create multiple models, each giving predictions which are not correlated to the other. Then we average the predictions from these models. Random Forest is a bagging technique.If all the trees created are similar to each other and give similar predictions, then averaging these predictions will not improve the model performance. Instead, we can create multiple trees on a different subset of data, so that even if these trees overfit, they will do so on a different set of points. These samples are taken with replacement.In simple words, we create multiple poor performing models and average them to create one good model. The individual models must be as predictive as possible, but together should be uncorrelated. We will now increase the number of estimators in our random forest and see the results.m = RandomForestRegressor(n_jobs=-1)m.fit(X_train, y_train)print_score(m)If we do not give a value to the n_estimator parameter, it is taken as 10 by default. We will get predictions from each of the 10 trees. Further, np.stack will be used to concatenate the predictions one over the other.preds = np.stack([t.predict(X_valid) for t in m.estimators_])preds.shapeThe dimensions of the predictions is (10, 12000) . This means we have 10 predictions for each row in the validation set.Now for comparing our model’s results against the validation set, here is the row of predictions, the mean of the predictions and the actual value from validation set.preds[:,0], np.mean(preds[:,0]), y_valid[0]The actual value is 9.17 but none of our predictions comes close to this value. On taking the average of all our predictions we get 9.07, which is a better prediction than any of the individual trees.(array([ 9.21034, 8.9872 , 8.9872 , 8.9872 , 8.9872 , 9.21034, 8.92266, 9.21034, 9.21034, 8.9872 ]),9.0700003890739005,9.1049798563183568)It’s always a good idea to visualize your model as much as possible. Here is a plot that shows the variation in r² value as the number of trees increases.plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);As expected, the r² becomes better as the number of trees increases. You can experiment with the n_estimator value and see how the r² value changes with each iteration. You’ll notice that after a certain number of trees, the r² value plateaus.Out-of-Bag (OOB) ScoreCreating a separate validation set for a small dataset can potentially be a problem since it will result in an even smaller training set. In such cases, we can use the data points (or samples) which the tree was not trained on.For this, we set the parameter oob_score =True.m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)m.fit(X_train, y_train)print_score(m)[0.10198464613020647, 0.2714485881623037, 0.9786192457999483, 0.86840992079038759, 0.84831537630038534]The oob_score is 0.84 which is close to that of the validation set. Let us look at some other interesting techniques by which we can improve our model.SubsamplingEarlier, we created a subset of 30,000 rows and the train set was randomly chosen from this subset. As an alternative, we can create a different subset each time so that the model is trained on a larger part of the data.df_trn, y_trn, nas = proc_df(df_raw, ‘SalePrice’)X_train, X_valid = split_vals(df_trn, n_trn)y_train, y_valid = split_vals(y_trn, n_trn)set_rf_samples(20000)We use set_rf_sample to specify the sample size. Let us check if the performance of the model has improved or not.m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)m.fit(X_train, y_train)print_score(m)[0.2317315086850927, 0.26334275954117264, 0.89225792718146846, 0.87615150359885019, 0.88097587673696554]We get a validation score of 0.876. So far, we have worked on a subset of one sample. We can fit this model on the entire dataset (but it will take a long time to run depending on how good your computational resources are!).reset_rf_samples()m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)m.fit(X_train, y_train)print_score(m)[0.07843013746508616, 0.23879806957665775, 0.98490742269867626, 0.89816206196980131, 0.90838819297302553]Other Hyperparameters to Experiment with and TuneMin sample leafThis can be treated as the stopping criteria for the tree. The tree stops growing (or splitting) when the number of samples in the leaf node is less than specified.m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3,n_jobs=-1, oob_score=True)m.fit(X_train, y_train)print_score(m)[0.11595869956476182, 0.23427349924625201, 0.97209195463880227, 0.90198460308551043, 0.90843297242839738]Here we have specified the min_sample_leaf as 3. This means that the minimum number of samples in the node should be 3 for each split. We see that the r² has improved for the validation set and reduced on the test set, concluding that the model does not overfit on the training data.Max featureAnother important parameter in random forest is max_features. We have discussed previously that the individual trees must be as uncorrelated as possible. For the same, random forest uses a subset of rows to train each tree. Additionally, we can also use a subset of columns (features) instead of using all the features. This is achieved by tweaking the max_features parameter.m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)m.fit(X_train, y_train)print_score(m)[0.11926975747908228, 0.22869111042050522, 0.97026995966445684, 0.9066000722129437, 0.91144914977164715]Setting max_features has slightly improved the validation score. Here the max_features is set to 0.5 which means using 50% of the features for each split. Keep in mind that this parameter can also take values like log2 or sqrt.Additional TopicsTips and tricks in Jupyter NotebooksJeremy Howard mentioned a few tips and tricks for navigating Jupyter Notebooks which newcomers will find quite useful. Below are some of the highlights:To find out which library the function is located in, simply type the function name and run the cell (shift-enter):displayTo see the documentation, use a question mark before the function:?displayTo see the source code of the function, use a double question mark before the function name:??displayCurse of dimensionalityThe curse of dimensionality is the idea that the more dimensions we have, the more points sit on the edge of that space. So if the number of columns is more, it creates more and more empty space. What that means, in theory, is that the distance between points is much less meaningful. This should not be true because the points still are different distances away from each other. Even though they are on the edges, we can still determine how far away they are from each other.Continuous, categorical, ordinal variablesContinuous variables are variables with integer or float values. For example — Age, distance, weight, income etcCategorical variables are usually strings or values representing names or labels. For example — gender, state, zip code, rank etcOrdinal variables are those categorical variables which have an order among them. For example rank (I, II, III ) or remark (poor, good, excellent) have an order.Overfitting and UnderfittingUnderfitting: A model that performs poorly on the train data and also on the test data. The model does not generalize well. (left plot)Overfitting: A model that performs extremely well on the train data but does not show a similar high performance on the test data. (right plot)Root mean squared log errorThe evaluation metric of our dataset is RMSLE. The formula for this isWe first take the mean of squared differences of log values. We take a square root of the result obtained. This is equivalent to calculating the root mean squared error (rmse) of log of the values.R-squareHere is the mathematical formula for R-square:SSregression is the sum of squares of (actual value — prediction)SStotal is the sum of squares of (actual value — average)The value of R-square can be anything less than 1. If the r square is negative, it means that your model is worse than predicting mean.Extremely Randomized TreeIn scikit-learn, we have another algorithm ExtraTreeClassifier which is extremely randomized tree model. Unlike Random forest, instead of trying each split point for every variable, it randomly tries a few split points for a few variables.End NotesThis article was a pretty comprehensive summary of the first two videos from fast.ai’s machine learning course. During the first lesson we learnt to code a simple random forest model on the bulldozer dataset. Random forest (and most ml algorithms) do not work with categorical variables. We faced a similar problem during the random forest implementation and we saw hoe can we use the date column and other categorical columns in the dataset for creating a model.In the second video, the concept of creating a validation set was introduced. We then used this validation set to check the performance of the model and tuned some basic hyper-parameters to improve the model. My favorite part from this video was plotting and visualizing the tree we built. I am sure you would have learnt a lot through these videos. I will shortly post another article covering the next two videos from the course.Update: Here is part two of the series (Covers Lesson 3, 4 and 5)An Intuitive Guide to Interpret a Random Forest Model using fastai library (Machine learning for Programmers — Part 2)Originally published at www.analyticsvidhya.com on October 8, 2018.","categories":[{"name":"假设检验","slug":"假设检验","permalink":"http://www.wortyby.com/categories/假设检验/"}],"tags":[{"name":"自由度","slug":"自由度","permalink":"http://www.wortyby.com/tags/自由度/"},{"name":"原假设","slug":"原假设","permalink":"http://www.wortyby.com/tags/原假设/"},{"name":"对立假设","slug":"对立假设","permalink":"http://www.wortyby.com/tags/对立假设/"}]},{"title":"Python日常","slug":"Python 日常","date":"2018-11-27T05:49:57.000Z","updated":"2018-11-27T05:49:57.000Z","comments":true,"path":"2018/11/27/Python 日常/","link":"","permalink":"http://www.wortyby.com/2018/11/27/Python 日常/","excerpt":"","text":"前言Python中内置了4种数据类型，包括：list，tuple，set，dict，这些数据类型都有其各自的特点，但是这些特点（比如dict无序）在一定程度上对数据类型的使用产生了约束，在某些使用场景下效率会比较低，比如有时候我们可能需要维护一个有序的字典等情况。在这种场景下我们可以使用Python内建的collections模块，它包括了很多有用的集合类，合理的使用可以提高我们代码的运行效率。 举例接下来主要对collections模块中的常用集合类进行介绍，调用collections模块：1from collections import * defaultdictdict在使用时，当key值不存在时，直接添加value时会出现错误，使用defaultdict可以很好的规避该错误。defaultdict是对字典类型的补充，它可以给字典的值设置一个类型，当key不存在时可以自动生成相应类型的value。举例:12345678910111213141516from collections import defaultdicttest_data = ( (&apos;cat&apos;, 2), (&apos;dog&apos;, 5), (&apos;sheep&apos;, 3), (&apos;cat&apos;, 1), (&apos;sheep&apos;, 2))test_data_dict = defaultdict(list)for name, num in test_data: test_data_dict[name].append(num) print test_data_dict orderedDict在Python3.6之前的字典是无序的，但是有时候我们需要保持字典的有序性，orderDict可以在dict的基础上实现字典的有序性，这里的有序指的是按照字典key插入的顺序来排列，这样就实现了一个先进先出的dict，当容量超出限制时，先删除最早添加的key。举例：12345678#orderedDictoriginal_dict = &#123;&apos;a&apos;: 2, &apos;b&apos;: 4, &apos;c&apos;: 5&#125;for key, value in original_dict.items(): print key, valueordered_dict = OrderedDict([(&apos;a&apos;, 2), (&apos;b&apos;, 4), (&apos;c&apos;, 5)])for key, value in ordered_dict.items(): print key, value可以看到orderDict是按照字典创建时的插入顺序来排序。 dequePython中的list是基于数组实现的，所以，查找容易，但是插入和删除操作时间复杂度较大。deque就是为了高效实现插入和删除操作的双向列表，适合用于队列和栈，而且线程安全。list只提供了append和pop方法来从list的尾部插入或者删除元素，deque新增了appendleft/popleft等方法可以更高效的在元素的开头来插入/删除元素。举例：1234567from collections import dequed = deque([1,2,3,4,5])d.extendleft([0])print dd.extend([6,7])d.popleft()print d可以进行双向操作元素，十分方便。 Counter字典子类，为可以哈希的对象计数。举例：123456789from collections import Countertest_counter_data = [&apos;cat&apos;, &apos;dog&apos;, &apos;sheep&apos;, &apos;cat&apos;, &apos;dog&apos;]counter_data = Counter()for item in test_counter_data: counter_data[item] += 1 print counter_data可以实现对一个对象中的元素进行计数。 namedtuple元组子类。我们知道，Python中元组的一个重要特征就是元素不可增删改，而查找tuple元素时一般采取索引。使用namedtuple(typename, field_name)可以命名tuple中的元素，之后便可使用名字来查找tuple中的值，有点类似于字典中的查找。举例：1234from collections import namedtupleanimal = namedtuple(&apos;animal&apos;, &apos;type age&apos;)mark = animal(type=&apos;dog&apos;, age=2)print mark.type使用namedtuple可以提高代码的可读性和文档性。","categories":[{"name":"Python 语言","slug":"Python-语言","permalink":"http://www.wortyby.com/categories/Python-语言/"}],"tags":[{"name":"collections","slug":"collections","permalink":"http://www.wortyby.com/tags/collections/"}]},{"title":"Data Vault 初探","slug":"Data Vault 初探","date":"2018-11-23T05:29:07.000Z","updated":"2018-11-23T05:29:07.000Z","comments":true,"path":"2018/11/23/Data Vault 初探/","link":"","permalink":"http://www.wortyby.com/2018/11/23/Data Vault 初探/","excerpt":"","text":"**Data Vault（DV）**模型是用于企业级的数据仓库建模。由Dan Linstedt在20世纪90年代提出。在最近几年，Data Vault模型获得了很多关注，并在BI社区里拥有了一批追随者。 定义Dan Linstedt将Data Vault模型定义如下：Data Vault是面向细节的，可追踪历史的，它是一组有连接关系的规范化的表的集合。这些表可以支持一个或多个业务功能，它是一种综合了第三范式（3NF）和星型模型优点的建模方法。其设计理念是要满足企业对灵活性、可扩展性、一致性和对需求的适应性要求，它是一种专为企业级数据仓库量身定制的建模方式。从上面的定义，可以看出Data Vault既是一种数据建模的方法论，又是构建企业数据仓库的一种具体方法。 组成Data Vault模型由三个模块组成，中心表、链接表、附属表。建模方法论里定义了Data Vault的组成部分和组成部分之间的交互方式。 最佳实践Data Vault的建模方法中还包括了 最佳实践，来指导构建企业数据仓库。例如，业务规则应该在数据的下游实现，就是说Data Vault只按照业务数据的原样保存数据，不做任何解释、过滤、清洗、转换。即使从不同数据源来的数据是自行矛盾的（例如同一个客户有不同的地址），Data Vault模型不会遵照任何业务的规则，如“系统A的地址为准”。Data Vault模型会保存两个不同版本的数据，对数据的解释将推迟到整个架构的后一个阶段 (数据集市)。原文链接","categories":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://www.wortyby.com/categories/数据仓库/"}],"tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://www.wortyby.com/tags/数据仓库/"}]},{"title":"卡方检验","slug":"卡方检验","date":"2018-11-15T06:37:14.000Z","updated":"2018-11-15T06:37:14.000Z","comments":true,"path":"2018/11/15/卡方检验/","link":"","permalink":"http://www.wortyby.com/2018/11/15/卡方检验/","excerpt":"","text":"The P value, or calculated probability, is the probability of finding the observed, or more extreme, results when the null hypothesis (H0) of a study question is true – the definition of ‘extreme’ depends on how the hypothesis is being tested. P is also described in terms of rejecting H0 when it is actually true, however, it is not a direct probability of this state.The null hypothesis is usually an hypothesis of “no difference” e.g. no difference between blood pressures in group A and group B. Define a null hypothesis for each study question clearly before the start of your study.The only situation in which you should use a one sided P value is when a large change in an unexpected direction would have absolutely no relevance to your study. This situation is unusual; if you are in any doubt then use a two sided P value.The term significance level (alpha) is used to refer to a pre-chosen probability and the term “P value” is used to indicate a probability that you calculate after a given study.The alternative hypothesis (H1) is the opposite of the null hypothesis; in plain language terms this is usually the hypothesis you set out to investigate. For example, question is “is there a significant (not due to chance) difference in blood pressures between groups A and B if we give group A the test drug and group B a sugar pill?” and alternative hypothesis is &quot; there is a difference in blood pressures between groups A and B if we give group A the test drug and group B a sugar pill&quot;.If your P value is less than the chosen significance level then you reject the null hypothesis i.e. accept that your sample gives reasonable evidence to support the alternative hypothesis. It does NOT imply a “meaningful” or “important” difference; that is for you to decide when considering the real-world relevance of your result.The choice of significance level at which you reject H0 is arbitrary. Conventionally the 5% (less than 1 in 20 chance of being wrong), 1% and 0.1% (P &lt; 0.05, 0.01 and 0.001) levels have been used. These numbers can give a false sense of security.In the ideal world, we would be able to define a “perfectly” random sample, the most appropriate test and one definitive conclusion. We simply cannot. What we can do is try to optimise all stages of our research to minimise sources of uncertainty. When presenting P values some groups find it helpful to use the asterisk rating system as well as quoting the P value:P &lt; 0.05 *P &lt; 0.01 **P &lt; 0.001Most authors refer to statistically significant as P &lt; 0.05 and statistically highly significant as P &lt; 0.001 (less than one in a thousand chance of being wrong).The asterisk system avoids the woolly term “significant”. Please note, however, that many statisticians do not like the asterisk rating system when it is used without showing P values. As a rule of thumb, if you can quote an exact P value then do. You might also want to refer to a quoted exact P value as an asterisk in text narrative or tables of contrasts elsewhere in a report.At this point, a word about error. Type I error is the false rejection of the null hypothesis and type II error is the false acceptance of the null hypothesis. As an aid memoir: think that our cynical society rejects before it accepts.The significance level (alpha) is the probability of type I error. The power of a test is one minus the probability of type II error (beta). Power should be maximised when selecting statistical methods. If you want to estimate sample sizes then you must understand all of the terms mentioned here.The following table shows the relationship between power and error in hypothesis testing:123456789101112131415 DECISIONTRUTH Accept H0: Reject H0: H0 is true: correct decision P type I error P 1-alpha alpha (significance) H0 is false: type II error P correct decision P beta 1-beta (power) H0 = null hypothesis P = probabilityIf you are interested in further details of probability and sampling theory at this point then please refer to one of the general texts listed in the reference section.You must understand confidence intervals if you intend to quote P values in reports and papers. Statistical referees of scientific journals expect authors to quote confidence intervals with greater prominence than P values.Notes about Type I error:is the incorrect rejection of the null hypothesismaximum probability is set in advance as alphais not affected by sample size as it is set in advanceincreases with the number of tests or end points (i.e. do 20 rejections of H0 and 1 is likely to be wrongly significant for alpha = 0.05)Notes about Type II error:is the incorrect acceptance of the null hypothesisprobability is betabeta depends upon sample size and alphacan’t be estimated except as a function of the true population effectbeta gets smaller as the sample size gets largerbeta gets smaller as the number of tests or end points increases","categories":[{"name":"假设检验","slug":"假设检验","permalink":"http://www.wortyby.com/categories/假设检验/"}],"tags":[{"name":"自由度","slug":"自由度","permalink":"http://www.wortyby.com/tags/自由度/"},{"name":"原假设","slug":"原假设","permalink":"http://www.wortyby.com/tags/原假设/"},{"name":"对立假设","slug":"对立假设","permalink":"http://www.wortyby.com/tags/对立假设/"}]},{"title":"特征工程","slug":"要看的文章","date":"2018-11-13T05:44:05.000Z","updated":"2018-11-13T05:44:05.000Z","comments":true,"path":"2018/11/13/要看的文章/","link":"","permalink":"http://www.wortyby.com/2018/11/13/要看的文章/","excerpt":"","text":"特征工程是一个非常重要的课题，是机器学习中不可缺少的一部分，但是它几乎很少出现于机器学习书本里面的某一章。在机器学习方面的成功很大程度上在于如果使用特征工程。 特征工程可以解决什么样的问题？在机器学习中，经常是用一个预测模型（线性回归，逻辑回归，SVD等）和一堆原始数据来得到一些预测的结果，人们需要做的是从这堆原始数据中去提炼较优的结果，然后做到最优的预测。这个就包括两个方面，第一就是如何选择和使用各种模型，第二就是怎么样去使用这些原始的数据才能达到最优的效果。那么怎么样才能够获得最优的结果呢？贴上一句经典的话就是：Actually the sucess of all Machine Learning algorithms depends on how you present the data.------ Mohammad Pezeshki直接翻译过来便是：事实上所有机器学习算法上面的成功都在于你怎么样去展示这些数据。由此可见特征工程在实际的机器学习中的重要性，从数据里面提取出来的特征好坏与否就会直接影响模型的效果。从某些层面上来说，所使用的特征越好，得到的效果就会越好。所需要的特征就是可以借此来描述已知数据的内在关系。总结一下就是：Better feature means flexibility. Better feature means simpler models. Better feature means better results.有的时候，可以使用一些不是最优的模型来训练数据，如果特征选择得好的话，依然可以得到一个不错的结果。很多机器学习的模型都能够从数据中选择出不错的结构，从而进行良好的预测。一个优秀的特征具有极强的灵活性，可以使用不那么复杂的，运算速度快，容易理解和维护的模型来得到不错的结果。 什么才是特征工程？Feature Engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.------ Jason BrownleeFeature Engineering is manually designing what the input x’s should be.------ Tomasz Malisiewicz从这个概念可以看出，特征工程其实是一个如何展示和表现数据的问题，在实际工作中需要把数据以一种“良好”的方式展示出来，使得能够使用各种各样的机器学习模型来得到更好的效果。如何从原始数据中去除不佳的数据，展示合适的数据就成为了特征工程的关键问题。 特征有用性的预估每次构造了一个特征，都需要从各个方面去证明该特征的有效性。一个特征是否重要主要在于该特征与要预测的东西是否是高度相关的，如果是高度相关，那么该特征就是十分重要的。比如常用的工具就是统计学里面的相关系数。 特征的构造过程在实际工作中首先肯定要确定具体的问题，然后就是数据的选择和准备过程，再就是模型的准备和计算工作，最后才是展示数据的预测结果。构造特征的一般步骤： 任务的确定：根据具体的业务确定需要解决的问题； 数据的选择：整合数据，收集数据； 预处理数据：设计数据展现的格式，清洗数据，选择合适的样本使得机器学习模型能够使用它。比方说一些年龄特征是空值或者负数或者大于200等，或者说某个页面的播放数据大于曝光数据，这些就是数据的不合理，需要在使用之前把这一批数据排除掉。 特征的构造：转化数据，使之成为有效的特征。常用的方法是标准化，归一化，特征的离散化等。 标准化（Standardization）：比方说有一些数字的单位是千克，有一些数字的单位是克，这个时候需要统一单位。如果没有标准化，两个变量混在一起搞，那么肯定就会不合适。 归一化（Normalization）：归一化是因为在特征会在不同的尺度下有不同的表现形式，归一化会使得各个特征能够同时以恰当的方式表现。比方说某个专辑的点击播放率一般不会超过0.2，但是专辑的播放次数可能会达到几千次，所以说为了能够在模型里面得到更合适结果，需要先把一些特征在尺度上进行归一化，然后进行模型训练。 特征的离散化（Discretization）：离散化是指把特征进行必要的离散处理，比方说年龄特征是一个连续的特征，但是把年龄层分成5－18岁（中小学生），19－23岁（大学生），24－29岁（工作前几年），30－40岁（成家立业），40－60岁（中年人）从某些层面来说比连续的年龄数据（比如说某人年龄是20岁1月3日之类的）更容易理解不同年龄层人的特性。 典型的离散化步骤：对特征做排序;选择合适的分割点;作出区间的分割;作出区间分割;查看是否能够达到停止条件。 模型的使用：创造模型，选择合适的模型，用合适的模型来进行预测，用各种统计指标来判断该特征是否合适； 上线的效果：通过在线测试来看效果。数据的转换（Transforming Data）就是把数据从原始的数据状态转换成适合模型计算的状态，从某些层面上来说，“数据转换“和”特征构造“的过程几乎是一致的。 特征工程的迭代过程 特征工程的迭代步骤： 选择特征：需要进行头脑风暴（brainstorm）。通过具体的问题分析，查看大量的数据，从数据中查看出可以提取出数据的关键； 设计特征：这个需要具体问题具体分析，可以自动进行特征提取工作，也可以进行手工进行特征的构造工作，甚至混合两种方法； 选择特征：使用不同的特征构造方法，来从多个层面来判断这个特征的选择是否合适； 计算模型：通过模型计算得到模型在该特征上所提升的准确率。 上线测试：通过在线测试的效果来判断特征是否有效。参考文章","categories":[{"name":"Scklearn","slug":"Scklearn","permalink":"http://www.wortyby.com/categories/Scklearn/"}],"tags":[{"name":"特征","slug":"特征","permalink":"http://www.wortyby.com/tags/特征/"}]},{"title":"基于Spark GraphX，弃GBDT和LR用FM","slug":"基于Spark GraphX，弃GBDT和LR用FM","date":"2018-11-07T08:54:30.000Z","updated":"2018-11-07T08:54:30.000Z","comments":true,"path":"2018/11/07/基于Spark GraphX，弃GBDT和LR用FM/","link":"","permalink":"http://www.wortyby.com/2018/11/07/基于Spark GraphX，弃GBDT和LR用FM/","excerpt":"","text":"世纪佳缘推荐场景先说一下我们的推荐场景。我们使用推荐的场景跟电影、商品推荐有很大的不同，商品的推荐可能只考虑到转化就可以了，我们要考虑推荐链的更长一些。我们的情况：用户登录网站，算法推荐出用户可能感兴趣的人，用户发信，收信用户看信。最大的不同点在于，我们的item也是人，设计算法时也要考虑item的感受。拿亚马逊来类比，亚马逊可能只需要考虑把一本书推荐给某个人，我们还要考虑这本书是不是想被推荐给这个人。举一个极端的例子，如果我们只是想提高男性用户发信，我们只需要给所有人推荐美女，这样发信量一定会暴涨。但是这样做会引发很多问题：1)美女一天不可能读上千封信；2)美女对于一些条件不好的男性根本没兴趣；３)非美女收不到信。所以无论是在产生候选，还是在排序的时候，我们都要同时考虑user和item。以上是我们在推荐场景上比较特殊的地方。 基于图算法产生候选集下面我主要说两个主题，先说我们如何产生推荐。今天主要说一下基于图的算法，我们的图算法是在Spark上实现的，使用用户历史发信数据，计算得到用户的推荐列表。（世纪佳缘对Spark的理解，可以参考这个文档：世纪佳缘吴金龙：Spark介绍——编辑注）我们的数据很稀疏，在图算法中，对于数据比较多的用户使用一跳节点，对于数据少的用户使用二跳甚至三跳节点的数据，这样可以避开CF中计算相似度的问题，对数据少的用户也能产生足够多的推荐。当时遇到的主要问题是数据：一是数据太大，二是数据质量不高。对那些出度（发信）和入度（收信）很多的点都要想办法进行抽样，否则会在计算时会耗尽内存。抽样时也尽量提高数据质量。对于收信很多的用户，抽样方法很简单，留下用户看了的信，去掉没看过的。对于发信的抽样，当然也可以只留下被看过的信，但是种方法有一点讲不通，因为看不看信是站在item的角度考虑的，对发信抽样，应该站在user的角度。我们尝试了直接扔掉发信过多的用户，随机抽样，保留最近数据几种方法。但是这样的方法都是很盲目的。对于发信而言，最主要的度量应该是：发信那一刻，用户是不是认真的。如果用户发信是很随意的，没有经过筛选，那这个数据其实没有多大意义。解决这个问题当然也可以做个模型，但是太复杂了，我们使用了一种简单的方法：根据用户的发信间隔判断，比如这封信与上一封信时间间隔10s以上，我们就认为用户在用心的发信，保留这个数据，否则扔掉。方法很简单，但是最终线上的效果却很好。我取了一个发信很随意的用户数据，统计了收信人在某个维度的分布，如下图所示。以下则是一个认真发信的用户，判断标准就是刚才说的时间间隔。可以明显看出来，认真发信的用户，只给符合自己要求的人发信，所以分布会更集中一些。但是不认真的用户，分布就很散了。 排序算法的实现第二个主题，说一下排序。主要是对上边产生的候选集排序后把最终结果展示给用户。用到的算法也是CTR预测中常用到的LR、FM、GBDT等。前边说过，我们除了要考虑user发信，还要考虑item会不会看信，甚至item会不会回信。因此一次排序通常会组合好几个模型：点击模型预测user从展示里点item的概率，发信模型预测发信概率，读信模型预测item读信概率。然后对这几种概率做个组合后得到最终的权重值。用到的工具，数据怎么处理，都跟大家一样，没什么要说的，这里重点提一个我们用到的评估方法。大致就是上图这条曲线，横座标是预测概率的一个区间，纵座标是在这个区间内真实值的平均。比如图中(0.45, 0.46)的点，计算方法是：取所有预测值在[0.445, 0.455)的真实值，记为X；avg(X)为纵座标值；这条曲线最完美的情况应该是只有(0, 0)，(1, 1)点有值，当然算法不可能做到这样。次好的情况，画出来的线应该与 y=x 重合。之前说过，我们的排序是把多个算法出来的概率值作组合，所以对每个算法的效果除了要求排序正确，还希望预测的概率值也尽量接近真实值。ROC、NDCG 可以很好的度量排序，但是没法度量真实值与预测值的偏差，这就是为什么我们特别关注这条线。 经验心得最后说一下我们几次算法尝试时遇到的问题。1.测试Facebook论文中提到的用GBDT提取特征的方法。当时为了方便，我们直接把给LR的特征作为GBDT的特征，然后把得到的叶子节点作为特征，与原来的特征组合到一起再扔给LR。（可以参考这篇博客：CTR预估中GBDT与LR融合方案——编辑注）线下效果和线上效果都有提升，我们推广了这个方法，但是发现其中一个模型没有任何效果。排查问题的时候发现，这个模型对所有特征作了离散化，出来的特征值全部非0即1。GBDT本来就是个树模型，能很好的处理非线性特征，使用离散化后的特征反而没什么效果。而且对于这种只有0、1值的情况，GBDT出现了不收敛的现象。2.不同的场景，使用不同的算法。LR是我们最常使用的，所以在做点击模型时，自然也是先上了LR，但是线上效果并不好。后来上FM，效果却好的出奇。如果登录过我们的网站，很容易发现原因：展示的场景，只能看到头像、地区、年龄等几个属性，LR使用了大量用户看不到的特征，这些特征对于模型来说是没有意义的。That’s all，我今天分享就到这里了。 Q&amp;A问：能否再详细介绍一下推荐系统所采用的工具？答：R的话，我们主要用Liblinear；FM主要用SVDFeature；数据主要放Hive；Redis、Memcache和MongoDB都会用到；实时计算，会对接Kafka。问：不实时计算可以吗？哪些方面是需要实时计算？答：最主要的计算，还是离线算好的。比如实时的排序，因为推荐列表动态生成，排序只能做成实时的。问：请问主要的算法用得什么语言开发的？答：线上Java，线下的代码就很随意了，Python/Java/Shell/Hive，什么方便用什么。问：做算法时，你觉的最大的障碍是啥？如何解决这些障碍？可以谈谈具体实现上遇到的一些困难。答：很多时候，一个模型效果不好，但是却不知道从哪里着手改进。不知道加什么样的特征会有效，换模型也没有效果，试过了能想到的所有方法。问：对数学要求高吗？答：我自己主要做开发工作，数学有要求，但是不是那么高，能看懂论文就好。问：杨老师，你们对用户的习惯有研究吗？如何做的？答：用户习惯，我们主要会统计一些数据，比如用户经常给多少岁的人发信，然后把这个统计作为特征放到模型里。问：请问下，在使用算法的过程中，对于对应算法的具体推演过程你们需要全部理解透么。感觉有时候一个算法对数学的要求好高啊，想理清缘由难度蛮大的。答：不需要全部理解透，至少我看了很多遍EM的推导，现在推，我还是不会。但是我知道EM推导大概怎么回事。问：一般你们怎么样从为解决某个问题，而选择需要利用哪些维度，然后出发去构建模型的？答：这个主要还是个人经验，做的多了，很容易就能找到最有效的特征。问：模型是基于已有的一些文章的还是在实验过程中改进的，有专门的算法部？答：模型主要还是基于已有的，除非很不符合我们的需要，否则很少去改。问：一些常用算法的推导，特性，用法都的知道吗？答：常用算法肯定是要知道的。问：杨老师，你刚刚学习的时候，从哪个简单示例或者文档入手的？答：《集体智慧编程》。问：能否介绍佳缘的推荐目前的实际效果，下一步的改进方向？答：分算法和场景，整体上看，如果原来什么算法都没有，可能会有50%左右的提升。下一步的方向，主要是具体细分用户，或者从其它维度细分算法。之前的只关注了按场景细分，以后细分的维度会拓宽些。问：请问您认为应届生应该怎样往机器学习方向发展呢？答：环境是最重要的，如果真想做，找个真正做这个的公司。我一直想做游戏，到现在都没做成[Smile]。原文链接参考文章","categories":[{"name":"Spark","slug":"Spark","permalink":"http://www.wortyby.com/categories/Spark/"}],"tags":[{"name":"FM","slug":"FM","permalink":"http://www.wortyby.com/tags/FM/"},{"name":"Graph","slug":"Graph","permalink":"http://www.wortyby.com/tags/Graph/"},{"name":"LR","slug":"LR","permalink":"http://www.wortyby.com/tags/LR/"},{"name":"GBDT","slug":"GBDT","permalink":"http://www.wortyby.com/tags/GBDT/"},{"name":"数据特征少的状况","slug":"数据特征少的状况","permalink":"http://www.wortyby.com/tags/数据特征少的状况/"}]},{"title":"Custom Email Alerts in Airflow","slug":"Custom Email Alerts in Airflow","date":"2018-11-06T09:14:44.000Z","updated":"2018-11-06T09:14:44.000Z","comments":true,"path":"2018/11/06/Custom Email Alerts in Airflow/","link":"","permalink":"http://www.wortyby.com/2018/11/06/Custom Email Alerts in Airflow/","excerpt":"","text":"Apache Airflow is great for coordinating automated jobs, and it provides a simple interface for sending email alerts when these jobs fail. Typically, one can request these emails by setting email_on_failure to True in your operators.These email alerts work great, but I wanted to include additional links in them (I wanted to include a link to my spark cluster which can be grabbed from the Databricks Operator). Here’s how I created a custom email alert on job failure.First, I set email_on_failure to False and use the operators’s on_failure_callback. I give on_failure_callback the function described below.12345678910111213141516171819from airflow.utils.email import send_emaildef notify_email(contextDict, **kwargs): &quot;&quot;&quot;Send custom email alerts.&quot;&quot;&quot; # email title. title = &quot;Airflow alert: &#123;task_name&#125; Failed&quot;.format(**contextDict) # email contents body = &quot;&quot;&quot; Hi Everyone, &lt;br&gt; &lt;br&gt; There&apos;s been an error in the &#123;task_name&#125; job.&lt;br&gt; &lt;br&gt; Forever yours,&lt;br&gt; Airflow bot &lt;br&gt; &quot;&quot;&quot;.format(**contextDict) send_email(&apos;you_email@address.com&apos;, title, body)send_email is a function imported from Airflow. contextDict is a dictionary given to the callback function on error. Importantly, contextDict contains lots of relevant information. This includes the Task Instance (key=’ti’) and Operator Instance (key=’task’) associated with your error. I was able to use the Operator Instance, to grab the relevant cluster’s address and I included this address in my email (this exact code is not present here).To use the notify_email, I set on_failure_callback equal to notify_email.I write out a short example airflow dag below.1234567891011121314151617181920212223from airflow.models import DAGfrom airflow.operators import PythonOperatorfrom airflow.utils.dates import days_agoargs = &#123; &apos;owner&apos;: &apos;me&apos;, &apos;description&apos;: &apos;my_example&apos;, &apos;start_date&apos;: days_ago(1)&#125;# run every day at 12:05 UTCdag = DAG(dag_id=&apos;example_dag&apos;, default_args=args, schedule_interval=&apos;0 5 * * *&apos;)def print_hello(): return &apos;hello!&apos;py_task = PythonOperator(task_id=&apos;example&apos;, python_callable=print_hello, on_failure_callback=notify_email, dag=dag)py_taskNote where set on_failure_callback equal to notify_email in the PythonOperator.Hope you find this helpful! Don’t hesitate to reach out if you have a question.","categories":[{"name":"workflow","slug":"workflow","permalink":"http://www.wortyby.com/categories/workflow/"}],"tags":[{"name":"airflow","slug":"airflow","permalink":"http://www.wortyby.com/tags/airflow/"},{"name":"Email","slug":"Email","permalink":"http://www.wortyby.com/tags/Email/"},{"name":"通知","slug":"通知","permalink":"http://www.wortyby.com/tags/通知/"}]},{"title":"airflow之DAGs详解","slug":"airflow之DAGs详解","date":"2018-11-05T08:24:01.000Z","updated":"2018-11-05T08:24:01.000Z","comments":true,"path":"2018/11/05/airflow之DAGs详解/","link":"","permalink":"http://www.wortyby.com/2018/11/05/airflow之DAGs详解/","excerpt":"","text":"前言启动dag调度器, 注意启动调度器, 并不意味着dag会被马上触发, dag触发需要符合它自己的schedule规则如果缺省了 END_DATE 参数, END_DATE 等同于START_DATE.使用 DummyOperator 来汇聚分支使用 ShortCircuitOperator/BranchPythonOperator 做分支使用 SubDagOperator 嵌入一个子dag使用 TriggerDagRunOperator 直接 trigger 另一个dag在创建 MyBashOperator 的实例时候, 为on_failure_callback 和on_success_callback 参数设置两个回调函数, 我们在回调函数中, 将success或failed状态记录到自己的表中.DAG的schedule_interval参数设置成None, 表明这个DAG始终是由外部触发。如果将default_args字典传递给DAG，DAG将会将字典应用于其内部的任何Operator上。这很容易的将常用参数应用于多个Operator，而无需多次键入。1234567default_args=dict( start_date=datetime(2016, 1, 1), owner=&apos;Airflow&apos;)dag = DAG(&apos;my_dag&apos;, default_args=default_args)op = DummyOperator(task_id=&apos;dummy&apos;, dag=dag)print(op.owner) # Airflow Airflow 命令initdb，初始化元数据DB，元数据包括了DAG本身的信息、运行信息等；resetdb，清空元数据DB；list_dags，列出所有DAG；list_tasks，列出某DAG的所有task；test，测试某task的运行状况；backfill，测试某DAG在设定的日期区间的运行状况；webserver，开启webserver服务；scheduler，用于监控与触发DAG。12345678910111213141516171819202122$ cd $&#123;AIRFLOW_HOME&#125;/dags $ python test_import.py # 保证代码无语法错误 $ airflow list_dags # 查看dag是否成功加载 airflow list_tasks test_import_dag –tree # 查看dag的树形结构是否正确 $ airflow test test_import_dag \\ test_import_task 2016-3-7 # 测试具体的dag的某个task在某个时间的运行是否正常 $ airflow backfill test_import_dag -s 2016-3-4 \\ -e 2016-3-7 # 对dag进行某段时间内的完整测试# print the list of active DAGsairflow list_dags# prints the list of tasks the &quot;tutorial&quot; dag_idairflow list_tasks tutorial# prints the hierarchy of tasks in the tutorial DAGairflow list_tasks tutorial --tree 使用supervisord进行deamonairflow本身没有deamon模式，所以直接用supervisord就ok了，我们只要写4行代码12345[program:airflow_web]command=/home/kimi/env/athena/bin/airflow webserver -p 8080 [program:airflow_scheduler]command=/home/kimi/env/athena/bin/airflow scheduler请注意，airflow test命令在本地运行任务实例，将其日志输出到stdout（屏幕上），不会影响依赖关系，并且不会将状态（运行，成功，失败，…）发送到数据库。 它只是允许简单的测试单个任务实例。如果使用depends_on_past = True，则单个任务实例将取决于上一个任务实例的成功与否，如果指定本身的start_date，则忽略此依赖关系12# start your backfill on a date rangeairflow backfill tutorial -s 2015-06-01 -e 2015-06-07 通过Xcom 在 task 之间传参可以直接使用jinja模板语言，在 模板变量***ti.xcom_push | ti.xcom_pull*** 中调用ti的 xcom_push 和 xcom_pull 方法，下面的例子为 t1 使用 xcom_push 推出了一个 kv，t2 通过 taskid 和 key 来接收.12345678910111213dag = DAG( dag_id=&apos;xcomtest&apos;, default_args=default_args, schedule_interval=&apos;*/2 * ** *&apos;) t1 = BashOperator( task_id=&apos;xcom&apos;, bash_command=&apos;&apos;&apos;&apos;&apos;&#123;&#123; ti.xcom_push(key=&apos;aaa&apos;, value=&apos;bbb&apos;) &#125;&#125;&apos;&apos;&apos;, dag=dag) t2 = BashOperator( task_id=&apos;xcom2&apos;, bash_command=&apos;&apos;&apos;&apos;&apos;echo&quot;&#123;&#123; ti.xcom_pull(key=&apos;aaa&apos;, task_ids=&apos;xcom&apos;) &#125;&#125;&quot; &apos;&apos;&apos;, dag=dag) t2.set_upstream(t1)airflow 提供了很多 Macros Variables，可以直接使用 jinja 模板语言调用宏变量12345678910111213templated_command = &quot;&quot;&quot;echo &quot;dag_run:&#123;&#123; dag_run &#125;&#125;&quot;echo &quot;run_id:&#123;&#123; run_id &#125;&#125;&quot;echo &quot;execution_date:&#123;&#123; execution_date &#125;&#125;&quot;echo &quot;ts:&#123;&#123; ts &#125;&#125;&quot;echo &quot;ti:&#123;&#123; ti &#125;&#125;&quot;sleep 3&quot;&quot;&quot; t1 = BashOperator( task_id=&apos;xcom&apos;, bash_command=templated_command, dag=dag)execution_date 并不是 task 的真正执行时间，而是上一周期 task 的执行时间。我们在 airflow 上看到一个任务是 6am 执行的，而且 interval=4hours ，那么execution_date 的值是 2am，而不是 6am暂时无法 ***hold***或***pause***某个task，只支持以 dag 为单位 pause当使用BashOperator时，command需要调用脚本时，脚本后需要有个空格，否则报错，暂时不清楚原因，但加空格后可以正常执行，如下例，run.sh后需加空格1234t1 = BashOperator( task_id=&apos;process_rankinglist&apos;, bash_command=&apos;/home/rankinglist_processor/run.sh &apos;, dag=dag)Airflow为Operator提供许多常见任务，包括：BashOperator - 执行bash命令PythonOperator - 调用任意的Python函数EmailOperator - 发送邮件HTTPOperator - 发送 HTTP 请求SqlOperator - 执行 SQL 命令Sensor - 等待一定时间，文件，数据库行，S3键等…除了这些基本的构建块之外，还有更多的特定Operator:DockerOperator，HiveOperator，S3FileTransferOperator，PrestoToMysqlOperator，SlackOperator 宏airflow 执行的命令或这种消息是支持 jinja2 模板语言；是一种宏，表示当前的日期，形如2016-12-16，支持的宏在https://airflow.incubator.apache.org/code.html#macros 工作流部署相关 关于airflow的环境变量的设置在BashOperator的env(dict类型)参数中添加环境变量，当然也可以在声明DAG时的default_args中添加env的声明，但需要注意，如果设置了env，airflow就不再访问系统的环境变量，所以这里设置的env一定要包含程序运行所需的所有环境变量，否则会出错123456import oslocal_env = os.environlocal_env[&apos;PATH&apos;] = os.environ[&apos;PATH&apos;] + &quot;:&quot; + Variable.get(&apos;PATH&apos;)local_env[&apos;JAVA_HOME&apos;] = Variable.get(&apos;JAVA_HOME&apos;) 在dag的default_args中添加&apos;env&apos;:dict(local_env) mark_successmark_success当task执行完成，但只是返回值为失败的时候，可以不rerun该task，而是marksuccess，然后继续执行下面的taskUI中的操作暂时未成功，点击总是提示“No task instances to markas successful ”，目前可以通过强制rerun下一个task，使下一个task成功，虽然失败的task的状态未改变，但已经不影响下面的task执行强制rerun的命令为airflowrun -f -i dag_id task_id execute_time return在UI界面中clear某个task，则airflow会自动rerun该task当使用run按钮时，要求使用CeleryExecutor才可执行，因需要使用redis数据库，故暂时未尝试 命令相关test： 用于测试特定的某个task，不需要依赖满足run: 用于执行特定的某个task，需要依赖满足backfill: 执行某个DAG，会自动解析依赖关系，按依赖顺序执行unpause: 将一个DAG启动为例行任务，默认是关的，所以编写完DAG文件后一定要执行这和要命令，相反命令为pausescheduler: 这是整个 airflow 的调度程序，一般是在后台启动clear: 清除一些任务的状态，这样会让scheduler来执行重跑前面的脚本里用到了变量，每个DAG在执行时都会传入一个具体的时间（datetime对象）， 这个ds就会在 render 命令时被替换成对应的时间。这里要特别强调一下， 对于周期任务，airflow传入的时间是上一个周期的时间（划重点），比如你的任务是每天执行， 那么今天传入的是昨天的日期，如果是周任务，那传入的是上一周今天的值 ExecutorSequentialExecutor：表示单进程顺序执行，通常只用于测试LocalExecutor：表示多进程本地执行，它用python的多进程库从而达到多进程跑任务的效果。CeleryExecutor：表示使用celery作为执行器，只要配置了celery，就可以分布式地多机跑任务，一般用于生产环境。sql_alchemy_conn ：这个配置让你指定 airflow 的元信息用何种方式存储，默认用sqlite，如果要部署到生产环境，推荐使用 mysql。smtp ：如果你需要邮件通知或用到 EmailOperator 的话，需要配置发信的 smtp 服务器 触发条件条件一触发条件有两个维度, 以T1&amp;T2-&gt;T3 这样的dag为例:一个维度是: 要根据dag上次运行T3的状态确定本次T3是否被调用, 由DAG的default_args.depends_on_past参数控制, 为True时, 只有上次T3运行成功, 这次T3才会被触发条件二另一个维度是: 要根据前置T1和T2的状态确定本次T3是否被调用,由T3.trigger_rule参数控制, 有下面6种情形, 缺省是all_success.all_success: (default) all parents have succeededall_failed: all parents are in a failed or upstream_failed stateall_done: all parents are done with their executionone_failed: fires as soon as at least one parent has failed, it does not wait for all parents to be doneone_success: fires as soon as at least one parent succeeds, it does not wait for all parents to be donedummy: dependencies are just for show, trigger at will 工程代码实践相关 业务分支处理airflow有两个基于PythonOperator的Operator来支持dag分支功能.ShortCircuitOperator, 用来实现流程的判断. Task需要基于ShortCircuitOperator, 如果本Task返回为False的话, 其下游Task将被skip; 如果为True的话, 其下游Task将会被正常执行. 尤其适合用在其下游都是单线节点的场景.BranchPythonOperator, 用来实现Case分支. Task需要基于BranchPythonOperator, airflow会根据本task的返回值(返回值是某个下游task的id),来确定哪个下游Task将被执行, 其他下游Task将被skip. connection 表我们的Task往往需要通过jdbc/ftp/http/webhdfs方式访问其他资源, 一般地访问资源时候都需要一些签证, airflow允许我们将这些connection以及鉴证存放在connection表中. 可以现在WebUI的Admin-&gt;Connections管理这些连接, 在代码中使用这些连接.MySQL 应该使用 mysqlclient 包, 我简单试了mysql-connector-python 有报错LocalExecutor 和 CeleryExecutor 都可用于生产环境, CeleryExecutor 将使用 Celery 作为Task执行的引擎, 扩展性很好, 当然配置也更复杂, 需要先setup Celery的backend(包括RabbitMQ, Redis)等. 其实真正要求扩展性的场景并不多, 所以LocalExecutor 是一个很不错的选择了.配置OS环境变量 AIRFLOW_HOME, AIRFLOW_HOME缺省为 ~/airflow运行下面命令初始化一个Sqlite backend DB, 并生成airflow.cfg文件your_python ${AIRFLOW_HOME}\\bin\\airflow initdb如果需要修改backend DB类型, 修改$AIRFLOW_HOME/airflow.cfg文件 sql_alchemy_conn后, 然后重新运行 airflow initdb .官方推荐使用MySQL/PostgreSQL做DB Server.有下面3个参数用于控制Task的并发度parallelism, 一个Executor同时运行task实例的个数dag_concurrency, 一个dag中某个task同时运行的实例个数max_active_runs_per_dag: 一个dag同时启动的实例个数start_date 有点特别，如果你设置了这个参数，那么airflow就会从start_date开始以 schedule_interval 的规则开始执行，例如设置成3天前每小时执行一次，那么在调度正常启动时，就会立即调度 24*3 次，但注意，脚本执行环境的时间还是当前的系统时间，而不会说真是把系统时间模拟成3天前，所以感觉这个功能应用场景比较好限。 DAG目录定义以及部分airflow.cfg 字段配置dags_folder目录支持子目录和软连接，因此不同的dag可以分门别类的存储起来schedule_interval=timedelta(minutes=1) 或者 crontab格式1crontab格式的介绍2sql_alchemy_conn = mysql://ct:152108@localhost/airflow对应字段解释如下： dialect+driver://username:password@host:port/database当遇到不符合常理的情况时考虑清空 airflow backend的数据库, 可使用 airflow resetdb 清空。删除dag文件后，webserver中可能还会存在相应信息，这时需要重启webserver并刷新网页。关闭webserver:1ps -ef|grep -Ei &apos;(airflow-webserver)&apos;| grep master | awk &apos;&#123;print $2&#125;&apos;|xargs -i kill &#123;&#125;界面的时候看起来比较蛋疼， utc-0的时间，修改…/python2.7/site-packages/airflow/www/templates/admin/master.html如下**(注释掉UCTSeconds，新增一行UTCSeconds)， 这样时间就是本地时间了**。 部署相关 验证以及开启调度验证脚本是否有问题：python xxx.py看是否能查询出新增的dags吗:airflow list_dags启动schedule ：airflow scheduler 注意点这里有的 start_date 有点特别，如果你设置了这个参数，那么airflow就会从start_date开始以 schedule_interval 的规则开始执行，例如设置成3天前每小时执行一次，那么在调度正常启动时，就会立即调度 24*3 次，但注意，脚本执行环境的时间还是当前的系统时间，而不会说真是把系统时间模拟成3天前，所以感觉这个功能应用场景比较好限在centos6.8上装特别顺利（运行时貌似一切都正常，就是任务一直处于running状态—debug了一番源代码， 发现内存要必需够大，发现必需用非root身份运行airflow worker, 务必保证核数够用，否则需要调低dag_concurrency, max_active_runs_per_dag,max_threads,parallelism, 否则worker出现莫名其妙的问题)airflow跑着跑着就挂了，一看内存还够用（可能需要不要钱的加内存），如果你到处找不到想要的错误日志。那么看看AIRFLOW_HOME下面是不是莫名其妙的多了几个 .err/.out 的文件，进去看看会有收获。在需要运行作业的机器上的安装airflow airflow[celery] celery[redis] 模块后,启动airflow worker即可.这样作业就能运行在多个节点上. 模块依赖安装安装主模块[airflow@airflow ~]1$ pip install apache-airflow安装数据库模块、密码模块[airflow@airflow ~]1$ pip install &quot;apache-airflow[postgres,password]&quot;原文链接airflow FAQairflow 之DAGS详解","categories":[{"name":"任务调度","slug":"任务调度","permalink":"http://www.wortyby.com/categories/任务调度/"}],"tags":[{"name":"Airflow","slug":"Airflow","permalink":"http://www.wortyby.com/tags/Airflow/"},{"name":"DAG","slug":"DAG","permalink":"http://www.wortyby.com/tags/DAG/"},{"name":"operators","slug":"operators","permalink":"http://www.wortyby.com/tags/operators/"},{"name":"Sensor","slug":"Sensor","permalink":"http://www.wortyby.com/tags/Sensor/"}]},{"title":"服务器 Log 清理任务构建","slug":"服务器 Log 清理任务构建","date":"2018-11-05T05:23:15.000Z","updated":"2018-11-05T05:23:15.000Z","comments":true,"path":"2018/11/05/服务器 Log 清理任务构建/","link":"","permalink":"http://www.wortyby.com/2018/11/05/服务器 Log 清理任务构建/","excerpt":"","text":"服务器运行的各种各样的程序，一般我们会将其日志记录输出到文件中进行记录保存。既然是程序运行，就会有成功与失败，成功的执行程序，我们不做过多讨论，我们就先谈谈程序执行失败的时候的日志处理，为了避免程序执行失败，业务开发不知道，以及程序日志过大的问题，我们来谈谈怎么处理这个case。先上代码，分别从配置环境，执行周期的设定以及工程结构思路上面来进行分析。 配置环境通过 .INI 文件来进行 清理周期的配置，以及Python 版本来执行脚本的设定。以及log 存放记录的路径保存，业务开发邮箱设定12345678910111213141516171819202122232425262728293031rom airflow import DAGfrom airflow.operators import BashOperatorfrom datetime import datetime, timedeltaimport ConfigParserconfig = ConfigParser.ConfigParser()config.read(&apos;/etc/conf.ini&apos;)WORK_DIR = config.get(&apos;dir_conf&apos;, &apos;work_dir&apos;)OUTPUT_DIR = config.get(&apos;dir_conf&apos;, &apos;log_output&apos;)PYTHON_ENV = config.get(&apos;dir_conf&apos;, &apos;python_env&apos;)default_args = &#123; &apos;owner&apos;: &apos;airflow&apos;, &apos;depends_on_past&apos;: False, &apos;start_date&apos;: datetime.today() - timedelta(days=1), &apos;retries&apos;: 2, &apos;retry_delay&apos;: timedelta(minutes=15),&#125;dag = DAG(&apos;daily_process&apos;, default_args=default_args, schedule_interval=timedelta(days=1))templated_command = &quot;echo &apos;single&apos; | &#123;python_env&#125;/python &#123;work_dir&#125;/mr/LogMR.py&quot;\\ .format(python_env=PYTHON_ENV, work_dir=WORK_DIR) + &quot; --start_date &#123;&#123; ds &#125;&#125;&quot;task = BashOperator( task_id=&apos;process_log&apos;, bash_command=templated_command, dag=dag) 什么是 ETL ETL 定义ETL 是常用的数据处理，在以前的公司里，ETL 差不多是数据处理的基础，要求非常稳定，容错率高，而且能够很好的监控。ETL的全称是 Extract，Transform，Load， 一般情况下是将乱七八糟的数据进行预处理，然后放到储存空间上。可以是SQL的也可以是NoSQL的，还可以直接存成file的模式。一开始我的设计思路是，用几个cron job和celery来handle所有的处理，然后将我们的log文件存在hdfs，还有一些数据存在mysql，大概每天跑一次。核心是能够scale，稳定，容错，roll back。我们的data warehouse就放在云上，就简单处理了。有了自己的ETL系统我觉得就很安心了，以后能够做数据处理和机器学习方面就相对方便一些。 问题来了一开始我设计的思路和Uber一开始的ETL很像，因为我觉得很方便。但是我发觉一个很严重的问题，我一个人忙不过来。首先，要至少写个前端UI来监控cron job，但是市面上的都很差。其次，容错的autorestart写起来很费劲，可能是我自己没有找到一个好的处理方法。最后部署的时候相当麻烦，如果要写好这些东西，我一个人的话要至少一个月的时间，可能还不是特别robust。在尝试写了2两天的一些碎片处理的脚本之后我发觉时间拖了实在太久了。最终选择使用 Airbnb的 airflow 工具进行二次开发。 通知机制通过邮件将程序log 里的错误信息截取出来，发送给业务开发方。 部署方式 使用supervisord 进行deamon由于 airflow 本身没有deamon 模式，所以直接用supervisord 就OK了，我们只写4行代码。12345[program:airflow_web]command=/home/kimi/env/athena/bin/airflow webserver -p 8080[program:airflow_scheduler]command=/home/kimi/env/athena/bin/airflow scheduler","categories":[{"name":"服务器","slug":"服务器","permalink":"http://www.wortyby.com/categories/服务器/"}],"tags":[{"name":"Log 处理","slug":"Log-处理","permalink":"http://www.wortyby.com/tags/Log-处理/"},{"name":"tmpwatch","slug":"tmpwatch","permalink":"http://www.wortyby.com/tags/tmpwatch/"}]},{"title":"阿里咨询","slug":"阿里咨询","date":"2018-11-03T07:23:14.000Z","updated":"2018-11-03T07:23:14.000Z","comments":true,"path":"2018/11/03/阿里咨询/","link":"","permalink":"http://www.wortyby.com/2018/11/03/阿里咨询/","excerpt":"","text":"想问的问题大类如下 环境单机or 容器化?建议我们这体量的公司该采用什么模式部署更经济合算，就可拓展方便运维方面来说。 运维资源合理评估各个组件的运维手册分享 开发技术栈;算法库以及算法选用标准;关联搜索怎么通过数据结构实现的，是通过图节点网络的顶点向量相似?阿里的推荐引擎大概的架构设计分享？天猫的首页推荐与个人帐户推荐列表，呈现的不同内容就是简单的以推荐规则不同？数据仓库与用户画像平台在推荐与搜索平台的角色定位以及功能定义，如果我们没有这个东西，怎么合理的构建这个体系，给个建议？搜索与推荐的数据存储方案，可以简单分享？ 分析评估怎么评判一个推荐或搜索结果的好坏，即推荐与搜索的评价标准体系;通过什么措施或什么方式进行分析是否达到这个评价标准。介绍什么是OLAP以及现有产品Data VaultData Vault 知乎专栏ETL新兴趋势","categories":[{"name":"咨询学习","slug":"咨询学习","permalink":"http://www.wortyby.com/categories/咨询学习/"}],"tags":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/tags/推荐/"},{"name":"算法","slug":"算法","permalink":"http://www.wortyby.com/tags/算法/"},{"name":"OLAP","slug":"OLAP","permalink":"http://www.wortyby.com/tags/OLAP/"}]},{"title":"HBase最佳实践","slug":"HBase最佳实践－读性能优化策略","date":"2018-11-01T03:06:04.000Z","updated":"2018-11-01T03:06:04.000Z","comments":true,"path":"2018/11/01/HBase最佳实践－读性能优化策略/","link":"","permalink":"http://www.wortyby.com/2018/11/01/HBase最佳实践－读性能优化策略/","excerpt":"","text":"任何系统都会有各种各样的问题，有些是系统本身设计问题，有些却是使用姿势问题。HBase也一样，在真实生产线上大家或多或少都会遇到很多问题，有些是HBase还需要完善的，有些是我们确实对它了解太少。总结起来，大家遇到的主要问题无非是Full GC异常导致宕机问题、RIT问题、写吞吐量太低以及读延迟较大。Full GC问题之前在一些文章里面已经讲过它的来龙去脉，主要的解决方案目前主要有两方面需要注意，一方面需要查看GC日志确认是哪种Full GC，根据Full GC类型对JVM参数进行调优，另一方面需要确认是否开启了BucketCache的offheap模式，建议使用LRUBlockCache的童鞋尽快转移到BucketCache来。当然我们还是很期待官方2.0.0版本发布的更多offheap模块。RIT问题，我相信更多是因为我们对其不了解，具体原理可以戳这里，解决方案目前有两个，优先是使用官方提供的HBCK进行修复（HBCK本人一直想拿出来分享，但是目前案例还不多，等后面有更多案例的话再拿出来说），使用之后还是解决不了的话就需要手动修复文件或者元数据表。而对于写吞吐量太低以及读延迟太大的优化问题，笔者也和很多朋友进行过探讨，这篇文章就以读延迟优化为核心内容展开，具体分析HBase进行读延迟优化的那些套路，以及这些套路之后的具体原理。希望大家在看完之后能够结合这些套路剖析自己的系统。一般情况下，读请求延迟较大通常存在三种场景，分别为：仅有某业务延迟较大，集群其他业务都正常整个集群所有业务都反映延迟较大某个业务起来之后集群其他部分业务延迟较大这三种场景是表象，通常某业务反应延迟异常，首先需要明确具体是哪种场景，然后针对性解决问题。下图是对读优化思路的一点总结，主要分为四个方面：客户端优化、服务器端优化、列族设计优化以及HDFS相关优化，下面每一个小点都会按照场景分类，文章最后进行归纳总结。下面分别进行详细讲解：HBase客户端优化和大多数系统一样，客户端作为业务读写的入口，姿势使用不正确通常会导致本业务读延迟较高实际上存在一些使用姿势的推荐用法，这里一般需要关注四个问题：scan缓存是否设置合理？优化原理：在解释这个问题之前，首先需要解释什么是scan缓存，通常来讲一次scan会返回大量数据，因此客户端发起一次scan请求，实际并不会一次就将所有数据加载到本地，而是分成多次RPC请求进行加载，这样设计一方面是因为大量数据请求可能会导致网络带宽严重消耗进而影响其他业务，另一方面也有可能因为数据量太大导致本地客户端发生OOM。在这样的设计体系下用户会首先加载一部分数据到本地，然后遍历处理，再加载下一部分数据到本地处理，如此往复，直至所有数据都加载完成。数据加载到本地就存放在scan缓存中，默认100条数据大小。通常情况下，默认的scan缓存设置就可以正常工作的。但是在一些大scan（一次scan可能需要查询几万甚至几十万行数据）来说，每次请求100条数据意味着一次scan需要几百甚至几千次RPC请求，这种交互的代价无疑是很大的。因此可以考虑将scan缓存设置增大，比如设为500或者1000就可能更加合适。笔者之前做过一次试验，在一次scan扫描10w+条数据量的条件下，将scan缓存从100增加到1000，可以有效降低scan请求的总体延迟，延迟基本降低了25%左右。优化建议：大scan场景下将scan缓存从100增大到500或者1000，用以减少RPC次数get请求是否可以使用批量请求？优化原理：HBase分别提供了单条get以及批量get的API接口，使用批量get接口可以减少客户端到RegionServer之间的RPC连接数，提高读取性能。另外需要注意的是，批量get请求要么成功返回所有请求数据，要么抛出异常。优化建议：使用批量get进行读取请求请求是否可以显示指定列族或者列？优化原理：HBase是典型的列族数据库，意味着同一列族的数据存储在一起，不同列族的数据分开存储在不同的目录下。如果一个表有多个列族，只是根据Rowkey而不指定列族进行检索的话不同列族的数据需要独立进行检索，性能必然会比指定列族的查询差很多，很多情况下甚至会有2倍～3倍的性能损失。优化建议：可以指定列族或者列进行精确查找的尽量指定查找离线批量读取请求是否设置禁止缓存？优化原理：通常离线批量读取数据会进行一次性全表扫描，一方面数据量很大，另一方面请求只会执行一次。这种场景下如果使用scan默认设置，就会将数据从HDFS加载出来之后放到缓存。可想而知，大量数据进入缓存必将其他实时业务热点数据挤出，其他业务不得不从HDFS加载，进而会造成明显的读延迟毛刺优化建议：离线批量读取请求设置禁用缓存，scan.setBlockCache(false)HBase服务器端优化一般服务端端问题一旦导致业务读请求延迟较大的话，通常是集群级别的，即整个集群的业务都会反映读延迟较大。可以从4个方面入手：读请求是否均衡？优化原理：极端情况下假如所有的读请求都落在一台RegionServer的某几个Region上，这一方面不能发挥整个集群的并发处理能力，另一方面势必造成此台RegionServer资源严重消耗（比如IO耗尽、handler耗尽等），落在该台RegionServer上的其他业务会因此受到很大的波及。可见，读请求不均衡不仅会造成本身业务性能很差，还会严重影响其他业务。当然，写请求不均衡也会造成类似的问题，可见负载不均衡是HBase的大忌。观察确认：观察所有RegionServer的读请求QPS曲线，确认是否存在读请求不均衡现象优化建议：RowKey必须进行散列化处理（比如MD5散列），同时建表必须进行预分区处理BlockCache是否设置合理？优化原理：BlockCache作为读缓存，对于读性能来说至关重要。默认情况下BlockCache和Memstore的配置相对比较均衡（各占40%），可以根据集群业务进行修正，比如读多写少业务可以将BlockCache占比调大。另一方面，BlockCache的策略选择也很重要，不同策略对读性能来说影响并不是很大，但是对GC的影响却相当显著，尤其BucketCache的offheap模式下GC表现很优越。另外，HBase 2.0对offheap的改造（HBASE-11425）将会使HBase的读性能得到2～4倍的提升，同时GC表现会更好！观察确认：观察所有RegionServer的缓存未命中率、配置文件相关配置项一级GC日志，确认BlockCache是否可以优化优化建议：JVM内存配置量 &lt; 20G，BlockCache策略选择LRUBlockCache；否则选择BucketCache策略的offheap模式；期待HBase 2.0的到来！HFile文件是否太多？优化原理：HBase读取数据通常首先会到Memstore和BlockCache中检索（读取最近写入数据&amp;热点数据），如果查找不到就会到文件中检索。HBase的类LSM结构会导致每个store包含多数HFile文件，文件越多，检索所需的IO次数必然越多，读取延迟也就越高。文件数量通常取决于Compaction的执行策略，一般和两个配置参数有关：hbase.hstore.compactionThreshold和hbase.hstore.compaction.max.size，前者表示一个store中的文件数超过多少就应该进行合并，后者表示参数合并的文件大小最大是多少，超过此大小的文件不能参与合并。这两个参数不能设置太’松’（前者不能设置太大，后者不能设置太小），导致Compaction合并文件的实际效果不明显，进而很多文件得不到合并。这样就会导致HFile文件数变多。观察确认：观察RegionServer级别以及Region级别的storefile数，确认HFile文件是否过多优化建议：hbase.hstore.compactionThreshold设置不能太大，默认是3个；设置需要根据Region大小确定，通常可以简单的认为hbase.hstore.compaction.max.size = RegionSize / hbase.hstore.compactionThresholdCompaction是否消耗系统资源过多？优化原理：Compaction是将小文件合并为大文件，提高后续业务随机读性能，但是也会带来IO放大以及带宽消耗问题（数据远程读取以及三副本写入都会消耗系统带宽）。正常配置情况下Minor Compaction并不会带来很大的系统资源消耗，除非因为配置不合理导致Minor Compaction太过频繁，或者Region设置太大情况下发生Major Compaction。观察确认：观察系统IO资源以及带宽资源使用情况，再观察Compaction队列长度，确认是否由于Compaction导致系统资源消耗过多优化建议：（1）Minor Compaction设置：hbase.hstore.compactionThreshold设置不能太小，又不能设置太大，因此建议设置为5～6；hbase.hstore.compaction.max.size = RegionSize / hbase.hstore.compactionThreshold（2）Major Compaction设置：大Region读延迟敏感业务（ 100G以上）通常不建议开启自动Major Compaction，手动低峰期触发。小Region或者延迟不敏感业务可以开启Major Compaction，但建议限制流量；（3）期待更多的优秀Compaction策略，类似于stripe-compaction尽早提供稳定服务HBase列族设计优化HBase列族设计对读性能影响也至关重要，其特点是只影响单个业务，并不会对整个集群产生太大影响。列族设计主要从两个方面检查：Bloomfilter是否设置？是否设置合理？优化原理：Bloomfilter主要用来过滤不存在待检索RowKey或者Row-Col的HFile文件，避免无用的IO操作。它会告诉你在这个HFile文件中是否可能存在待检索的KV，如果不存在，就可以不用消耗IO打开文件进行seek。很显然，通过设置Bloomfilter可以提升随机读写的性能。Bloomfilter取值有两个，row以及rowcol，需要根据业务来确定具体使用哪种。如果业务大多数随机查询仅仅使用row作为查询条件，Bloomfilter一定要设置为row，否则如果大多数随机查询使用row+cf作为查询条件，Bloomfilter需要设置为rowcol。如果不确定业务查询类型，设置为row。优化建议：任何业务都应该设置Bloomfilter，通常设置为row就可以，除非确认业务随机查询类型为row+cf，可以设置为rowcolHDFS相关优化HDFS作为HBase最终数据存储系统，通常会使用三副本策略存储HBase数据文件以及日志文件。从HDFS的角度望上层看，HBase即是它的客户端，HBase通过调用它的客户端进行数据读写操作，因此HDFS的相关优化也会影响HBase的读写性能。这里主要关注如下三个方面：Short-Circuit Local Read功能是否开启？优化原理：当前HDFS读取数据都需要经过DataNode，客户端会向DataNode发送读取数据的请求，DataNode接受到请求之后从硬盘中将文件读出来，再通过TPC发送给客户端。Short Circuit策略允许客户端绕过DataNode直接读取本地数据。（具体原理参考此处）优化建议：开启Short Circuit Local Read功能，具体配置戳这里Hedged Read功能是否开启？优化原理：HBase数据在HDFS中一般都会存储三份，而且优先会通过Short-Circuit Local Read功能尝试本地读。但是在某些特殊情况下，有可能会出现因为磁盘问题或者网络问题引起的短时间本地读取失败，为了应对这类问题，社区开发者提出了补偿重试机制 – Hedged Read。该机制基本工作原理为：客户端发起一个本地读，一旦一段时间之后还没有返回，客户端将会向其他DataNode发送相同数据的请求。哪一个请求先返回，另一个就会被丢弃。优化建议：开启Hedged Read功能，具体配置参考这里数据本地率是否太低？数据本地率：HDFS数据通常存储三份，假如当前RegionA处于Node1上，数据a写入的时候三副本为(Node1,Node2,Node3)，数据b写入三副本是(Node1,Node4,Node5)，数据c写入三副本(Node1,Node3,Node5)，可以看出来所有数据写入本地Node1肯定会写一份，数据都在本地可以读到，因此数据本地率是100%。现在假设RegionA被迁移到了Node2上，只有数据a在该节点上，其他数据（b和c）读取只能远程跨节点读，本地率就为33%（假设a，b和c的数据大小相同）。优化原理：数据本地率太低很显然会产生大量的跨网络IO请求，必然会导致读请求延迟较高，因此提高数据本地率可以有效优化随机读性能。数据本地率低的原因一般是因为Region迁移（自动balance开启、RegionServer宕机迁移、手动迁移等）,因此一方面可以通过避免Region无故迁移来保持数据本地率，另一方面如果数据本地率很低，也可以通过执行major_compact提升数据本地率到100%。优化建议：避免Region无故迁移，比如关闭自动balance、RS宕机及时拉起并迁回飘走的Region等；在业务低峰期执行major_compact提升数据本地率HBase读性能优化归纳在本文开始的时候提到读延迟较大无非三种常见的表象，单个业务慢、集群随机读慢以及某个业务随机读之后其他业务受到影响导致随机读延迟很大。了解完常见的可能导致读延迟较大的一些问题之后，我们将这些问题进行如下归类，读者可以在看到现象之后在对应的问题列表中进行具体定位：hbase2hbase3hbase4HBase读性能优化总结性能优化是任何一个系统都会遇到的话题，每个系统也都有自己的优化方式。 HBase作为分布式KV数据库，优化点又格外不同，更多得融入了分布式特性以及存储系统优化特性。文中总结了读优化的基本突破点，有什么不对的地方还望指正，有补充的也可以一起探讨交流！HBase 查询优化原文链接","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://www.wortyby.com/tags/HBase/"}]},{"title":"用户画像的横空出世","slug":"用户画像的横空出世-","date":"2018-11-01T02:23:57.000Z","updated":"2018-11-01T02:23:57.000Z","comments":true,"path":"2018/11/01/用户画像的横空出世-/","link":"","permalink":"http://www.wortyby.com/2018/11/01/用户画像的横空出世-/","excerpt":"","text":"系统应该包含基本的反馈和响应，也可能包含某些刺激，先不谈用户画像。从名词解释说起三个概念 ，反馈，响应，刺激 概念定义:反馈: 定义为用户在阿基米德的所有信息输入;比如收听，点击，各种看的见的操作。响应: 针对用户的反馈输出对应的事件响应;比如 用户收听的流状态稳不稳定等等，收听的音频是否存在，节目是否下线，等等刺激: 为了得到某些特定的响应，可能需要针对反馈 加上 一些处理 ;比如，我希望给 20-30 岁的平台用户，推荐一些符合该年龄段的信息，然后通过响应看效果。可能涉及到一些数据ETL 操作所以一个通用的系统是基本包含 反馈和响应的，但是不会包含着用户画像。看你邮件回复的是帮助做各种运营手段。那么用户画像是什么呢，是不是就是一个分析手段？从概念灌输上，是没啥大问题的。但是我们如果要做这件事情，可能光知道概念还不够，要知道怎么做，还要剖析它。通过和同事讨论几次之后私下认为，用户画像就是一个数据精细化维度的数据集合，就是一个数据仓库，这是个人意见，可能还有不到位的地方。像你分享的附件，各种各样的效果，我统一都是归类到用户响应里。但是计算机的各种各样的信息能够被电子消费掉，是需要数字信号处理的，而不是处理什么 男/女 标签。所以我昨天说他们的邮件都是虚的，因为他们只讲 反馈 和 响应，最重要的 转换 不讲。就像学生问老师，为什么三角函数数学表达式可以表示信号，老师说，通过采集器采集到抽样数据，经过一个系统转化，输出的就是三角函数信号了，不给你讲这个系统转化的黑盒，我想那些想知道黑盒原理的学生心里有想法吧。所以我觉得用户画像的定义是用户画像:将 “反馈” 进系统的数据，经过各种各样的预处理，方便供 “响应” 使用的一个转化过程，这个过程是将 文本/音频/视频数据，通过各种sql 操作，组合更新放到 数据库，当响应需要各种各样的标签需求的时候，直接通过各种各样的sql 语句进行抽离。我们再给每个功能需求的sql 进行另存为的保存 为 “标签”，这个 标签可以是简单的基本用户信息获取，也可以是各种各样的外部定制化运营需求。而各种各样的转化，本质上就是各种sql 语句组合。通俗地讲用户画像是上面反馈和刺激 之后的一个记忆暂存，等待着各种标签请求，将其输出到最终的呈现节点，比如excel，web，csv，等等格式。****，让普罗大众用户能够好好接受。所以，我觉得目前我们既然用数据库的话，本质就是一个&quot;sql语句 查询” 管理系统，没什么大花头。以上是个人见解，可能有眼光狭窄，没看见的地方。","categories":[{"name":"用户画像","slug":"用户画像","permalink":"http://www.wortyby.com/categories/用户画像/"}],"tags":[{"name":"系统","slug":"系统","permalink":"http://www.wortyby.com/tags/系统/"}]},{"title":"理解索引：HBase介绍和架构","slug":"理解索引：HBase介绍和架构","date":"2018-10-30T08:33:33.000Z","updated":"2018-10-30T08:33:33.000Z","comments":true,"path":"2018/10/30/理解索引：HBase介绍和架构/","link":"","permalink":"http://www.wortyby.com/2018/10/30/理解索引：HBase介绍和架构/","excerpt":"","text":"最近有个需求，要修改现有存储结构，涉及查询条件和查询效率的考量，看了几篇索引和HBase相关的文章，回忆了相关知识，结合项目需求，说说自己的理解和总结。前几篇文章重点介绍MySQL索引相关的知识，从索引优点、索引结构演化过程，到SQL查询过程、执行计划，再到最后的索引优化，错过的朋友可以回顾下前几篇文章：索引结构和数据定位过程查询过程和高级查询执行计划详细介绍索引优化后面会开始介绍HBase索引相关的，和MySQL对比下，以加深对索引的理解，本篇先介绍下HBase，它的一些特性和整体架构，为后面介绍索引做准备，通过介绍，你会了解到：为什么会出现HBaseHBase的特性HBase的整体架构部分内容摘录了几个博友的文章，最后会给出文章链接，感谢他们的精彩分析。 为什么会出现 HBase任何一个技术的出现都是有原因的，了解它为什么出现，以及它解决了什么问题，更有助于理解它的特性和设计思想。 MySQL 瓶劲MySQL是一个关系型数据库，有很高的数据一致性和持久性保证，当访问量特别高时，特别是写入操作，会有很大的O性能瓶颈。虽然可以通过主从读写分离、分库分表的方式解决，但随着数据量不断增大、并发不断增高，MySQL应用开发越来越复杂，也越来越具有技术挑战性。另外，分表分库的规则的设定都是需要经验的，虽然有 Cobar、MyCat、Sharding-JDBC、TDDL、DBProxy 中间件层来屏蔽开发者的复杂性，但是避免不了整个架构的复杂性。分库分表的子库到一定阶段又面临扩展问题，需求的变更可能又需要一种新的 sharding。MySQL的扩展性差、大数据下IO压力大、表结构更改困难，正是MySQL开发人员面临的问题，也是MySQL的瓶颈。虽然有这些瓶颈，但对数据一致性要求特别高的业务，还是要使用它，但是有的应用场景不需要太高的一致性，在大数据量、高并发的业务中，可以选择其他存储方案。 NoSQL 的出现NoSQL数据库种类繁多，但是一个共同的特点都是去掉关系数据库的关系型特性，数据之间无关系，这样就非常容易扩展。 NOSQL有如下特点：模式自由: 不像传统的关系型数据库需要定义数据库、数据表等结构才可以存取数据，数据表中的每一条记录都可能有不同的属性和格式；逆范式: 去除约束，降低事务要求，更利于数据的分布式存储，与MySQL范式相反；多分区存储: 存储在多个节点上，很好地进行水平扩展，提高数据的读、写性能；多副本异步复制: 为了保证数据的安全性，会保存数据的多个副本;弹性可扩展: 可以在系统运行过程中动态的增删节点，数据自动平衡移动，不需要人工的干预操作；软事务：事务是关系型数据库的一个特点，NoSQL数据库不能完全满足事务的ACID特性，但是能保证事务的最终一致性； NOSQL有一些理论支持:CAP理论: 就是平衡一致性、可用性、分区容错性，因为最多只能同时实现2个，需要做平衡取舍；BASE模型: Basically Availble 基本可用，Soft state 状态可以有一段时间不同步，Eventual Consistency 最终一致性，不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化；放上网上的一张图，可以看到不同数据库测试点不同：上图也根据颜色区分了不同的数据模型，这里简单总结下。关系型数据库：一直在介绍的MySQL，就不说了；键值：内容缓存，主要用于处理大量数据的高访问负载，查找速度快，但数据无结构化，通常只被当作字符串或者二进制数据，比如Redis、MemcacheDB；列存储数据库：分布式的文件系统，按列存储，针对某一列或者某几列的查询有非常大的IO优势，以列簇式存储，将同一列数据存在一起，查找速度快，可扩展性强，更容易进行分布式扩展，但功能相对局限，比如BigTable、HBase、Cassandra；文档型数据库：存储类似JSON格式的内容，可对某些字段建立索引功能，是最像关系型的数据库，Key-Value对应的键值对，Value为结构化数据，数据结构要求不严格，表结构可变，但查询性能不高，而且缺乏统一的查询语法，比如MongoDB、CouchDB； HBase 产生背景上面提到，随着数据规模越来越大，大量业务场景开始考虑数据存储的水平扩展，海量数据量存储成为提升应用性能的瓶颈，单台机器无法负载海量的数据处理，随之而来的出现了很多的分布式存储解决方案，HBase就是其中之一。HBase－－DataBase on Hadoop，基于分布式文件系统上面建立的数据库，HBase是面向列的开源数据库。开源团队根据2008年Google发布了一篇关于Google搜索引擎BigTable的核心思想的论文，实现了基于分布式文件系统的列数据库。随后加入Apache基金会，成为Hadoop生态圈中的顶级项目被大家熟知。 HBase 的特性高性能HBase中存储了一套HDFS的索引，通过表名－&gt;行健－&gt;列族－&gt;列限定符－&gt;时间版本这一套索引来定位数据的位置，HBase为每一列数据维护了一套索引规则，对于具体某一具体条数据的查询可以非常快速的通过B＋树定位数据存储位置并将其取出。另外，HBase通常以集群部署，数据被分散到多个节点存储，当客户端发起查询请求的时候，集群里面多个节点并行执行查询操作，最后将不同节点的查询结果进行合并返回给客户端，提高IO性能。高可用HBase集群中任意一个节点宕机都不会导致集群瘫痪。这取决于两方面原因:第一方面，ZooKeeper解决了HBase中心化问题；另一方面，HBase将数据存放在HDFS上面，HDFS的数据冗余存放在不同节点，一个节点瘫痪可从其他节点取得数据，保证了HBase的高可用。易扩展Hbase的扩展性主要体现在两个方面，一个是基于上层处理能RegionServer的扩展，一个是基于存储的扩展HDFS。无模式使用HBase不需要预先定义表中有多少列，也不需要定义每一列存储的数据类型，HBase在需要的时候可以动态增加列和指定存储数据类型。 HBase 的整体架构Hbase在整个Hadoop生态圈的地位如下图： 了解几个概念rowkey: Rowkey的概念和mysql中的主键相似，Hbase使用Rowkey来唯一的区分某一行的数据；region: 和MySQL的分区或者分片差不多，Hbase会将一个大表的数据基于Rowkey的不同范围分配到不通的Region中，每个Region负责一定范围的数据访问和存储；timestamp: timestamp对Hbase来说至关重要，因为它是实现Hbase多版本的关键，在写入数据的时候，如果用户没有指定对应的timestamp，Hbase会自动添加一个timestamp，timestamp和服务器时间保持一致。相同rowkey的数据按照timestamp倒序排列，默认查询的是最新的版本，可以指定timestamp的值来读取旧版本的数据。 内部基本组成Hbase的整体主要由zookeeper，Hmaster，HRegionServer，Hdfs文件系统组成，由这四部分共同完成数据的读取与写入。不同范围的数据被划分到不同地方，称为HRegion，不同的HRegion被放在不同的主机上，当查询数据的时候，只要根据rowkey先找到数据在那个范围的HRegion中，就可以直接到那个HRegion中找到数据，查询效率比传统的数据库快很多。 整体结构 HMater在Region Split后，负责新Region的分配；新机器加入时，管理HRegion Server的负载均衡，调整Region分布；在HRegion Server宕机后，负责失效HRegion Server 上的Regions迁移； Region ServerRegion server维护Master分配给它的region，处理对这些region的IO请求；HRegion Server管理了很多table的分区，也就是region； ZookeeperZooKeeper为HBase集群提供协调服务，它管理着HMaster和HRegionServer的状态(available/alive等)，并且会在它们宕机时通知给HMaster；zookeeper中管理着hbase的元数据，例如-root-的位置所在； HDFS数据文件的存放处，由于其本身的分布式存储机制，所以数据文件很安全；hadoop的datanode最好和region在同一主机上，方便读取数据，尽量避免网络数据传输；这篇主要是个引子，下一篇重点介绍下HBase中的索引，以及设计rowkey的一些原则和技巧。原文链接 参考文章大并发大数量中的MYSQL瓶颈与NOSQL介绍NoSQL你知多少？大数据Hadoop之HBase认识Hbase技术详细学习笔记Hbase的架构简单解析","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://www.wortyby.com/tags/HBase/"},{"name":"架构","slug":"架构","permalink":"http://www.wortyby.com/tags/架构/"}]},{"title":"Learning To Rank 研究与应用","slug":"Learning To Rank 研究与应用","date":"2018-10-23T12:26:13.000Z","updated":"2018-10-23T12:26:13.000Z","comments":true,"path":"2018/10/23/Learning To Rank 研究与应用/","link":"","permalink":"http://www.wortyby.com/2018/10/23/Learning To Rank 研究与应用/","excerpt":"","text":"Learning To Rank 研究与应用原文链接tensorflow交互稀疏矩阵运算如何在 spark 分布式矩阵实现协同过滤推荐？Kylin的cube模型Apache Kylin安装部署Deep Learning for NLP Best PracticesIT瘾社区产品数据体系建设基础：一个产品的数据体系建设信息的分布率卷积网络理解cnn 为 NLP 准备word2vec 数学原理理解","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/tags/推荐/"},{"name":"深度学习","slug":"深度学习","permalink":"http://www.wortyby.com/tags/深度学习/"}]},{"title":"稀疏矩阵Compressed Sparse Row (CSR)存储原理","slug":"CSR存储原理","date":"2018-10-23T12:19:55.000Z","updated":"2018-10-23T12:19:55.000Z","comments":true,"path":"2018/10/23/CSR存储原理/","link":"","permalink":"http://www.wortyby.com/2018/10/23/CSR存储原理/","excerpt":"","text":"为了解决通过one-hot 转化之后，产生的高维数据存储问题，我们设计了一个新的存储过程见图 原理图 原理解析通过将矩阵的4x4的数据存储为只有三行的结构体数据的过程本质是将数据不为零的数据存放好具体来说就是 indptr最后一个数字代表data 数据一共有几个非零值，即有效数据位数其余的数字代表的是矩阵中的每行的非零数据在 data(value) 字段里的的索引值,比如 &quot;1&quot;代表的是 矩阵 第零行的第零个数据，所以 row offsets = 0,结合indices 里的索引，可以准确的还原出 矩阵的数据形式。 indices每个数字代表的是data(value)对应的非零数据在矩阵中的对应的列索引，比如 &quot;7&quot;代表的是 矩阵 第零行的第一个数据，所以 indices = 1 data矩阵里的所有非零数据结合 indptr 和 indices 2项数据进行还原成 matrix 原理实现代码12345678910111213141516171819# coding: utf-8from scipy.sparse.csr import csr_matrix docs = [[&quot;hello&quot;, &quot;world&quot;, &quot;hello&quot;], [&quot;goodbye&quot;, &quot;cruel&quot;, &quot;world&quot;]]indptr = [0] # 存放的是行偏移量indices = [] # 存放的是data中元素对应的列编号（列编号可重复）data = [] # 存放的是非0数据元素vocabulary = &#123;&#125; # key是word词汇，value是列编号for d in docs: # 遍历每个文档 for term in d: # 遍历文档的每个词汇term # setdefault如果term不存在，则将新term和他的列 # 编号len(vocabulary)加入到词典中，返回他的编号； # 如果term存在，则不填加，返回已存在的编号 index = vocabulary.setdefault(term, len(vocabulary)) indices.append(index) data.append(1) indptr.append(len(indices))# csr_matrix可以将同一个词汇次数求和csr_matrix((data, indices, indptr), dtype=int).toarray()","categories":[{"name":"编码","slug":"编码","permalink":"http://www.wortyby.com/categories/编码/"}],"tags":[{"name":"存储","slug":"存储","permalink":"http://www.wortyby.com/tags/存储/"}]},{"title":"深度学习在推荐系统中的应用总结","slug":"深度学习在推荐系统中的应用总结 ","date":"2018-10-15T05:31:31.000Z","updated":"2018-10-15T05:31:31.000Z","comments":true,"path":"2018/10/15/深度学习在推荐系统中的应用总结 /","link":"","permalink":"http://www.wortyby.com/2018/10/15/深度学习在推荐系统中的应用总结 /","excerpt":"","text":"深度学习在推荐系统中的应用总结原文链接","categories":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/categories/推荐/"}],"tags":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/tags/推荐/"},{"name":"深度学习","slug":"深度学习","permalink":"http://www.wortyby.com/tags/深度学习/"}]},{"title":"查询词-站点相关性计算","slug":"查询词-站点相关性计算","date":"2018-10-15T05:19:38.000Z","updated":"2018-10-15T05:19:38.000Z","comments":true,"path":"2018/10/15/查询词-站点相关性计算/","link":"","permalink":"http://www.wortyby.com/2018/10/15/查询词-站点相关性计算/","excerpt":"","text":"引言在自然语言处理、文本挖掘中，常常使用词向量作为单词（Word）内在含义的表达，从传统的向量表达到近几年的词嵌入（Word Embedding）表达，词向量已经作为一种文本的常用特征得到广泛应用。类似的，一些研究者希望通过网络结构中的连接关系，得到网络中顶点（vertex）的向量表示，作为基本特征应用到聚类、分类等任务上。最近在研究图片搜索中关于query和site的关系时，希望能通过它们的连接关系得到embedding向量作为特征。本文总结了5种近几年常用的将网络结构中节点转化为节点向量的方法，不同研究者对这一问题的命名有出入，如node2vector，graph representation，network embedding等，但实质上是同一个问题。接下来本文对deepwalk、line、DNGR、SDNE、node2vector五种方法分别做介绍。 DeepwalkDeepwalk来源于《DeepWalk: Online Learning of Social Representations》这篇论文，它的思想非常简单，主要借鉴了word2vec，将网络结构通过Random walk的方式，转换为类似“sentence”的节点序列的形式。Word2Vec是Mikolov带领Google研发的用来产生词嵌入表达的模型，其中又包括skip-grams 或continuous-bag-of-words（CBOW）两种方式。Word2Vec相关介绍很多，本文不再赘述。在deepwalk的这篇论文中，为了说明网络结构中的节点和文本中的词具有可比性，作者根据对社交网络的图和Wikipedia中的文本进行分别统计，发现都遵循zipf’s定律，说明词和经过Random walk后图的节点，具有相似的特性。如下图所示：将网络结构转化为“sentence”序列后，以通过word2vec中的Skip-gram模型或者CBOW模型，训练得到每个节点的向量表示形式，进而可以用余弦距离或者欧式距离来求得两个节点之间的相似度。这篇论文采用了Skip-gram模型。Deepwalk算法描述，见下图。对图G，随机采样1个节点v，然后以此为起点连续采样，直到达到最大路径长度t，再通过Skip-gram来更新参数。Deepwalk实现很简单，在网络结构上主要考虑节点间是否存在连接的边，但是效果稳定，在评测数据上取得了很不错的效果，且代码健壮。在应用上deepwalk支持directed/undirected网络，原始代码不支持带权重的网络结构。Deepwalk工具包地址：deepwalk LINELINE模型源于论文《LINE: Large-scale Information Network Embedding》，这篇文章提出了两个规则（order），我们通过文中的示例来具体理解这两种规则。如下图所示网络：这个网络结构中，对5、6、7三个节点而言，根据first-order来说6、7节点更相似，因为它们有一条直接相连的边；根据second-order来说5、6节点更相似，因为它们有四个公共邻居节点。根据这两种计算相似性的规则，论文作者相应的提出了两种训练节点向量的算法。First-order：这种方法主要考虑两个节点间的没有方向(undirected)的边，用表示节点向量，那么可以用下面的式子计算节点和的联合概率：根据经验，在网络结构中和联合分布的经验概率可以表示为，其中，模型的目标函数就是希望优化两种联合概率的距离其中函数表示两种分布的“距离”，论文中选用了KL散度来衡量。此时，目标函数转化为Second-order：由于第二规则着重考虑邻居节点对中心节点的表达，类似于文本中上下文对中心词的表达，上下文一致时两个中心词就极有可能相似，因此在第二规则中对有向(directed)边，定义了节点间的生成概率：根据经验，我们可以知道节点生成节点的概率为，其中表示节点指向所有边的权重和。那么模型的目标函数如下：同样适用KL散度定义距离，且使，消去常数项，可以使目标函数转化为：在Second-order的目标生成概率函数中可以发现，当需要计算节点i到节点j的转移概率时，需要计算所有和节点i相邻节点的内积，这样明显会极大增加求解过程中的计算量。论文作者采用负采样（negative sampling）的方法对生成概率进行替换，如下面式子所示：其中是sigmoid函数，是负采样的样本个数。是一个表示负样本分布的参数，具体可参看论文。发散思考：本文作者在这两种方法的基础上，提出了一些具体问题上进行讨论。第一个问题是网络节点中低维度节点应该怎样训练？低维度节点指的是某些节点的“邻居节点”太少，这样可供其采集的样本就很少。在这些节点上学习到的特征就很弱。为了增加其信息的丰富性，作者尝试使用构建邻接边的方式丰富网络结构，主要思想是假如存在临边 、，那么就可以确定出的权重。公式如下：第二个问题是针对一个网络结构训练完毕后，如何添加入新的节点？这是一个应用性很强的问题，其本质是要考虑如何不重新整合全局信息而得到新加入节点在这个空间维度的向量表达。在Word2Vec中就可以实现在训练结果的基础上，加入新的语料继续优化。在这里，作者提出了一个简单的优化目标，在不改变原始网络中节点向量的基础上，根据新节点的连接关系对新节点进行优化。两种规则的优化目标函数如下：两个目标函数较易理解，不再赘述。除了以上两个问题，作者还对这两种向量的结合问题做了思考。毕竟这两种方法都挺不错，如果能结合起来学出来一套综合的向量按理说效果应该更好啊。不过很遗憾的是由于这两个方法的出发点不一样，很难把两种算法结合起来学习。但是却有一种简单粗暴的方法，就是使用两种规则分别训练，最后直接把两个向量拼接起来。效果在具体任务上见仁见智吧，我在自己的任务上试验过，反而比单独的效果还差一点，也是很不解。有兴趣的童鞋可以再研究研究，没准就能搞一篇不错的论文出来。LINE工具包：LINE Node2VectorNode2Vector算法源于论文《node2vec: Scalable Feature Learning for Networks》，这篇论文可以看作是deepwalk的升级版。整体上来看，Node2Vector在deepwalk的基础上改变了节点游走的方式，考虑了更多的信息，deepwalk在下一个节点的选择上只是对当前节点的所有邻居节点中random选择一个，而本文方法就更复杂些，在一些评测上也取得了更好的效果。下面就具体看一下Node2Vec的实现方法。论文作者首先提出了网络节点间游走（采样）的两种方式：Breadth-first Sampling（BFS）和Depth-first Sampling（DFS），其实有点类似常说的广搜和深搜。我们用论文中的一张图来说明这两种采样方式：相信上面这张图已经直观的解释了BFS和DFS，不再细说。这两种采样方式风格不同，但各有千秋，都挺符合我们的认知，论文作者觉得应该把两种采样方式结合起来啊，这样岂不是很牛逼？但是怎样结合是一个大问题。不着急，我们先来看下面这个图，这是作者论文中的一张图：这种图充分说明论文作者如何将两种游走方式结合起来的。在deepwalk中，一条walk序列从节点走到节点，接下来往哪里走呢？就是从随机选一个，然后依次类推的往下走。但在Node2Vector中作者将四个节点共分为3类，对当前节点(cur_node)来说，节点的上一个节点(pre_node)，是cur_node和pre_node同时连接的节点，是其他邻居节点。我们还可以将这三类节点按照DFS和BFS的理论去分析，比如再往想//走就是DFS，往/走就是BFS，总而言之这些都可以算是物理意义上的解释吧。为了实现这种结合的游走方式，作者定义了、两个参数，将这三类节点的概率值分别定义为、、，参数、是作为参数传入的。根据网络结构的不同特点，可以调整、来实现对不同采样方式的偏爱。如下图所示：到这里，针对节点t能来说，我们得到了能转移到不同类别节点的概率，常规做法是归一化之后按照概率随机选取，但这篇论文并没有直接这样做，而是选用了Alias算法进行抽样。Alias算法相比较于直接抽样方法效率上要高一些，效果上见仁见智，这里不再具体介绍。此时，我们就可以得到节点序列，已经完成了整个算法的核心部分，因为接下来就是直接用word2vec对节点序列进行训练（对，就是这么任性）就好了。另外，值得一提的是，由于在next_node的选择上需要&lt;pre_node, cur_node&gt;两个节点的信息，因此在初始的转移概率空间复杂度上就是deepwalk的平方。当节点个数较多的时候可能出现out of memory。当然，也由于这种游走方式的复杂性，在网络节点的个数较少时，node2vec的效果整体上要优于deepwalk，但在节点多到可能影响空间限制时，deepwalk效果更优。Node2vec源码：node2vec SDNESNDE方法源于论文《Structural Deep Network Embedding》，这篇论文从Auto-encoder方法上进行改进，从而实现对网络节点的embedding。Auto-encoder方法，简单来说可以算是一种降维的方法，在图片应用上比较多，将原始的高维向量在尽可能避免信息丢失的情况下降维得到目标维度的向量。Auto-encoder的网络结构大致如下：Auto-encoder算法主要达到的是一个“降维”的目的，并不能学习出表示节点关系的embedding表达。在这篇论文中，作者通过在训练过程中加入节点信息使得学习出的向量能过表示出节点的内在关系。接下来我们来详细介绍下SDNE，先给出下面这幅图，描述了整个SDNE的网络结构：第一步，我们来了解下Auto-encoder，就是上图中的左边部分，给定一个原始的出入向量，可以得到：通过decoder的过程，我们可以得到最终输出，目标函数是最小化输入和输出的误差：第二步，对Auto-encoder算法加入惩罚因子。在图片的自编码中，由于原始输入的每一维度都不为0，但是在网络结构中，我们将节点和其他节点的关系（0、1关系/权重关系）作为该节点的原始输入。对一些节点多、连接少的网络结构而言，原始输入十分稀疏，这会导致encode/decode后的向量更趋向零向量。因此将目标函数改为如下：其中，表示Hadamard乘法,，如果，则,否则。第三步，学习节点间的关系信息。这是这篇论文在Auto-encoder较大的改进应用。作者认为两个节点直接相连的时候，那么最终的短处向量y就应该更相似。基于这个目标，能够得到如下loss函数：其中就是我们需要的固定维度的最终向量。最终，基于以上这些，作者给出了最终的目标函数：前两项在在上面已经解释过，最后一项是个正则化项防止过拟合，定义如下：关于求解过程不再具体阐述。以上就是SDNE的模型框架。 DNGRNDGR算法来源于论文《Deep Neural Networks for Learning Graph Representations》，这篇论文和SDNE算法有异曲同工之妙，都应用了auto-encoder，但在如何学习节点间连接信息的方法上却另辟蹊径。SDNE为了学习到邻接节点的相似信息，在目标函数中加入中间结果的loss函数（具体见上文），而本文则根据邻接信息改变每个节点的原始表达，作为自编码模型的输入。由于输入向量本身就含有网络连接信息，因此经过“压缩”后的向量也能表达节点关系。具体我们一步步来介绍。第一步，通过random surfing模型得到概率共现矩阵。作者认为将网络节点转化为序列可能存在一些问题，比如序列长度，采样次数，这些都不好确定。根据PageRank算法的启发，作者希望通过概率转移矩阵/邻接矩阵A学习到网络结构信息。由于步骤较为简单且便于理解，此处直接通过matlab代码来说明:其中A是原始邻接/概率转移矩阵，alpha是平滑参数，决定节点是否需要转移。P0和M矩阵初始分别是单位矩阵和全0矩阵，M是最终产出结果。ScaleSimMat是对矩阵进行主对角线置0后，对每列进行归一化。第二步是得到PPMI矩阵，对上步得到的矩阵进行处理。其中PMI(pointwise mutual information，点互信息)定义如下：在PMI的基础上，对值为负的元素，全部置为0，得到PPMI矩阵。第三步是进行自编码，论文作者使用了SDAE(stacked denoising autoencoders)编码。SDAE和AE的差距不大，主要区别是在训练之前对input向量进行了容错处理，具体做法是按照一定的方式对input向量中的少部分元素置0，这样可以在某种程度上对输入向量的依赖程度，提高整体的学习效果。具体encode/decode的过程和AE基本一致，可以在SDNE方法中看到。DNGR方法在编码之前，通过转移矩阵学习到节点间的连接信息，和SDNE有共通之处。但在实际应用过程中却会遇到很多问题，可以在上列步骤中看到需要进行矩阵运算，当网络节点很多时，就要面临大型矩阵的运算问题，需要单独将矩阵运算部分拿出来，利用Hadoop等方式运算（主要是大型矩阵的存储）。笔者尝试一般超过5W节点后基本单机是难以直接实现的，因此在工业实现上还需要具体改进算法。DNGR工具包：DNGR（matlab） DNGR-Keras(python) 实践在实际工作中，考虑到效率和性能，我们一般会选择最简单的模型进行尝试，因此我们选用Deepwalk作为我们探索query和site相关性的首选模型。数据来源：从User搜索日志中，随机选择60W query，打压至搜狗图片搜索，爬取首页结果。可以得到大约60w * 24 对query和site的关系(图片结果页首页有24张)。数据生成：通过query和site的链接关系，生成基于query-site的二部图，再通过邻接表的形式，将二部图表示出来。如下如所示：向量生成：将这个处理后的文件作为Deepwalk的输入，之后就可以通过类似word2vec的方式显示效果了。未登录query处理：可能你已经发现，上面生成的向量，只包含选取的那些query和site，大约21Wquery和40Wsite，针对未登录的site，我们认为很可能站点太小或者有是垃圾站点，对整体效果影响不大，因此忽略不计。这里主要针对未登录的query进行处理。最直接的想法，当一个新的query到来时，是否可以从已知query中找到一个语义最相近的query来表示这个未登录query呢？然而，这个方法局限性也很大，很难保证已知query的语义都能覆盖到。那么进一步地思考，是否我们可以通过query作为桥梁，学习site空间和文本空间的一个映射关系呢？有了这个关系，我们就可以将文本空间的query映射到site空间，进而可以和site计算相似度了。这里我们采用了 DSSM (Deep Structured Semantic Model)来学习不同语义空间的关系。这样，对于任意一个query，我们都可以获取与其相关的site信息了。Demo展示：为了方便观察，我们搭建了一个Demo观察效果。可以看到，有明显图片意图的query或者垂直类站点效果都不错，但是比如http://nipic.com这种综合类的站点，对应的query种类也比较多，效果并不理想。 小结：以上五种方法中，整体来说deepwalk和node2vector一脉相承，DNGR和SDNE异曲同工，LINE在没有直接借助现成工具，但思想上和前面两种方法更接近。从效果上来说，deepwalk在大型网络上比小网络上效果要好，源于其完全随机的游走方式需要大数据位基础，在小型网络上 node2vector 比 deepwalk 效果整体好一些。在具体应用上, deepwalk 和 LINE 对大数据的适应性更好，其余三种方法原始代码对大型网络（节点&gt;5W）难以实现，需要通过其他方式改进。原文链接","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"http://www.wortyby.com/tags/线性代数/"},{"name":"搜索","slug":"搜索","permalink":"http://www.wortyby.com/tags/搜索/"},{"name":"关联关系","slug":"关联关系","permalink":"http://www.wortyby.com/tags/关联关系/"},{"name":"概率论","slug":"概率论","permalink":"http://www.wortyby.com/tags/概率论/"}]},{"title":"主成分分析(PCA)和奇异值分解(SVD)","slug":"主成分分析(PCA)和奇异值分解(SVD)","date":"2018-10-12T06:00:24.000Z","updated":"2018-10-12T06:00:24.000Z","comments":true,"path":"2018/10/12/主成分分析(PCA)和奇异值分解(SVD)/","link":"","permalink":"http://www.wortyby.com/2018/10/12/主成分分析(PCA)和奇异值分解(SVD)/","excerpt":"","text":"引子特征抽取的目标是根据原始的d个特征的组合形成k个新的特征，即将数据从d维空间映射到k维空间。 在文本分析领域常用的降维方法是主成分分析（PrincipalComponent Analysis, PCA）和奇异值分解（Singular ValueDecomposition， SVD）。在下文的叙述中，将沿袭机器学习的常用符号，使用x表示一个列向量，它是样本x在d维空间中的点。而由n个样本构成的数据集可以表示为一个d×n的矩阵X PCAPCA背后的数学基础是特征值分析，即Σv=λv,v是特征向量，λ是特征值。PCA的目标是最大化数据间累积方差。 PCA 一般过程是去除平均值:means;计算X的协方差矩阵 Σ;计算 Σ 的特征向量和特征值(特征向量用列向量 v_d×1 表示)；将特征值从大到小排序；保留最上面的k个特征向量（这k个特征向量保证了数据映射到特征值最大的特征向量的方向时，数据间的累积方差最大，数据映射到第二大的特征向量时，数据间的累积方差次之，且特征向量之间保持正交）构成的特征向量矩阵 V_d×k;将数据转换到上述k个特征向量构建的新空间中V^T*X=A_k×n +means,A是一个k×n的矩阵） SVDSVD将原始的数据集X分解成三个矩阵U，Σ和V^T:X_d×n=U_d×dΣ_d×nV^T_n×n其中是对角矩阵，对角元素成为奇异值，这些奇异值是矩阵X*X^T特征值的平方根。将奇异值从大到小排序后，若设定新空间的维度为k（kV^T_k×n和U_d×k。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"http://www.wortyby.com/tags/特征工程/"},{"name":"特征向量","slug":"特征向量","permalink":"http://www.wortyby.com/tags/特征向量/"},{"name":"线性代数","slug":"线性代数","permalink":"http://www.wortyby.com/tags/线性代数/"},{"name":"PCA","slug":"PCA","permalink":"http://www.wortyby.com/tags/PCA/"},{"name":"SVD","slug":"SVD","permalink":"http://www.wortyby.com/tags/SVD/"}]},{"title":"PCA 的数学原理","slug":"PCA的数学原理","date":"2018-10-12T05:46:26.000Z","updated":"2018-10-12T05:46:26.000Z","comments":true,"path":"2018/10/12/PCA的数学原理/","link":"","permalink":"http://www.wortyby.com/2018/10/12/PCA的数学原理/","excerpt":"","text":"引子矩阵相乘本质是进行降维变换;线性变换是求得尽可能的不相关信息来最大表征原始信息的过程;基础解系是用来将线性相关的特征进行线性无关变换的数学工具。但向量组的基础解系不唯一，所以我们需要找到一个能尽可能独立的特征向量组，就需要借助 方差与协方差 来进行分析特征向量分布状况，从而找到能尽可能表征向量组的基础解系。这个过程就是 PCA?对于高阶相关性数据，可以考虑 kernel PCA 。有了上面谈到的数学背景，我们今天就来谈谈PCA(Principal Component Analysis)的数学原理。 PCA 的数学原理PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。当然我并不打算把文章写成纯数学文章，而是希望用直观和易懂的方式叙述PCA的数学原理，所以整个文章不会引入严格的数学推导。希望读者在看完这篇文章后能更好的明白PCA的工作原理。 数据的向量表示及降维问题一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下：(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子:(500,240,25,13,2312.15)T注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指列向量。我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。 向量的表示及基变换既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。 内积与投影下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为：(a1,a2,,an)T(b1,b2,,bn)T=a1b1+a2b2++anbn内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则A=(x1,y1)B=(x2,y2)则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：好，现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为|A|cos(a)其中|A|=x12+y12是向量A的模，也就是A线段的标量长度。注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式：AB=|A||B|cos(a)现在事情似乎是有点眉目了：A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让|B|=1那么就变成了：AB=|A|cos(a)也就是说，设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。 基下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量：在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。不过我们常常忽略，只有一个(3,2)本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。更正式的说，向量(x,y)实际上表示线性组合：x(1,0)T+y(0,1)T不难证明所有二维向量都可以表示为这样的线性组合。此处(1,0)和(0,1)叫做二维空间中的一组基。所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。我们之所以默认选择(1,0)和(0,1)为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。例如，(1,1)和(-1,1)也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为(12,12)和 (12,12)现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为(52,12)下图给出了新的基以及(3,2)在新基上坐标值的示意图：另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。 基变换的矩阵表示下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将(3,2)变换为新基上的坐标，就是用(3,2)与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换：(1/21/21/21/2)(32)=(5/21/2)太漂亮了！其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示：(1/21/21/21/2)(123123)=(2/24/26/2000)于是一组向量的基变换被干净的表示为矩阵的相乘。一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。数学表示为：(p1p2pR)(a1a2aM)=(p1a1p1a2p1aMp2a1p2a2p2aMpRa1pRa2pRaM)其中pi是一个行向量，表示第i个基，aj是一个列向量，表示第j个原始数据记录。特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。 协方差矩阵及优化目标上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式：(1124213344)其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后(1102020011)我们可以看下五条数据在平面直角坐标系内的样子：现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。下面，我们用数学方法表述这个问题。 方差上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：Var(a)=1m∑i=1m(aiμ)2由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：Var(a)=1m∑i=1mai2于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。 协方差对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：Cov(a,b)=1m∑i=1maibi可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。 协方差矩阵上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X:X=(a1a2amb1b2bm)然后我们用X乘以X的转置，并乘上系数1/m1mXXT=(1m∑i=1mai21m∑i=1maibi1m∑i=1maibi1m∑i=1mbi2)奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设C=1mXXT则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。 协方差矩阵对角化根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：D=1mYYT=1m(PX)(PX)T=1mPXXTPT=P(1mXXT)PT=PCPT现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足PCPT是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。至此，我们离“发明”PCA还有仅一步之遥！现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：实对称矩阵不同特征值对应的特征向量必然正交。设特征向量λ重数为r，则必然存在r个线性无关的特征向量对应于λ，因此可以将这r个特征向量单位正交化。由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为e1,e2,,en，我们将其按列组成矩阵：E=(e1e2en)则对协方差矩阵C有如下结论：ETCE=Λ=(λ1λ2λn)其中Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。到这里，我们发现我们已经找到了需要的矩阵P：P=ETP是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照Λ中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。 算法及实例为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。 PCA算法总结一下PCA的算法步骤：设有m条n维数据。将原始数据按列组成n行m列矩阵X将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值求出协方差矩阵C=1mXXT求出协方差矩阵的特征值及对应的特征向量将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵PY=PX 即为降维到k维后的数据 实例这里以上文提到的(1102020011)为例，我们用PCA方法将这组二维数据其降到一维。因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：C=15(1102020011)(1210002101)=(65454565)然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为：λ1=2,λ2=2/5其对应的特征向量分别是：c1(11),c2(11)其中对应的特征向量分别是一个通解，c1和c2可取任意实数。那么标准化后的特征向量为：(1/21/2),(1/21/2)因此我们的矩阵P是：P=(1/21/21/21/2)可以验证协方差矩阵C的对角化：PCPT=(1/21/21/21/2)(6/54/54/56/5)(1/21/21/21/2)=(2002/5)最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：Y=(1/21/2)(1102020011)=(3/21/203/21/2)降维投影结果如下图： 进一步讨论根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"特征工程","slug":"特征工程","permalink":"http://www.wortyby.com/tags/特征工程/"},{"name":"特征向量","slug":"特征向量","permalink":"http://www.wortyby.com/tags/特征向量/"},{"name":"线性代数","slug":"线性代数","permalink":"http://www.wortyby.com/tags/线性代数/"},{"name":"概率论","slug":"概率论","permalink":"http://www.wortyby.com/tags/概率论/"}]},{"title":"Apache Kylin Cube 高级设置","slug":"Apache Kylin Cube 高级设置","date":"2018-10-12T01:47:42.000Z","updated":"2018-10-12T01:47:42.000Z","comments":true,"path":"2018/10/12/Apache Kylin Cube 高级设置/","link":"","permalink":"http://www.wortyby.com/2018/10/12/Apache Kylin Cube 高级设置/","excerpt":"","text":"引子随着维度数目的增加，Cuboid 的数量会爆炸式地增长。为了缓解 Cube 的构建压力，Apache Kylin 引入了一系列的高级设置，帮助用户筛选出真正需要的 Cuboid。这些高级设置包括聚合组（Aggregation Group）、联合维度（Joint Dimension）、层级维度（Hierachy Dimension）和必要维度（Mandatory Dimension）等。”众所周知，Apache Kylin 的主要工作就是为源数据构建 N 个维度的 Cube，实现聚合的预计算。理论上而言，构建 N 个维度的 Cube 会生成 2N 个 Cuboid， 如图 1 所示，构建一个 4 个维度（A，B，C, D）的 Cube，需要生成 16 个Cuboid。图1随着维度数目的增加 Cuboid 的数量会爆炸式地增长，不仅占用大量的存储空间还会延长 Cube 的构建时间。为了缓解 Cube 的构建压力，减少生成的 Cuboid 数目，Apache Kylin 引入了一系列的高级设置，帮助用户筛选出真正需要的 Cuboid。这些高级设置包括 聚合组（Aggregation Group）、联合维度（Joint Dimension）、**层级维度（Hierachy Dimension）和必要维度（Mandatory Dimension）**等，本系列将深入讲解这些高级设置的含义及其适用的场景。 聚合组（Aggregation Group）**用户根据自己关注的维度组合，可以划分出自己关注的组合大类，这些大类在 Apache Kylin 里面被称为聚合组。**例如图 1 中展示的 Cube，如果用户仅仅关注维度 AB 组合和维度 CD 组合，那么该 Cube 则可以被分化成两个聚合组，分别是聚合组 AB 和聚合组 CD。如图 2 所示，生成的 Cuboid 数目从 16 个缩减成了 8 个。图2用户关心的聚合组之间可能包含相同的维度，例如聚合组 ABC 和聚合组 BCD 都包含维度 B 和维度 C。这些聚合组之间会衍生出相同的 Cuboid，例如聚合组 ABC 会产生 Cuboid BC，聚合组 BCD 也会产生 Cuboid BC。这些 Cuboid不会被重复生成，一份 Cuboid 为这些聚合组所共有，如图 3 所示。图3有了聚合组用户就可以粗粒度地对 Cuboid 进行筛选，获取自己想要的维度组合。 聚合组应用实例假设创建一个交易数据的 Cube，它包含了以下一些维度：顾客 ID buyer_id 交易日期 cal_dt、付款的方式 pay_type 和买家所在的城市 city。有时候，分析师需要通过分组聚合 city、cal_dt 和 pay_type 来获知不同消费方式在不同城市的应用情况；有时候，分析师需要通过聚合 city 、cal_dt 和 buyer_id，来查看顾客在不同城市的消费行为。在上述的实例中，推荐建立两个聚合组，包含的维度和方式如图 4 ：图4聚合组 1： [cal_dt, city, pay_type]聚合组 2： [cal_dt, city, buyer_id]在不考虑其他干扰因素的情况下，这样的聚合组将节省不必要的 3 个 Cuboid: [pay_type, buyer_id]、[city, pay_type, buyer_id] 和 [cal_dt, pay_type, buyer_id] 等，节省了存储资源和构建的执行时间。 Case 1:1SELECT cal_dt, city, pay_type, count(*) FROM table GROUP BY cal_dt, city, pay_type则将从 Cuboid [cal_dt, city, pay_type] 中获取数据。 Case2:1SELECT cal_dt, city, buy_id, count(*) FROM table GROUP BY cal_dt, city, buyer_id则将从 Cuboid [cal_dt, city, pay_type] 中获取数据。 Case3 如果有一条不常用的查询:1SELECT pay_type, buyer_id, count(*) FROM table GROUP BY pay_type, buyer_id则没有现成的完全匹配的 Cuboid。此时，Apache Kylin 会通过在线计算的方式，从现有的 Cuboid 中计算出最终结果。 联合维度（Joint Dimension）用户有时并不关心维度之间各种细节的组合方式，例如用户的查询语句中仅仅会出现 group by A, B, C，而不会出现 group by A, B 或者 group by C 等等这些细化的维度组合。这一类问题就是联合维度所解决的问题。例如将维度 A、B 和 C 定义为联合维度，Apache Kylin 就仅仅会构建 Cuboid ABC，而 Cuboid AB、BC、A 等等Cuboid 都不会被生成。最终的 Cube 结果如图5所示，Cuboid 数目从 16 减少到 4。图5 联合维度应用实例假设创建一个交易数据的Cube，它具有很多普通的维度，像是交易日期 cal_dt，交易的城市 city，顾客性别 sex_id 和支付类型 pay_type 等。分析师常用的分析方法为通过按照交易时间、交易地点和顾客性别来聚合，获取不同城市男女顾客间不同的消费偏好，例如同时聚合交易日期 cal_dt、交易的城市 city 和顾客性别 sex_id 来分组。在上述的实例中，推荐在已有的聚合组中建立一组联合维度，包含的维度和组合方式如图6：图6聚合组：[cal_dt, city, sex_id，pay_type]联合维度： [cal_dt, city, sex_id] Case 1：1SELECT cal_dt, city, sex_id, count(*) FROM table GROUP BY cal_dt, city, sex_id则它将从Cuboid [cal_dt, city, sex_id]中获取数据 Case2如果有一条不常用的查询：1SELECT cal_dt, city, count(*) FROM table GROUP BY cal_dt, city则没有现成的完全匹配的 Cuboid，Apache Kylin 会通过在线计算的方式，从现有的 Cuboid 中计算出最终结果。 层级维度（Hierarchy Dimension）用户选择的维度中常常会出现具有层级关系的维度。例如对于国家（country）、省份（province）和城市（city）这三个维度，从上而下来说国家／省份／城市之间分别是一对多的关系。也就是说，用户对于这三个维度的查询可以归类为以下三类:group by countrygroup by country, province（等同于group by province）group by country, province, city（等同于 group by country, city 或者group by city）以图7所示的 Cube 为例，假设维度 A 代表国家，维度 B 代表省份，维度 C 代表城市，那么ABC 三个维度可以被设置为层级维度，生成的Cube 如图7所示。图7例如，Cuboid [A,C,D]=Cuboid[A, B, C, D]，Cuboid[B, D]=Cuboid[A, B, D]，因而 Cuboid[A, C, D] 和 Cuboid[B, D] 就不必重复存储。图8展示了 Kylin 按照前文的方法将冗余的Cuboid 剪枝从而形成图 2 的 Cube 结构，Cuboid 数目从 16 减小到 8。图8 层级维度应用实例假设一个交易数据的 Cube，它具有很多普通的维度，像是交易的城市 city，交易的省 province，交易的国家 country， 和支付类型 pay_type等。分析师可以通过按照交易城市、交易省份、交易国家和支付类型来聚合，获取不同层级的地理位置消费者的支付偏好。在上述的实例中，建议在已有的聚合组中建立一组层级维度（国家country／省province／城市city），包含的维度和组合方式如图9：图9聚合组：[country, province, city，pay_type]层级维度： [country, province, city] Case 1 当分析师想从城市维度获取消费偏好时：1SELECT city, pay_type, count(*) FROM table GROUP BY city, pay_type则它将从 Cuboid [country, province, city, pay_type] 中获取数据。 Case 2 当分析师想从省级维度获取消费偏好时：1SELECT province, pay_type, count(*) FROM table GROUP BY province, pay_type则它将从Cuboid [country, province, pay_type] 中获取数据。 Case 3 当分析师想从国家维度获取消费偏好时：1SELECT country, pay_type, count(*) FROM table GROUP BY country, pay_type则它将从Cuboid [country, pay_type] 中获取数据。 Case 4 如果分析师想获取不同粒度地理维度的聚合结果时：无一例外都可以由图 3 中的 cuboid 提供数据 。例如，SELECT country, city, count(*) FROM table GROUP BY country, city 则它将从 Cuboid [country, province, city] 中获取数据。 必要维度 （Mandatory Dimension）用户有时会对某一个或几个维度特别感兴趣，所有的查询请求中都存在group by这个维度，那么这个维度就被称为必要维度，只有包含此维度的Cuboid会被生成（如图10）。图10以图 1中的Cube为例，假设维度A是必要维度，那么生成的Cube则如图11所示，维度数目从16变为9。图11 必要维度应用实例假设一个交易数据的Cube，它具有很多普通的维度，像是交易时间order_dt，交易的地点location，交易的商品product和支付类型pay_type等。其中，交易时间就是一个被高频作为分组条件（group by）的维度。 如果将交易时间order_dt设置为必要维度，包含的维度和组合方式如图12：图12 系列总结根据本系列的原理介绍，在Kylin的高级设置中，用户可以根据查询需求对Cube构建预计算的结果进行优化（剪枝），从而减少占用的存储空间。 而优化得当的Cube可以在占用尽量少的存储空间的同时提供极强的查询性能。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"KYLIN","slug":"KYLIN","permalink":"http://www.wortyby.com/tags/KYLIN/"},{"name":"CUBE","slug":"CUBE","permalink":"http://www.wortyby.com/tags/CUBE/"}]},{"title":"Apache Kylin Cube 构建原理","slug":"Apache Kylin Cube 构建原理","date":"2018-10-08T11:43:25.000Z","updated":"2018-10-08T11:43:25.000Z","comments":true,"path":"2018/10/08/Apache Kylin Cube 构建原理/","link":"","permalink":"http://www.wortyby.com/2018/10/08/Apache Kylin Cube 构建原理/","excerpt":"","text":"本文主要介绍了Apache Kylin是如何将Hive表中的数据转化为HBase的KV结构，并简单介绍了Kylin的SQL查询是如何转化为HBase的Scan操作。 Apache Kylin 是什么Apache Kylin是一个开源的、基于Hadoop生态系统的OLAP查询引擎，能够通过SQL接口对十亿、甚至百亿行的超大数据集实现秒级的多维分析查询。 OLAP 是什么即联机分析处理：以复杂的分析型查询为主，需要扫描，聚合大量数据。 Kylin如何实现超大数据集的秒级多维分析查询预计算对于超大数据集的复杂查询，既然现场计算需要花费较长时间，那么根据空间换时间的原理，我们就可以提前将所有可能的计算结果计算并存储下来，从而实现超大数据集的秒级多维分析查询。 Kylin的预计算是如何实现的将数据源Hive表中的数据按照指定的维度和指标 由计算引擎MapReduce离线计算出所有可能的查询结果(即Cube)存储到HBase中。 Cube 和 Cuboid是什么简单地说，一个cube就是一个Hive表的数据按照指定维度与指标计算出的所有组合结果。其中每一种维度组合称为cuboid，一个cuboid包含一种具体维度组合下所有指标的值。如下图，整个立方体称为1个cube，立方体中每个网格点称为1个cuboid，图中（A,B,C,D）和（A，D）都是cuboid，特别的，（A,B,C,D）称为Base cuboid。cube的计算过程是逐层计算的，首先计算Base cuboid，然后计算维度数依次减少，逐层向下计算每层的cuboid。图1 Cuboid 的维度和指标如何转换为HBase的KV结构简单的说Cuboid的维度会映射为HBase的Rowkey，Cuboid的指标会映射为HBase的Value。如下图所示： 图2如上图原始表所示：Hive表有两个维度列 year 和 city，有一个指标列 price。如上图预聚合表所示：我们具体要计算的是 year 和 city 这两个维度所有维度组合（即4个cuboid）下的 sum(priece) 指标，这个指标的具体计算过程就是由MapReduce完成的。如上图字典编码所示：为了节省存储资源，Kylin对维度值进行了字典编码。图中将 beijing 和shanghai 依次编码为0和1。如上图HBase KV存储所示：在计算cuboid过程中，会将Hive表的数据转化为HBase的KV形式。Rowkey的具体格式是 cuboid id + 具体的维度值（最新的Rowkey中为了并发查询还加入了ShardKey），以预聚合表内容的第2行为例，其维度组合是（year，city），所以cuboid id就是00000011，cuboid是8位，具体维度值是1994和shanghai，所以编码后的维度值对应上图的字典编码也是11，所以HBase的Rowkey就是0000001111，对应的HBase Value就是 sum(priece) 的具体值。所有的cuboid计算完成后，会将cuboid转化为HBase的 KeyValue 格式生成HBase的HFile，最后将HFile load进cube对应的HBase表中。 Cube 构建过程重要源码分析 从Hive表生成Base Cuboid在实际的cube构建过程中，会首先根据cube的Hive事实表和维表生成一张大宽表，然后计算大宽表列的基数，建立维度字典，估算cuboid的大小，建立cube对应的HBase表，再计算base cuboid。计算base cuboid就是一个MapReduce作业，其输入是上面提到的Hive大宽表，输出是的key是各种维度组合，value是Hive大宽表中指标的值。1234567891011121314org.apache.kylin.engine.mr.steps.BaseCuboidJobmap 阶段生成key-value的代码如下： protected void outputKV(Context context) throws IOException, InterruptedException &#123; intermediateTableDesc.sanityCheck(bytesSplitter); byte[] rowKey = buildKey(bytesSplitter.getSplitBuffers()); outputKey.set(rowKey, 0, rowKey.length); ByteBuffer valueBuf = buildValue(bytesSplitter.getSplitBuffers()); outputValue.set(valueBuf.array(), 0, valueBuf.position()); context.write(outputKey, outputValue); &#125;所有计算cuboid的reduce阶段代码都一样，见下面。 从Base Cuboid 逐层计算 Cuboid。从base cuboid 逐层计算每层的cuboid，也是MapReduce作业，map阶段每层维度数依次减少，reduce阶段对指标进行聚合。12345678910111213141516171819org.apache.kylin.engine.mr.steps.CuboidReducer public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; aggs.reset(); //MeasureAggregators 根据每种指标的不同类型对指标进行聚合 for (Text value : values) &#123; codec.decode(ByteBuffer.wrap(value.getBytes(), 0, value.getLength()), input); if (cuboidLevel &gt; 0) &#123; // Base Cuboid 的 cuboidLevel 是0 aggs.aggregate(input, needAggr); //指标进行进一步聚合 &#125; else &#123; aggs.aggregate(input); &#125; &#125; aggs.collectStates(result); ByteBuffer valueBuf = codec.encode(result); outputValue.set(valueBuf.array(), 0, valueBuf.position()); context.write(key, outputValue); Cuboid 转化为HBase的HFile。主要就是数据格式的转化。详情请参考： Hive 数据 bulkload 导入 HBase 不同类型的指标是如何进行聚合的每种不同的指标都会有对应的聚合算法，所有指标聚合的基类是org.apache.kylin.measure.MeasureAggregator 。其核心方法如下：12345abstract public void reset();//不同类型的指标算法会实现该方法abstract public void aggregate(V value);abstract public V getState();以最简单的long类型的sum指标为例：12345678910111213141516171819public class LongSumAggregator extends MeasureAggregator&lt;LongMutable&gt; &#123; LongMutable sum = new LongMutable(); @Override public void reset() &#123; sum.set(0); &#125; @Override public void aggregate(LongMutable value) &#123; sum.set(sum.get() + value.get()); &#125; @Override public LongMutable getState() &#123; return sum; &#125;&#125; SQL查询是如何转化为HBase的Scan操作的还是以图2举例，假设查询SQL如下：1234select year, sum(price)from tablewhere city = &quot;beijing&quot;group by year这个SQL涉及维度 year 和 city，所以其对应的cuboid是00000011，又因为city的值是确定的 beijing,所以在Scan HBase时就会Scan Rowkey以00000011开头且city的值是 beijing 的行，取到对应指标 sum(price)的值，返回给用户。 总结本文主要介绍了Apache Kylin是如何将Hive表中的数据转化为HBase的KV结构，并简单介绍了Kylin的SQL查询是如何转化为HBase的Scan操作。希望对大家有所帮助。原文链接","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"KYLIN","slug":"KYLIN","permalink":"http://www.wortyby.com/tags/KYLIN/"},{"name":"CUBE","slug":"CUBE","permalink":"http://www.wortyby.com/tags/CUBE/"}]},{"title":"PySpark 之 DataFrame","slug":"PySpark 之 DataFrame","date":"2018-09-30T05:36:51.000Z","updated":"2018-09-30T05:36:51.000Z","comments":true,"path":"2018/09/30/PySpark 之 DataFrame/","link":"","permalink":"http://www.wortyby.com/2018/09/30/PySpark 之 DataFrame/","excerpt":"","text":"背景DataFrame是在Spark 1.3中正式引入的一种以RDD为基础的不可变的分布式数据集，类似于传统数据库的二维表格，数据在其中以列的形式被组织存储。如果熟悉Pandas，其与Pandas DataFrame是非常类似的东西。DataFrame API受到R和Python（Pandas）中的数据框架的启发，但是从底层开始设计以支持现代大数据和数据科学应用程序。作为现有RDD API的扩展，DataFrame具有以下功能：能够从单台笔记本电脑上的千字节数据扩展到大型群集上的PB级数据支持各种数据格式和存储系统通过Spark SQL Catalyst优化器实现最先进的优化和代码生成通过Spark无缝集成所有大数据工具和基础架构Python，Java，Scala和R的API（通过SparkR开发）对于熟悉其他编程语言数据框架的新用户，此API应该让他们感到宾至如归。对于现有的Spark用户，此扩展API将使Spark更易于编程，同时通过智能优化和代码生成来提高性能。通过DataFrame与Catalyst优化器，现有的Spark程序迁移到DataFrame时性能得到改善。由于优化器生成用于执行的JVM字节码，因此Python用户将体验到与Scala和Java用户相同的高性能。 创建DataFrameSpark中有两种方式可以将数据从RDD转化为DataFrame：反射推断或者编程指定。反射推断是Spark应用程序自动识列的类型，然后通过Spark SQL将行对象的RDD转换为DataFrame。编程指定则是在运行之前，人工从Spark SQL中引入数据类型分配给不同的列。 使用数据结构：普通读取csv为DataFrames数据。123456#### 读取csv为DataFrametraffic = spark.read.csv(&apos;E:\\Documents\\Desktop\\data.csv&apos;, header=&apos;true&apos;)#### 创建临时表traffic.createOrReplaceTempView(&quot;traffic&quot;)#### 显示前10行traffic.show(10)打印表结构，可以看出Spark自动将所有列推断为string，这不是我们想要的类型。1traffic.printSchema()通过pandas辅助读取csv。123456import pandas as pd df = pd.read_csv(&apos;E:\\Documents\\Desktop\\data.csv&apos;) traffic = spark.createDataFrame(df)traffic.createOrReplaceTempView(&quot;traffic&quot;)traffic.printSchema() 反射推断1234traffic = spark.read.csv(&apos;E:\\Documents\\Desktop\\data.csv&apos;, header=&apos;true&apos;, inferSchema=&apos;true&apos;)traffic.createOrReplaceTempView(&quot;traffic&quot;)traffic.show(10)traffic.printSchema()inferSchema属性用来指示是否使用自动推断，默认为False。 编程指定尽管自动推断比较方便，如果启用了inferSchema，则函数将数据全部读入以确定输入模式。要避免遍历整个数据一次，应该使用模式明确指定模式。StructField(field, data_type=None, nullable=True, metadata=None)field – Either the name of the field or a StructField objectdata_type – If present, the DataType of the StructField to createnullable – Whether the field to add should be nullable (default True)metadata – Any additional metadata (default None)123456789101112131415from pyspark.sql.types import *# 指定DataFrame每个列的模式schema = StructType([... StructField(&quot;detectorid&quot;, IntegerType()),... StructField(&quot;starttime&quot;,StringType()),... StructField(&quot;volume&quot;, IntegerType()),... StructField(&quot;speed&quot;, FloatType()),... StructField(&quot;occupancy&quot;, FloatType())])# 使用指定模式读入traffic = spark.read.csv(&apos;E:\\Documents\\Desktop\\data.csv&apos;, header=&apos;true&apos;, schema=schema)traffic.createOrReplaceTempView(&quot;traffic&quot;)traffic.show(10)traffic.printSchema() DataFrame查询 常用API select()投影一组表达式并返回一个新的DataFrame。参数：cols - 列名称（字符串）或表达式（列）的列表。 如果其中一个列名是’*’，则该列将展开以包含当前DataFrame中的所有列。1234567891011&gt;&gt;&gt; traffic.select(&quot;speed&quot;).show(5)+-----+|speed|+-----+|56.52||53.54||54.64||54.94||51.65|+-----+only showing top 5 rows filter()使用给定的条件过滤行。where()是filter()的别名。参数：condition - 类型的一列.BooleanType或一个SQL表达式的字符串。12345678910111213141516171819202122&gt;&gt;&gt; traffic.filter(traffic.speed &gt; 50).show(5)+----------+--------------+------+-----+---------+|detectorid| starttime|volume|speed|occupancy|+----------+--------------+------+-----+---------+| 100625|2015/12/1 0:00| 48|56.52| 1.29|| 100625|2015/12/1 0:15| 50|53.54| 1.48|| 100625|2015/12/1 0:30| 25|54.64| 0.62|| 100625|2015/12/1 0:45| 34|54.94| 0.85|| 100625|2015/12/1 1:00| 23|51.65| 0.6|+----------+--------------+------+-----+---------+only showing top 5 rows&gt;&gt;&gt; traffic.where(traffic.volume &gt; 50).show(5)+----------+--------------+------+-----+---------+|detectorid| starttime|volume|speed|occupancy|+----------+--------------+------+-----+---------+| 100625|2015/12/1 3:45| 61|57.62| 1.65|| 100625|2015/12/1 4:00| 69| 56.7| 1.89|| 100625|2015/12/1 4:15| 94|56.53| 2.69|| 100625|2015/12/1 4:30| 87|55.53| 2.58|| 100625|2015/12/1 4:45| 161|55.51| 4.62|+----------+--------------+------+-----+---------+only showing top 5 rows drop()返回删除指定列的新DataFrame。参数：cols - 要删除的列的字符串名称，要删除的列或要删除的列的字符串名称的列表。1234567891011&gt;&gt;&gt; traffic.drop(&quot;speed&quot;).show(5)+----------+--------------+------+---------+|detectorid| starttime|volume|occupancy|+----------+--------------+------+---------+| 100625|2015/12/1 0:00| 48| 1.29|| 100625|2015/12/1 0:15| 50| 1.48|| 100625|2015/12/1 0:30| 25| 0.62|| 100625|2015/12/1 0:45| 34| 0.85|| 100625|2015/12/1 1:00| 23| 0.6|+----------+--------------+------+---------+only showing top 5 rows cache()使用默认存储级别（MEMORY_AND_DISK）持久保存DataFrame。1traffic.cache() collect()以Row列表形式返回所有记录。1traffic.collect() show()将前n行打印到控制台。参数：n - 要显示的行数。truncate - 如果设置为True，则默认截断超过20个字符的字符串。 如果设置为大于1的数字，则截断长字符串以截断长度并将其右对齐。1234567891011&gt;&gt;&gt; traffic.show(5)+----------+--------------+------+-----+---------+|detectorid| starttime|volume|speed|occupancy|+----------+--------------+------+-----+---------+| 100625|2015/12/1 0:00| 48|56.52| 1.29|| 100625|2015/12/1 0:15| 50|53.54| 1.48|| 100625|2015/12/1 0:30| 25|54.64| 0.62|| 100625|2015/12/1 0:45| 34|54.94| 0.85|| 100625|2015/12/1 1:00| 23|51.65| 0.6|+----------+--------------+------+-----+---------+only showing top 5 rows count()返回此DataFrame中的行数。12&gt;&gt;&gt; traffic.count()17814 columns以列表形式返回所有列名称。12&gt;&gt;&gt; traffic.columns[&apos;detectorid&apos;, &apos;starttime&apos;, &apos;volume&apos;, &apos;speed&apos;, &apos;occupancy&apos;] dtypes将所有列名称及其数据类型作为列表返回。12&gt;&gt;&gt; traffic.dtypes[(&apos;detectorid&apos;, &apos;int&apos;), (&apos;starttime&apos;, &apos;string&apos;), (&apos;volume&apos;, &apos;int&apos;), (&apos;speed&apos;, &apos;double&apos;), (&apos;occupancy&apos;, &apos;double&apos;)] fillna()替换的空值，别名na.fill()。参数：value - int，long，float，string或dict。 用来替换空值的值。 如果值是字典，则子集将被忽略，并且值必须是从列名（字符串）到替换值的映射。 替换值必须是int，long，float，boolean或string。子集 - 要考虑的列名称的可选列表。 子集中指定的不具有匹配数据类型的列将被忽略。 例如，如果value是一个字符串，并且子集包含一个非字符串列，则非字符串列将被忽略。12&gt;&gt;&gt; traffic.na.fill(10)&gt;&gt;&gt; traffic.na.fill(&#123;&apos;volume&apos;: 0, &apos;speed&apos;: &apos;0&apos;&#125;) corr()以双精度值计算DataFrame的两列的相关性。 目前只支持Pearson Correlation Coefficient。 DataFrame.corr()和DataFrameStatFunctions.corr()是彼此的别名。参数:col1 - 第一列的名称col2 - 第二列的名称方法 - 相关方法。 目前只支持“Pearson”12&gt;&gt;&gt; traffic.corr(&quot;volume&quot;, &quot;speed&quot;)-0.588695158526705 cov()计算给定列的样本协方差（由它们的名称指定）作为双精度值。 DataFrame.cov（）和DataFrameStatFunctions.cov（）是别名。参数：col1 - 第一列的名称col2 - 第二列的名称12&gt;&gt;&gt; traffic.cov(&quot;volume&quot;, &quot;speed&quot;)-1166.285227777989 describe()计算数字和字符串列的统计信息。这包括count，mean，stddev，min和max。 如果未给出列，则此函数将计算所有数字或字符串列的统计信息。1234567891011121314151617181920&gt;&gt;&gt; df.describe().show()+-------+--------------+--------------+------------------+------------------+------------------+|summary| detectorid| starttime| volume| speed| occupancy|+-------+--------------+--------------+------------------+------------------+------------------+| count| 17814| 17814| 17814| 17737| 17814|| mean| 100627.5| null|208.72779836083978| 45.94760105993146|13.775621421354007|| stddev|1.707873064514| null| 129.673023730382|15.010086497913619|13.391984211880049|| min| 100625|2015/12/1 0:00| 0| 1.14| 0.0|| max| 100630|2015/12/9 9:45| 528| 69.33| 73.25|+-------+--------------+--------------+------------------+------------------+------------------+&gt;&gt;&gt; traffic.describe([&apos;speed&apos;]).show()+-------+------------------+|summary| speed|+-------+------------------+| count| 17737|| mean| 45.94760105993146|| stddev|15.010086497913619|| min| 1.14|| max| 69.33|+-------+------------------+ distinct()返回包含此DataFrame中不同行的新DataFrame。12&gt;&gt;&gt; traffic.distinct().count()17814 createOrReplaceGlobalTempView()使用给定名称创建或替换全局临时视图。此临时视图的生命周期与此Spark应用程序相关联。1234&gt;&gt;&gt; traffic.createOrReplaceGlobalTempView(&quot;traffic&quot;)&gt;&gt;&gt; df = spark.sql(&quot;select * from traffic&quot;)&gt;&gt;&gt; df.count()17814 createOrReplaceTempView()使用此DataFrame创建或替换本地临时视图。此临时表的生命周期与用于创建此DataFrame的SparkSession相关联。1234&gt;&gt;&gt; traffic.createOrReplaceTempView(&quot;traffic&quot;)&gt;&gt;&gt; df = spark.sql(&quot;select * from traffic&quot;)&gt;&gt;&gt; df.count()17814 使用SQL查询由于创建了临时表，我们可以对临时表执行sql操作。1234567891011121314151617181920212223242526&gt;&gt;&gt; spark.sql(&quot;select * from traffic where volume &gt; 50 and speed &gt; 50&quot;).show()+----------+---------------+------+-----+---------+|detectorid| starttime|volume|speed|occupancy|+----------+---------------+------+-----+---------+| 100625| 2015/12/1 3:45| 61|57.62| 1.65|| 100625| 2015/12/1 4:00| 69| 56.7| 1.89|| 100625| 2015/12/1 4:15| 94|56.53| 2.69|| 100625| 2015/12/1 4:30| 87|55.53| 2.58|| 100625| 2015/12/1 4:45| 161|55.51| 4.62|| 100625| 2015/12/1 5:00| 203|55.41| 5.96|| 100625| 2015/12/1 5:15| 185|55.14| 6.61|| 100625| 2015/12/1 5:30| 308|52.39| 9.87|| 100625| 2015/12/1 5:45| 343|51.01| 11.49|| 100625|2015/12/1 10:15| 306| 50.6| 11.98|| 100625|2015/12/1 10:30| 334|51.42| 11.53|| 100625|2015/12/1 10:45| 349|52.67| 11.51|| 100625|2015/12/1 11:00| 262|52.36| 10.54|| 100625|2015/12/1 12:00| 255|52.47| 9.36|| 100625|2015/12/1 12:15| 346|50.25| 13.44|| 100625|2015/12/1 12:30| 367| 51.2| 12.47|| 100625|2015/12/1 12:45| 330|52.78| 11.56|| 100625|2015/12/1 13:00| 306|52.36| 12.01|| 100625|2015/12/1 13:30| 371|50.28| 13.93|| 100625|2015/12/1 13:45| 294|50.62| 12.92|+----------+---------------+------+-----+---------+only showing top 20 rows Dataset除了DataFrame，Spark 1.6中还引入了Dataset API，其提供了一种类型安全的面向对象的编程接口，但是其只能在Java与Scala中使用。Python不能使用该API的原因是因为其本身不是一种类型安全的语言。在Spark 2.0中DataFrame API被整合入如Dataset API，DataFrame是Dataset未类型化API的一个别名。未类型化的API：DataFrame = Dataset[Row]类型化的API：Dataset[T]","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"DataFrame","slug":"DataFrame","permalink":"http://www.wortyby.com/tags/DataFrame/"},{"name":"spark","slug":"spark","permalink":"http://www.wortyby.com/tags/spark/"}]},{"title":"大数据分析利器KYLIN 之用户权限篇","slug":"大数据分析利器KYLIN 之用户权限篇","date":"2018-09-30T03:00:38.000Z","updated":"2018-09-30T03:00:38.000Z","comments":true,"path":"2018/09/30/大数据分析利器KYLIN 之用户权限篇/","link":"","permalink":"http://www.wortyby.com/2018/09/30/大数据分析利器KYLIN 之用户权限篇/","excerpt":"","text":"Apache Kylin 部署之不完全指南增加用户java线上运行器OLAP系统解析：Apache Kylin和Baidu Palo哪家强？Apache Kylin Cube 构建原理","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"KYLIN","slug":"KYLIN","permalink":"http://www.wortyby.com/tags/KYLIN/"}]},{"title":"数据项目该做的事情","slug":"数据计划","date":"2018-09-26T07:37:53.000Z","updated":"2018-09-26T07:37:53.000Z","comments":true,"path":"2018/09/26/数据计划/","link":"","permalink":"http://www.wortyby.com/2018/09/26/数据计划/","excerpt":"","text":"autherdateversionseven2018-09-20version 1.0 现状 数据同步: 数据同步器应该统一到Hbase ，hive 查询使用 外部表创建 临时表的方式去做 ETL 或其他的hive 运算。将计算结果再存入HBASE 供实时查询，或是写入 redis 给接口调用。实时数据计算，由行为记录统一提前写入HBase数据迁移，统一由任务调度终端，使用sqoop 倒入Hbase，避免使用python 包装代码。 现状是1.所有数据都放Hive;2.数据迁移，采用定时脚本去同步数据库。 数据检索: 数据字典: 数据字典重新构建与否？现状评估是否能够相对准确的计算相似度。 展望 数据预测:依赖特征工程以及数据挖掘的结果进行数据预测 内容画像;没什么概念 用户画像; 用户帖子偏好 用户音频/视频/节目/专辑偏好 用户图片偏好 预测效果展示平台; 特征工程:利用 hive 结合 kylin 和 superset 可以实时构建用户的不同特征展示 特征提取; 数值特征; 类别特征; 时间特征; 空间特征; 文本特征; 常用模型理解各个模型的区别以及使用场景 逻辑回归 因式分解机 梯度提升 问题数学建模 建立评估指标 分类指标 回归指标 排序指标 样本选择 原数据与训练数据选择 交叉验证 构建pipeline 执行 数据分组，按照指标，进行交叉验证数据预测的概率大小 以及确认是否过拟合？ 数据挖掘 用户画像 用户标识域外平台(微信，微博，知乎…)平台内(设备和UID) 用户特征数据收听发帖评论点赞年龄(平台内外用户，看各个平台用户群分布)收听时间点(平台内外用户，看各个平台用户时间分配) 样本数据随机抽取其他平台的用户特征来匹配收听我们平台内容的可能性大小，相应的去其他平台拉用户做推广？随机的从我们的平台用户特征去匹配其他平台内容，看合作机会做推广？ 标签建模可以采用kylin 与 superset 做实时的标签构建与BI 报表展示,来让运营人员进行对应的运营策略制定。 用户画像实时查询系统方便观测平台内用户状态与热点内容追踪 人群画像分析系统方便观测用户群体分布以及意见领袖产生影响力的领域。 评论挖掘 评论标签提取 文字评论的情感分析 音频评论的情感分析 图片评论的情感分析 标签的实时查询系统 音频挖掘 音频标签提取 音频的音文转写 音频的情感分析 音频挖掘信息的实时查询系统 帖子挖掘 帖子标签提取 帖子的情感分析 帖子的实时查询系统实时跟踪平台内帖子的情绪，防止负面内容传播，在平台内可以先遏制掉负面内容传播的可能性，设定一个警戒值 搜索与推荐根据用户的搜索更新用户的偏好进而去推荐内容; 搜索 构建数据的图关系得到类目的权重以及重要程度 构建图边关系节点向量利用用户的搜索次，生成用户可能感兴趣的推荐搜索词 类目相关性实时查询系统实时的显示平台内用户的搜索；平台也可以利用人群属性和用户属性，动态的推荐用户搜索关键词，引导用户去查看平台希望他看的内容。或是他可能想看的内容 推荐 以域外热点数据来推平台内数据知乎微博。。。 平台内的特征数据推荐根据协同过滤算法推荐用户可能要看的可能性内容。 引导用户完成搜索 用户引导的产品定义与衡量标准 搜索前的引导推荐 查询词 搜索中的引导查询词 补全 搜索后的引导相关性搜索 搜索的实时反馈平台展示平台实时搜索平台外的热门搜索与平台的热门搜索比较，确定平台的定位差异还是归一到平台外的搜索。 数据召回数据消费反馈回炉进行更新模型与特征数据召回有以下几种方式 基于协同过滤的召回 基于位置的召回 基于搜索查询的召回 基于用户关系图的召回 基于用户实时行为的召回 A/B Test 通过abtest 方式来进行效果的测试与比较 数据推送 app端 微信小程序端 举例开车的时候，推荐收听的节目应该以交通主题相关的音频节目相关；休息的时候，应该推荐实时性高的热点帖子内容。睡觉的时候，应该推荐轻松的故事广播为主等等。。。。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"架构项目","slug":"架构项目","permalink":"http://www.wortyby.com/tags/架构项目/"}]},{"title":"腾讯推荐—让您的系统瞬间个性化","slug":"腾讯推荐—让您的系统瞬间个性化","date":"2018-09-26T06:39:48.000Z","updated":"2018-09-26T06:39:48.000Z","comments":true,"path":"2018/09/26/腾讯推荐—让您的系统瞬间个性化/","link":"","permalink":"http://www.wortyby.com/2018/09/26/腾讯推荐—让您的系统瞬间个性化/","excerpt":"","text":"腾讯推荐“腾讯推荐”是腾讯大数据近期大力打造的开放服务平台，旨在集业务接入、数据上报、算法计算、实时推荐和效果监控于一体，对外提供全自动实时精准推荐服务。 腾讯推荐官网: tuijian.qq.com 推荐是什么？文章中提到的推荐均是指在海量的物品中自动为用户选取到感兴趣或合适的信息。就腾讯新闻应用而言，全国各地每天产生的信息总数堪称海量，但屏幕尺寸有限，能给用户展示的新闻也不多，这就涉及到帮助用户在新闻海洋中万里挑一，找到吻合用户兴趣的信息。类似的场景不胜枚举，淘宝天猫，腾讯视频，图片社区，应用宝…互联网下，信息过载已然不可逆转，推荐系统的精确性也因此举足轻重。 腾讯推荐的目标腾讯推荐平台的目标是帮助第三方应用实现个性化推荐功能。我们着力于：将系统资源云服务化，帮助应用节省建设个性化服务的开销。以推荐方式开放用户画像，帮助应用增强个性化服务能力。实现数据接入，算法计算和实时推荐全流程自动化，降低应用构建实时推荐系统的门槛。具体而言，应用开发者仅需上报相应的物品信息和用户行为，平台便会自动结合腾讯用户画像和应用本身的数据对其用户进行个性化推荐。以腾讯新闻应用为例，应用开发者将每天发生的新闻信息以及用户行为包括点击、浏览、评论等数据实时上报到腾讯推荐平台。平台会自动基于腾讯用户画像和行为数据计算出用户与新闻之间的关系。每当用户打开腾讯新闻时，应用便会向推荐平台实时请求当前用户的个性化推荐结果，从而将相关信息展示在用户眼前。目前，腾讯推荐平台支持着公司内20多项核心业务，实践证明个性化推荐服务帮助业务无论在用户体验，产品粘性，还是点击率，转化率等方面均有显著提升，因此是时候将这套实时个性化推荐平台对外开放，以求生态稳定，共生共赢。总的说来,腾讯推荐作为一种云服务，降低了第三方应用构建实时推荐系统的门槛，节省了第三方应用建设个性化服务的系统资源，更将腾讯用户画像以服务的方式对第三方应用开放，增加了应用个性化服务能力。 业务接入按系统指引，用户仅需3步：填写资料、注册业务、添加场景便可完成业务接入。管理员审核通过后，系统会自动为当前业务分配存储和计算资源，同时初始化推荐引擎。在此过程中，系统控制中心会与DSF、TDBank、TDP和TRE各子系统通信，下发当前业务相关配置，所有这一切对用户是透明的。待系统初始化完毕，对用户而言万事俱备，只差数据。 数据上报腾讯推荐基于业务自身上报的ItemInfo（物品信息）和ActionInfo（用户行为），并结合8亿腾讯用户画像提供个性化推荐服务。为方便业务开发者上报ItemInfo和ActionInfo两类数据，目前腾讯推荐提供了C++和Java两种版本的SDK。我们希望业务上报更加完善和准确的行为数据，使得我们提供更加精准推荐服务的同时，也能更好地结合腾讯的用户画像为该产品提供更有价值的属性分析，比如年龄，性别，学历，分布地域，用户偏好，活跃程度等。 用户画像腾讯推荐基于腾讯8亿用户，千亿关系链和众多腾讯系相关产品数据打造了用户覆盖率高且兴趣覆盖度广的统一用户画像。我们基于腾讯众多产品中的行为数据为用户采集丰富的兴趣标签。系统会自动对这些原始标签进行聚类和分类，从而对用户兴趣进行抽象。系统会自动建立标签—&gt;主题（topic）—&gt;类目的映射关系，从而为用户画像进行多粒度、多尺度兴趣刻画。同时系统会根据实时上报的行为，不断更新当前用户的画像兴趣。系统会对上报的物品信息进行文本分析，从而将物品信息抽象并映射到与用户画像相同的主题和类目空间。这样便建立了用户与物品间的泛联系。 推荐策略腾讯推荐平台中，我们提供了5种推荐策略供开发者选用：热度，人群热度，猜你喜欢，猜你喜欢（时间优先）和相关推荐。我们屏蔽策略背后的算法细节与参数，降低用户的使用门槛，同时提供足够的自由度让用户自行选择策略的组合。之所以提供5类推荐策略，除了本身的业务要求外，也建议开发者能基于此在一定程度上解决推荐算法带来的过滤气泡问题，例如将人群热度和猜你喜欢的推荐结果进行穿插后对用户展现。我们会基于上报的点击、曝光行为以及用户画像，采用机器学习方法对当前用户可能点击某物品的概率进行预估。推荐系统内部有多种算法同时ABTest和融合，以求达到推荐效果的最优。综上所述，腾讯推荐平台提供了整套个性化推荐解决方案。过程中，平台将系统资源、用户画像、算法计算以推荐服务的方式对外开放，从而帮助第三方应用节省资源的同时还为其增加了个性化服务能力。平台内部实现了资源初始化、数据处理、算法计算和运维纠错的全流程自动化，因此降低了应用接入门槛，减少了接入时间，增加了服务能力的稳定性。原文","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/tags/推荐/"}]},{"title":"FM(因式分解机推荐算法原理)","slug":"FM(因式分解机推荐算法原理）","date":"2018-09-26T02:59:06.000Z","updated":"2018-09-26T02:59:06.000Z","comments":true,"path":"2018/09/26/FM(因式分解机推荐算法原理）/","link":"","permalink":"http://www.wortyby.com/2018/09/26/FM(因式分解机推荐算法原理）/","excerpt":"","text":"背景对于分解机(Factorization Machines，FM)推荐算法原理，本来想自己单独写一篇的。但是看到peghoty写的FM不光简单易懂，而且排版也非常好，因此转载过来，自己就不再单独写FM了。 原理 目录 预测任务 模型方程 回归分类 学习算法原文","categories":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/categories/推荐/"}],"tags":[{"name":"FM","slug":"FM","permalink":"http://www.wortyby.com/tags/FM/"}]},{"title":"深入浅出ML之Factorization家族","slug":"深入浅出ML之Factorization家族","date":"2018-09-21T09:51:49.000Z","updated":"2018-09-21T09:51:49.000Z","comments":true,"path":"2018/09/21/深入浅出ML之Factorization家族/","link":"","permalink":"http://www.wortyby.com/2018/09/21/深入浅出ML之Factorization家族/","excerpt":"","text":"内容列表 写在前面 因子分解机因子分解机（Factorization Machine，简称FM），又称分解机器。是由Konstanz大学（德国康斯坦茨大学）Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？用户在网站上的行为数据会被Server端以日志的形式记录下来，这些数据通常会存放在多台存储机器的硬盘上。以我浪为例，各产品线纪录的用户行为日志会通过flume等日志收集工具交给数据中心托管，它们负责把数据定时上传至HDFS上，或者由数据中心生成Hive表。我们会发现日志中大多数出现的特征是categorical类型的，这种特征类型的取值仅仅是一个标识，本身并没有实际意义，更不能用其取值比较大小。比如日志中记录了用户访问的频道（channel）信息，如”news”, “auto”, “finance”等。假设channel特征有10个取值，分别为{“auto”,“finance”,“ent”,“news”,“sports”,“mil”,“weather”,“house”,“edu”,“games”}。部分训练数据如下：userchanneluser1sportsuser2newsuser3financeuser4houseuser5eduuser6news……特征 ETL 过程中，需要对categorical型特征进行one-hot编码（独热编码），即将categorical型特征转化为数值型特征。channel特征转化后的结果如下：userchn-autochn-financechn-entchn-newschn-sportschn-milchn-weatherchn-housechn-educhn-gamesuser10000100000user20001000000user30100000000user40000000100user50000000010user60001000000可以发现，由 one-hot编码 带来的数据稀疏性会导致特征空间变大。上面的例子中，一维categorical特征在经过one-hot编码后变成了10维数值型特征。真实应用场景中，未编码前特征总维度可能仅有数十维或者到数百维的categorical型特征，经过one-hot编码后，达到数千万、数亿甚至更高维度的数值特征在业内都是常有的。我组广告和推荐业务的点击预估系统，编码前是特征不到100维，编码后（包括feature hashing）的维度达百万维量级。此外也能发现，特征空间增长的维度取决于categorical型特征的取值个数。在数据稀疏性的现实情况下，我们如何去利用这些特征来提升learning performance？ 特征关联以及表征形式或许在学习过程中考虑特征之间的关联信息。针对特征关联，我们需要讨论两个问题：1. 为什么要考虑特征之间的关联信息？2. 如何表达特征之间的关联？ 为什么要考虑特征之间的关联信息？大量的研究和实际数据分析结果表明：某些特征之间的关联信息（相关度）对事件结果的的发生会产生很大的影响。从实际业务线的广告点击数据分析来看，也正式了这样的结论。 如何表达特征之间的关联？表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征：人工特征工程（数据分析＋人工构造）；通过模型做组合特征的学习（深度学习方法、FM/FFM方法）本章主要讨论FM和FFM用来学习特征之间的关联。我们在《第01章：深入浅出ML之Regression家族》看到的多项式回归模型，其中的交叉因子项xixj就是组合特征最直观的例子。xixj表示特征xi和xj的组合，当xi和xj都非零时，组合特征xixj才有意义。这里我们以二阶多项式模型（degree=2时）为例，来分析和探讨FM原理和参数学习过程。 FM模型表达为了更好的介绍FM模型，我们先从多项式回归、交叉组合特征说起，然后自然地过度到FM模型。二阶多项式回归模型我们先看二阶多项式模型的表达式：y&#x005E;(x):=w0+&#x2211;i=1nwixi&#x23DF;&#x7EBF;&#x6027;&#x56DE;&#x5F52;+&#x2211;i=1n&#x2211;j=i+1nwijxixj&#x23DF;&#x4EA4;&#x53C9;&#x9879;&#xFF08;&#x7EC4;&#x5408;&#x7279;&#x5F81;&#xFF09;(n.ml.1.9.1)其中，n表示样本特征维度，截距 w0∈R,w＝{w1,w2,⋯,wn}∈Rn,wij∈Rn×n 为模型参数。从公式(n.ml.1.9.1)可知，交叉项中的组合特征参数总共有n(n−1)2个。在这里，任意两个交叉项参数wij都是独立的。然而，在数据非常稀疏的实际应用场景中，交叉项参数的学习是很困难的。why？因为我们知道，回归模型的参数w的学习结果就是从训练样本中计算充分统计量（凡是符合指数族分布的模型都具有此性质），而在这里交叉项的每一个参数wij的学习过程需要大量的xi、xj同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“xi和xj都非零”的样本数就会更少。训练样本不充分，学到的参数wij就不是充分统计量结果，导致参数wij不准确，而这会严重影响模型预测的效果（performance）和稳定性。How to do it ?那么，如何在降低数据稀疏问题给模型性能带来的重大影响的同时，有效地解决二阶交叉项参数的学习问题呢？矩阵分解方法已经给出了解决思路。这里借用CMU讨论课中提到的FM课件和美团－深入FFM原理与实践中提到的矩阵分解例子（美团技术团队的分享很赞👍）。在基于Model-Based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。如下图所示。上图把每一个user表示成了一个二维向量，同时也把item表示成一个二维向量，两个向量的内积就是矩阵中user对item的打分。根据矩阵分解的启发，如果把多项式模型中二阶交叉项参数wij组成一个对称矩阵W（对角元素设为正实数），那么这个矩阵就可以分解为W=VVT，V∈Rn×k称为系数矩阵，其中第i行对应着第i维特征的隐向量 (这部分在FM公式解读中详细介绍)。将每个交叉项参数wij用隐向量的内积⟨vi,vj⟩表示，是FM模型的核心思想。下面对FM模型表达式和参数求解过程，给出详细解读。 FM模型表达这里我们只讨论二阶FM模型（degree＝2），其表达式为：y&#x005E;(x):=w0+&#x2211;i=1nwixi+&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj(ml.1.9.1)其中，vi表示第i特征的隐向量，⟨⋅,⋅⟩表示两个长度为k的向量的内积，计算公式为：&#x27E8;vi,vj&#x27E9;:=&#x2211;f=1kvi,f&#x22C5;vj,f(ml.1.9.2) 公式解读： 线性模型＋交叉项直观地看FM模型表达式，前两项是线性回归模型的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将两个互异的特征分量之间的关联信息考虑进来。用交叉项表示组合特征，从而建立特征与结果之间的非线性关系。 交叉项系数 → 隐向量内积由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数wij（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积⟨vi,vj⟩表示wij。具体的，FM求解过程中的做法是：对每一个特征分量xi引入隐向量vi＝(vi,1,vi,2,⋯,vi,k)，利用vivTj内积结果对交叉项的系数wij进行估计，公式表示：w&#x005E;ij:=vivjT隐向量的长度k称为超参数(k∈N+,k≪n)，vi=(vi,1,vi,2,⋯,vi,k)的含义是用k个描述特征的因子来表示第i维特征。根据公式(ml.1.9.1)，二阶交叉项的参数由n⋅n个减少到n⋅k个，远少于二阶多项式模型中的参数数量。此外，参数因子化表示后，使得xhxi的参数与xixj的参数不再相互独立。这样我们就可以在样本稀疏情况下相对合理的估计FM模型交叉项的参数。具体地：&#x27E8;vh,vi&#x27E9;:=&#x2211;f=1kvh,f&#x22C5;vi,f(1)&#x27E8;vi,vj&#x27E9;:=&#x2211;f=1kvi,f&#x22C5;vj,f(2)(n.ml.1.9.2)xhxi 与xixj的系数分别为⟨vh,vi⟩和⟨vi,vj⟩，他们之间有共同项vi。也就是说，所有包含xi的非零组合特征（存在某个j≠i，使得xixj≠0）的样本都可以用来学习隐向量vi，这在很大程度上避免了数据稀疏行造成参数估计不准确的影响。在二阶多项式模型中，参数whi和wij的学习过程是相互独立的。论文中还提到FM模型的应用场景，并且说公式(ml.1.9.1)作为一个通用的拟合模型（Generic Model），可以采用不同的损失函数来解决具体问题。比如：FM应用场景损失函数说明回归均方误差（MSE）损失Mean Square Error，与平方误差类似二类分类Hinge/Cross-Entopy损失分类时，结果需要做sigmoid变换排序. FM参数学习 等式变换公式(ml.1.9.1)中直观地看，FM模型的复杂度为O(kn2)，但是通过下面的等价转换，可以将FM的二次项化简，其复杂度可优化到O(kn)。即：&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj=12&#x2211;f=1k&#x27EE;(&#x2211;i=1nvi,fxi)2&#x2212;&#x2211;i=1nvi,f2xi2&#x27EF;(ml.1.9.3)下面给出详细推导：&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj(1)=12&#x2211;i=1n&#x2211;j=1n&#x27E8;vi,vj&#x27E9;xixj&#x2212;12&#x2211;i=1n&#x27E8;vi,vi&#x27E9;xixi(2)=12(&#x2211;i=1n&#x2211;j=1n&#x2211;f=1kvi,fvj,fxixj&#x2212;&#x2211;i=1n&#x2211;f=1kvi,fvi,fxixi)(3)=12&#x2211;f=1k&#x27EE;(&#x2211;i=1nvi,fxi)&#x22C5;(&#x2211;j=1nvj,fxj)&#x2212;&#x2211;i=1nvi,f2xi2&#x27EF;(4)=12&#x2211;f=1k&#x27EE;(&#x2211;i=1nvi,fxi)2&#x2212;&#x2211;i=1nvi,f2xi2&#x27EF;(5)(n.ml.1.9.3) 解读第（1）步到第（2）步，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角与上三角相等，有下式成立： A=12(2A+B)&#x2212;12B.A=&#x2211;i=1n&#x2211;j=i+1n&#x27E8;vi,vj&#x27E9;xixj&#x005F;;B=12&#x2211;i=1n&#x27E8;vi,vi&#x27E9;xixi&#x005F;(n.ml.1.9.4)如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：&#x2202;&#x2202;&#x03B8;y(x)={1,if&#x03B8;isw0(&#x5E38;&#x6570;&#x9879;)xiif&#x03B8;iswi(&#x7EBF;&#x6027;&#x9879;)xi&#x2211;j=1nvj,fxj&#x2212;vi,fxi2,if&#x03B8;isvi,f(&#x4EA4;&#x53C9;&#x9879;)(ml.1.9.4) 其中， vj,f是隐向量vj的第f个元素。 梯度法训练FM给出伪代码 FM训练复杂度由于∑nj=1vj,fxj只与f有关，在参数迭代过程中，只需要计算第一次所有f的∑nj=1vj,fxj，就能够方便地得到所有vi,f的梯度。显然，计算所有f的∑nj=1vj,fxj的复杂度是O(kn)；已知∑nj=1vj,fxj时，计算每个参数梯度的复杂度是O(n)；得到梯度后，更新每个参数的复杂度是 O(1)；模型参数一共有nk+n+1个。因此，FM参数训练的时间复杂度为O(kn)。综上可知，FM算法可以在线性时间内完成模型训练，以及对新样本做出预测，所以说FM是一个非常高效的模型。 FM总结上面我们主要是从FM模型引入（多项式开始）、模型表达和参数学习的角度介绍的FM模型，这里我把我认为FM最核心的精髓和价值总结出来，与大家讨论。FM模型的核心作用可以概括为以下3个： 1. FM降低了交叉项参数学习不充分的影响one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数wij用对应特征隐向量的内积表示，即⟨vi,vj⟩（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数wij的过程，转变为学习n个单特征对应k维隐向量的过程。很明显，单特征参数（k维隐向量vi）的学习要比交叉项参数wij学习得更充分。示例说明：假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下：共现交叉特征样本数注&lt;女性，汽车&gt;500同时出现&lt;女性，汽车&gt;的样本数&lt;女性，化妆品&gt;1000同时出现&lt;女性，化妆品&gt;的样本数&lt;男性，汽车&gt;1500同时出现&lt;男性，汽车&gt;的样本数&lt;男性，化妆品&gt;0样本中无此特征组合项&lt;女性，汽车&gt; 的含义是 女性看汽车广告。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响。 2. FM提升了模型预估能力依然看上面的示例，样本中没有*&lt;男性，化妆品&gt;交叉特征，即没有男性看化妆品广告的数据。如果用多项式模型来建模，对应的交叉项参数w男性,化妆品是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告*场景给出准确地预估。FM模型是否能得到交叉项参数w男性,化妆品呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为w男性,化妆品=⟨v男性,v化妆品⟩。用男性特征隐向量v男性和化妆品特征隐向量v化妆品的内积表示交叉项参数w男性,化妆品。由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用⟨v男性,v化妆品⟩得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估能力。 3. FM提升了参数学习效率这个显而易见，参数个数由(n2+n+1)变为(nk+n+1)个，模型训练复杂度也由O(mn2)变为O(mnk)。m为训练样本数。对于训练样本和特征数而言，都是线性复杂度。此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。 总结最后一句话总结，FM最大特点和优势：FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力。 场感知分解机场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自于Yu-Chin Juan与其比赛队员，它们借鉴了辣子Michael Jahrer的论文中field概念，提出了FM的升级版模型。通过引入field的概念，FFM吧相同性质的特征归于同一个field。在FM开头one-hot编码中提到用于访问的channel，编码生成了10个数值型特征，这10个特征都是用于说明用户PV时对应的channel类别，因此可以将其放在同一个field中。那么，我们可以把同一个categorical特征经过one-hot编码生成的数值型特征都可以放在同一个field中。同一个categorical特征可以包括用户属性信息（年龄、性别、职业、收入、地域等），用户行为信息（兴趣、偏好、时间等），上下文信息（位置、内容等）以及其它信息（天气、交通等）。在FFM中，每一维特征xi，针对其它特征的每一种”field” fj，都会学习一个隐向量vi,fj。因此，隐向量不仅与特征相关，也与field相关。假设每条样本的n个特征属于f个field，那么FFM的二次项有nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。因此可以吧FM看作是FFM的特例，即把所有的特征都归属到一个field是的FFM模型。根据FFM的field敏感特性，可以导出其模型表达式：\\[ŷ (x):=w0+∑i=1nwixi+∑i=1n∑j=i+1n⟨vi,fj,vj,fi⟩xixj(ml.1.9.5)\\]其中，fj是第j个特征所属的field。如果隐向量的长度为k，那么FFM的二交叉项参数就有nfk个，远多于FM模型的nk个。此外，由于隐向量与field相关，FFM的交叉项并不能够像FM那样做化简，其预测复杂度为O(kn2)。这里以NTU_FFM.pdf和美团－深入FFM原理与实践都提到的例子，给出FFM－Fields特征组合的工作过程。给出一下输入数据：UserMovieGenrePriceYuChin3IdiotsComedy, Drama$9.99Price是数值型特征，实际应用中通常会把价格划分为若干个区间（即连续特征离散化），然后再one-hot编码，这里假设$9.99对应的离散化区间tag为”2”。当然不是所有的连续型特征都要做离散化，比如某广告位、某类广告／商品、抑或某类人群统计的历史CTR（pseudo－CTR）通常无需做离散化。该条记录可以编码为5个数值特征，即User^YuChin, Movie^3Idiots, Genre^Comedy, Genre^Drama, Price2*。其中*GenreComedy, Genre^Drama属于同一个field。为了说明FFM的样本格式，我们把所有的特征和对应的field映射成整数编号。Field NameField IndexFeature NameFeature IndexUser1User^YuChin1Movie2Movie^3Idiots2Genre3Genre^Comedy3－－Genre^Drama4Price4Price^25那么，FFM所有的（二阶）组合特征共有10项（C25=5×42!=10），即为：其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有C2n=n(n−1)2个。 FFM应用场景在我们的广告业务系统、商业推荐以及自媒体－推荐系统中，FFM模型作为点击预估系统中的核心算法之一，用于预估广告、商品、文章的点击率（CTR）和转化率（CVR）。在鄙司广告算法团队，点击预估系统已成为基础设施，支持并服务于不同的业务线和应用场景。预估模型都是离线训练，然后定时更新到线上实时计算，因此预估问题最大的差异就体现在数据场景和特征工程。以广告的点击率为例，特征主要分为如下几类：用户属性与行为特征：广告特征：上下文环境特征：为了使用开源的FFM模型，所以的特征必须转化为field_id:feat_id:value格式，其中field_id表示特征所属field的编号，feat_id表示特征编号，value为特征取值。数值型的特征如果无需离散化，只需分配单独的field编号即可，如历史pseudo-ctr。categorical特征需要经过one-hot编码转化为数值型，编码产生的所有特征同属于一个field，特征value只能是0/1, 如用户年龄区间、性别、兴趣、人群等。开源工具FFM使用时，注意事项（参考新浪广告算法组的实战经验和美团－深入FFM原理与实践）: 样本归一化： 特征归一化： 省略0值特征：回归、分类、排序等。推荐算法，预估模型（如CTR预估等） 参考资料Sina广告点击预估系统实践FM、FFM相关Paper、技术博客美团技术团队原文","categories":[{"name":"推荐","slug":"推荐","permalink":"http://www.wortyby.com/categories/推荐/"}],"tags":[{"name":"FM","slug":"FM","permalink":"http://www.wortyby.com/tags/FM/"},{"name":"算法","slug":"算法","permalink":"http://www.wortyby.com/tags/算法/"}]},{"title":"Hive,Hbase,HDFS等之间的关系","slug":"Hive,Hbase,HDFS等之间的关系","date":"2018-09-21T05:17:26.000Z","updated":"2018-09-21T05:17:26.000Z","comments":true,"path":"2018/09/21/Hive,Hbase,HDFS等之间的关系/","link":"","permalink":"http://www.wortyby.com/2018/09/21/Hive,Hbase,HDFS等之间的关系/","excerpt":"","text":"Hive：hive不支持更改数据的操作，Hive基于数据仓库，提供静态数据的动态查询。其使用类SQL语言，底层经过编译转为MapReduce程序，在Hadoop上运行，数据存储在HDFS上。 HDFS:HDFS是GFS的一种实现，他的完整名字是分布式文件系统，类似于FAT32，NTFS，是一种文件格式，是底层的。Hive与Hbase的数据一般都存储在HDFS上。hadoop HDFS为他们提供了高可靠性的底层存储支持。 hbase:Hbase是Hadoop database，即Hadoop数据库。它是一个适合于非结构化数据存储的数据库，HBase基于列的而不是基于行的模式。HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据。Hadoop HDFS为HBase提供了高可靠性的底层存储支持，Hadoop MapReduce为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定服务和failover机制。Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS（关系型数据库）数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。 Pig：Pig的语言层包括一个叫做PigLatin的文本语言,Pig Latin是面向数据流的编程方式。Pig和Hive类似更侧重于数据的查询和分析，底层都是转化成MapReduce程序运行。区别是Hive是类SQL的查询语言，要求数据存储于表中，而Pig是面向数据流的一个程序语言。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"http://www.wortyby.com/tags/Hive/"},{"name":"HBase","slug":"HBase","permalink":"http://www.wortyby.com/tags/HBase/"},{"name":"HDFS","slug":"HDFS","permalink":"http://www.wortyby.com/tags/HDFS/"}]},{"title":"大数据组件原理总结-Hadoop、Hbase、Kafka、Zookeeper、Spark","slug":"大数据组件原理总结","date":"2018-09-21T05:13:56.000Z","updated":"2018-09-21T05:13:56.000Z","comments":true,"path":"2018/09/21/大数据组件原理总结/","link":"","permalink":"http://www.wortyby.com/2018/09/21/大数据组件原理总结/","excerpt":"","text":"Hadoop原理分为HDFS与Yarn两个部分。HDFS有Namenode和Datanode两个部分。每个节点占用一个电脑。Datanode定时向Namenode发送心跳包，心跳包中包含Datanode的校验等信息，用来监控Datanode。HDFS将数据分为块，默认为64M每个块信息按照配置的参数分别备份在不同的Datanode，而数据块在哪个节点上，这些信息都存储到Namenode上面。Yarn是MapReduce2，可以集成更多的组件，如spark、mpi等。MapReduce包括Jobtraker与Tasktraker两个部分。其中JobTraker是在主节点上，负责整体的调度。Tasktraker在slave节点上，当提交任务后，将其交给Jobtraker进行调度，调度到该任务之后就会将jar包发送到响应的Tasktraker，以实现分布式中移动计算资源而非移动数据。因为这些任务都是并发执行的，所以每个任务需要调用哪个节点的数据必须非常的明确，通常这一部分是通过Jobtraker进行指挥。在MapReduce的开始，每个block作为一个Map输入，Map输入后就进入shuffle阶段。Shuffle阶段主要分为Map和Reduce两个阶段。在Map Shuffle阶段，Map输入按用户的程序进行处理，生成的结果按key排序，然后进入内存，溢出后形成一个spill写入磁盘，这样多个spill在这个节点中进行多路归并排序（胜者树）形成一个排序文件写入磁盘，这期间那些Map文件交给那些节点处理由Jobtraker进行调度，最后生成一个大排序的文件，并且删除spill。之后，再将多个节点上已经排序的文件进行行多路归并排序（一个大文件N分到n个节点，每个节点分为k个spill，一个spill长度s，时间复杂度是N(logn（n个节点多路归并排序） + logk（每个节点内k个spill排序） + logs（每个spill内部进行排序）)，N=nks,所以最后的复杂度还是NlogN）。完成Map Shuffle阶段后通知Jobtraker进入Reduce Shuffle阶段。在这个阶段，因为已经排序，很容易将用户的程序直接作用到相同key的数据上，这些数据作为Reduce的输入进行处理，最终将输出的结果数据写入到HDFS上，并删除磁盘数据。Map一般多，Reduce少，所以通过Hash的方法将Map文件映射到Reduce上，进行处理，这个阶段叫做Patition。为了避免譬如所有数据相加这种操作使得数据负载移动的数量少的Reduce阶段，造成效率低下的结果，我们可以在在Map Shuffle阶段加一个Combine阶段，这个Combine是在每一台节点上将已经排序好的文件进行一次Reduce并将结果作为Reduce Shuffle阶段的输入，这样可以大大减少数据的输入量。通常Reduce的个数通过用户来指定，通常和CPU个数相适应才能使其效率达到最大。 HBase原理Hbase是列存储数据库。其存储的组织结构就是将相同的列族存储在一起，因此得名的。Hbase存储有行键，作为唯一标识，列表示为&lt;列族&gt;:&lt;列&gt;存储信息，如address：city，address：provice，然后是时间戳。Hbase物理模型中，有一个总结点HMaster，通过其自带的zookeeper与客户端相连接。Hbse作为分布式每一个节点作为一个RegionServer，维护Region的状态和管理。Region是数据管理的基本单位。最初只有一个，通过扩充后达到阈值然后分裂，通过Server控制其规模。在RegionServer中，每一个store作为一个列族。当数据插入进来，新数据成为Memstore，写入内存，当Memstore达到阈值后，通过Flashcache进程将数据写入storeFile，也就是当内存数据增多后溢出成一个StoreFile写入磁盘，这里和Hadoop的spill类似，这个过程是在HDFS上进行的操作。所以数据的插入并不是追加的过程，而是积累成大块数据后一并写入。当StoreFile数量过多时，进行合并，将形成一个大的StoreFile并且删除掉原来的StoreFile。再当StoreFile大小超过一定阈值后，分裂成Region。HBase有一个ROOT表和META表。META表记录用户Region的信息，但是随着数据增多，META也会增大，进而分裂成多个Region ，那我们用ROOT表记录下META的信息，是一个二级表，而zookeeper中记录ROOT表的location。当我们需找找到一条信息时，先去zookeeper查找ROOT，从ROOT中查找META找到META位置，在进入META表中寻找该数据所在Region，再读取该Region的信息。HBase适合大量插入又同时读的情况，其瓶颈是硬盘的传输速度而不再是像Oracle一样瓶颈在硬盘的寻道速度。 Zookeeper原理Zookeeper是一个资源管理库，对节点进行协调、通信、失败处理、节点损坏的处理等，是一个无中心设计，主节点通过选举产生。Zookeeper的节点是Znode。每一个节点可以存放1M的数据，client访问服务器时创建一个Znode，可以是短暂的Znode，其上可以放上观察Watcher对node进行监控。Zookeeper有高可用性，每个机器复制一份数据，只要有一般以上的机器可以正常的运行，整个集群就可以工作。比如6台的集群容忍2台断开，超过两台达到一般的数量就不可以，因此集群通常都是奇数来节约资源。Zookeeper使用zab协议，是一个无中心协议，通过选举的方式产生leader，通过每台机器的信息扩散选举最闲的资源利用较少的节点作为主控。同时当配置数据有更改更新时，在每个节点上有配置watcher并触发读取更改，。因此能够保证一致性。每个节点通过leader广播的方式，使所有follower同步。Zookeeper可以实现分布式锁机制。通过watcher监控，对每个Znode的锁都有一个独一的编号，按照序号的大小比较，来分配锁。当一个暂时Znode完结后删除本节点，通知leader完结，之后下一个Znode获取锁进行操作。 Kafka原理Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。它被设计为一个分布式系统，易于向外扩展；它同时为发布和订阅提供高吞吐量；它支持多订阅者，当失败时能自动平衡消费者；它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。broker和生产者、消费者各自都是集群，集群中的各个实例他们之间是对等的，集群扩充节点很方便。Kafka的基本概念包括话题、生产者、消费者、代理或者kafka集群。话题是特定类型的消息流。消息是字节的有效负载，话题是消息的分类名或种子名。生产者是能够发布消息到话题的任何对象。已发布的消息保存在一组服务器中，它们被称为代理或Kafka集群。消费者可以订阅一个或多个话题，并从Broker拉数据，从而消费这些已发布的消息。Kafka的存储布局非常简单。话题的每个分区对应一个逻辑日志。物理上，一个日志为相同大小的一组分段文件。每次生产者发布消息到一个分区，代理就将消息追加到最后一个段文件中。当发布的消息数量达到设定值或者经过一定的时间后，段文件真正写入磁盘中。写入完成后，消息公开给消费者。段文件机制和Hadoop中spill类似。消费者始终从特定分区顺序地获取消息，如果消费者知道特定消息的偏移量，也就说明消费者已经消费了之前的所有消息。消费者向代理发出异步拉请求，准备字节缓冲区用于消费。每个异步拉请求都包含要消费的消息偏移量与其它消息系统不同，Kafka代理是无状态的。这意味着消费者必须维护已消费的状态信息。这些信息由消费者自己维护，代理完全不管。消费者可以故意倒回到老的偏移量再次消费数据。这违反了队列的常见约定，但被证明是许多消费者的基本特征。kafka的broker在配置文件中可以配置最多保存多少小时的数据和分区最大的空间占用，过期的和超量的数据会被broker自动清除掉。Kafka会记录offset到zk，同时又在内存中维护offset，允许快速的checkpoint，如果consumer比partition多是浪费，因为kafka不允许partition上并行consumer读取。同时，consumer比partition少，一个consumer会对应多个partition，有可能导致partition中数据的读取不均匀，也不能保证数据间的顺序性，kafka只有在一个partition读取的时候才能保证时间上是有顺序的。增加partition或者consumer或者broker会导致rebalance，所以rebalance后consumer对应的partition会发生变化。 Spark原理spark 可以很容易和yarn结合，直接调用HDFS、Hbase上面的数据，和hadoop结合。配置很容易。spark发展迅猛，框架比hadoop更加灵活实用。减少了延时处理，提高性能效率实用灵活性。也可以与hadoop切实相互结合。spark核心部分分为RDD。Spark SQL、Spark Streaming、MLlib、GraphX、Spark R等核心组件解决了很多的大数据问题，其完美的框架日受欢迎。其相应的生态环境包括zepplin等可视化方面，正日益壮大。大型公司争相实用spark来代替原有hadoop上相应的功能模块。Spark读写过程不像hadoop溢出写入磁盘，都是基于内存，因此速度很快。另外DAG作业调度系统的宽窄依赖让Spark速度提高。RDD是弹性分布式数据也是spark的核心，完全弹性的，如果数据丢失一部分还可以重建。有自动容错、位置感知调度和可伸缩性，通过数据检查点和记录数据更新金象容错性检查。通过SparkContext.textFile()加载文件变成RDD，然后通过transformation构建新的RDD，通过action将RDD存储到外部系统。RDD使用延迟加载，也就是懒加载，只有当用到的时候才加载数据。如果加载存储所有的中间过程会浪费空间。因此要延迟加载。一旦spark看到整个变换链，他可以计算仅需的结果数据，如果下面的函数不需要数据那么数据也不会再加载。转换RDD是惰性的，只有在动作中才可以使用它们。Spark分为driver和executor，driver提交作业，executor是application早worknode上的进程，运行task，driver对应为sparkcontext。Spark的RDD操作有transformation、action。Transformation对RDD进行依赖包装，RDD所对应的依赖都进行DAG的构建并保存，在worknode挂掉之后除了通过备份恢复还可以通过元数据对其保存的依赖再计算一次得到。当作业提交也就是调用runJob时，spark会根据RDD构建DAG图，提交给DAGScheduler，这个DAGScheduler是在SparkContext创建时一同初始化的，他会对作业进行调度处理。当依赖图构建好以后，从action开始进行解析，每一个操作作为一个task，每遇到shuffle就切割成为一个taskSet，并把数据输出到磁盘，如果不是shuffle数据还在内存中存储。就这样再往前推进，直到没有算子，然后运行从前面开始，如果没有action的算子在这里不会执行，直到遇到action为止才开始运行，这就形成了spark的懒加载，taskset提交给TaskSheduler生成TaskSetManager并且提交给Executor运行，运行结束后反馈给DAGScheduler完成一个taskSet，之后再提交下一个，当TaskSet运行失败时就返回DAGScheduler并重新再次创建。一个job里面可能有多个TaskSet，一个application可能包含多个job。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://www.wortyby.com/tags/HBASE/"},{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"HADOOP","slug":"HADOOP","permalink":"http://www.wortyby.com/tags/HADOOP/"},{"name":"KAFKA","slug":"KAFKA","permalink":"http://www.wortyby.com/tags/KAFKA/"},{"name":"ZOOKEEPER","slug":"ZOOKEEPER","permalink":"http://www.wortyby.com/tags/ZOOKEEPER/"}]},{"title":"Happybase 进行 过滤查询","slug":"Happybase 进行 过滤查询","date":"2018-09-18T08:19:47.000Z","updated":"2018-09-18T08:19:47.000Z","comments":true,"path":"2018/09/18/Happybase 进行 过滤查询/","link":"","permalink":"http://www.wortyby.com/2018/09/18/Happybase 进行 过滤查询/","excerpt":"","text":"背景HBASE 中的数据 ，当利用 scan 进行数据查询的时候，如果不给过滤条件，他会将所有数据都输出。如果我们仅仅是想得到某些列的数据，那么我们需要祭出filter 神器。或是 columns 来指定我们需要哪些列的数据。下面我们上代码来看看 filter 和 column 来进行 筛选数据的用法。 初始化代码123import happybaseconnection = happybase.Connection(&apos;localhost&apos;, autoconnect=False)table = connection.table(&apos;live_gift&apos;) columns 来指定列名称通过columns 来确定 当前需要哪些列的数据，一般会加上一个 limit 参数不废话，先上代码看看它为何方神圣。1data = table.scan(columns=[b&apos;g:user_id&apos;, b&apos;g:gift_price&apos;], limit=20)得到的结果就是以user_id，gift_price 2列的数据一般情况下，这种方式都是简单的查看数据，但更实际的场景可能需要综合各种条件进行查询，那么就需要祭出 filter 神器了 filter 来过滤符合条件的数据进行输出通过filter 来确定当前需要符合这些条件的数据进行输出。filter 是以字符串形式存在，如果是多个条件查询，就综合拼接成一个字符串 单一条件查询拿出开始时间大于 这个时间的数据1data = table.scan(columns=[b&apos;s:live_id&apos;, b&apos;s:start_time&apos;], filter=&quot;SingleColumnValueFilter(&apos;s&apos;, &apos;start_time&apos;, &gt;=, &apos;binary:1490976000&apos;)&quot;) 多个条件查询拿出 live_id = 15909的数据，gift_price=20 的数据1data = table.scan(columns=[b&apos;g:user_id&apos;, b&apos;g:gift_price&apos;], filter=&quot;SingleColumnValueFilter(&apos;g&apos;, &apos;live_id&apos;, =, &apos;binary:15909&apos;)&quot; and &quot;SingleColumnValueFilter(&apos;g&apos;, &apos;gift_price&apos;, =, &apos;binary:20&apos;)&quot;, limit=20)happyhbase官方文档","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://www.wortyby.com/tags/HBASE/"}]},{"title":"python redis之连接池的原理","slug":"python redis之连接池的原理","date":"2018-09-14T06:14:30.000Z","updated":"2018-09-14T06:14:30.000Z","comments":true,"path":"2018/09/14/python redis之连接池的原理/","link":"","permalink":"http://www.wortyby.com/2018/09/14/python redis之连接池的原理/","excerpt":"","text":"我们使用redis连接池, 却不怎么了解这个连接池的原理, 今天我们来研究一下 什么是连接池通常情况下, 当我们需要做redis操作时, 会创建一个连接, 并基于这个连接进行redis操作, 操作完成后, 释放连接,一般情况下, 这是没问题的, 但当并发量比较高的时候, 频繁的连接创建和释放对性能会有较高的影响于是, 连接池就发挥作用了连接池的原理是, 通过预先创建多个连接, 当进行redis操作时, 直接获取已经创建的连接进行操作, 而且操作完成后, 不会释放, 用于后续的其他redis操作这样就达到了避免频繁的redis连接创建和释放的目的, 从而提高性能了 原理那么, 在redis-py中, 他是怎么进行连接池管理的呢连接池使用首先看下如何进行连接池操作的1234rdp = redis.ConnectionPool(host=&apos;127.0.0.1&apos;, port=6379, password=&apos;xxxxx&apos;)rdc = redis.StrictRedis(connection_pool=rdp)rdc.set(&apos;name&apos;, &apos;Yi_Zhi_Yu&apos;)rdc.get(&apos;name&apos;) 原理解析当redis.ConnectionPool 实例化的时候, 做了什么123456789def __init__(self, connection_class=Connection, max_connections=None, **connection_kwargs): max_connections = max_connections or 2 ** 31 if not isinstance(max_connections, (int, long)) or max_connections &lt; 0: raise ValueError(&apos;&quot;max_connections&quot; must be a positive integer&apos;) self.connection_class = connection_class self.connection_kwargs = connection_kwargs self.max_connections = max_connections这个连接池的实例化其实未做任何真实的redis连接, 仅仅是设置最大连接数, 连接参数和连接类StrictRedis 实例化的时候, 又做了什么12345def __init__(self, ...connection_pool=None...): if not connection_pool: ... connection_pool = ConnectionPool(**kwargs) self.connection_pool = connection_pool以上仅保留了关键部分代码可以看出, 使用StrictRedis 即使不创建连接池, 他也会自己创建到这里, 我们还没有看到什么redis连接真实发生继续下一步就是set 操作了, 很明显, 这个时候一定会发生redis连接(要不然怎么set)123def set(self, name, value, ex=None, px=None, nx=False, xx=False): ... return self.execute_command(&apos;SET&apos;, *pieces)我们继续看看execute_command12345678910111213141516def execute_command(self, *args, **options): &quot;Execute a command and return a parsed response&quot; pool = self.connection_pool command_name = args[0] connection = pool.get_connection(command_name, **options) try: connection.send_command(*args) return self.parse_response(connection, command_name, **options) except (ConnectionError, TimeoutError) as e: connection.disconnect() if not connection.retry_on_timeout and isinstance(e, TimeoutError): raise connection.send_command(*args) return self.parse_response(connection, command_name, **options) finally: pool.release(connection)终于, 在这我们看到到了连接创建123456789101112connection = pool.get_connection(command_name, **options)这里调用的是ConnectionPool的get_connectiondef get_connection(self, command_name, *keys, **options): &quot;Get a connection from the pool&quot; self._checkpid() try: connection = self._available_connections.pop() except IndexError: connection = self.make_connection() self._in_use_connections.add(connection) return connection如果有可用的连接, 获取可用的链接, 如果没有, 创建一个123456def make_connection(self): &quot;Create a new connection&quot; if self._created_connections &gt;= self.max_connections: raise ConnectionError(&quot;Too many connections&quot;) self._created_connections += 1 return self.connection_class(**self.connection_kwargs)终于, 我们看到了, 在这里创建了连接在ConnectionPool的实例中, 有两个list, 依次是 _available_connections, _in_use_connections,分别表示可用的连接集合和正在使用的连接集合, 在上面的get_connection中, 我们可以看到获取连接的过程是从可用连接集合尝试获取连接,如果获取不到, 重新创建连接将获取到的连接添加到正在使用的连接集合上面是往**_in_use_connections**里添加连接的, 这种连接表示正在使用中, 那是什么时候将正在使用的连接放回到可用连接列表中的呢这个还是在execute_command里, 我们可以看到在执行redis操作时, 在finally部分, 会执行一下1pool.release(connection)连接池对象调用release方法, 将连接从_in_use_connections 放回 _available_connectionsÅ, 这样后续的连接获取就能再次使用这个连接了release 方法如下1234567def release(self, connection): &quot;Releases the connection back to the pool&quot; self._checkpid() if connection.pid != self.pid: return self._in_use_connections.remove(connection) self._available_connections.append(connection) 总结至此, 我们把连接池的管理流程走了一遍, ConnectionPool通过管理可用连接列表(_available_connections) 和 正在使用的连接列表 从而实现连接池管理","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.wortyby.com/tags/redis/"},{"name":"连接池","slug":"连接池","permalink":"http://www.wortyby.com/tags/连接池/"}]},{"title":"HBase中CloumnFamily的设计规则","slug":"HBase中CloumnFamily的设计规则","date":"2018-09-14T05:56:39.000Z","updated":"2018-09-14T05:56:39.000Z","comments":true,"path":"2018/09/14/HBase中CloumnFamily的设计规则/","link":"","permalink":"http://www.wortyby.com/2018/09/14/HBase中CloumnFamily的设计规则/","excerpt":"","text":"HBase本身的设计目标是 支持稀疏表，而 稀疏表通常会有很多列，但是每一行有值的列又比较少。 替代方案如果不使用Column Family的概念，那么有两种设计方案： 方案一把所有列的数据放在一个文件中（也就是传统的按行存储）。那么当我们想要访问少数几个列的数据时，需要遍历每一行，读取整个表的数据，这样子是很低效的。 方案二把每个列的数据单独分开存在一个文件中（按列存储）。那么当我们想要访问少数几个列的数据时，只需要读取对应的文件，不用读取整个表的数据，读取效率很高。然而，由于稀疏表通常会有很多列，这会导致文件数量特别多，这本身会影响文件系统的效率。而Column Family的提出就是为了在上面两种方案中做一个折中。HBase中 将一个Column Family中的列存在一起，而不同Column Family的数据则分开。由于在HBase中Column Family的数量通常很小，同时HBase建议把经常一起访问的比较类似的列放在同一个Column Family中，这样就可以在访问少数几个列时，只读取尽量少的数据。 优化：因为一直在做hbase的应用层面的开发，所以体会的比较深的一点是hbase的表结构设计会对系统的性能以及开销上造成很大的区别，本篇文章先按照hbase表中的rowkey、columnfamily、column、timestamp几个方面进行一些分析。最后结合分析如何设计一种适合应用的高效表结构。 表的属性 最大版本数：通常是3，如果对于更新比较频繁的应用完全可以设置为1，能够快速的淘汰无用数据，对于节省存储空间和提高查询速度有效果。不过这类需求在海量数据领域比较小众。 压缩算法：可以尝试一下最新出炉的snappy算法，相对lzo来说，压缩率接近，压缩效率稍高，解压效率高很多。 inmemory：表在内存中存放，一直会被忽略的属性。如果完全将数据存放在内存中，那么hbase和现在流行的内存数据库memorycached和redis性能差距有多少，尚待实测。 bloomfilter：根据应用来定，看需要精确到rowkey还是column。不过这里需要理解一下原理，bloomfilter的作用是对一个region下查找记录所在的hfile有用。即如果一个region下的hfile数量很多，bloomfilter的作用越明显。适合那种compaction赶不上flush速度的应用。 rowkeyrowkey是hbase的key-value存储中的key，通常使用用户要查询的字段作为rowkey，查询结果作为value。可以通过设计满足几种不同的查询需求。 数字rowkey的从大到小排序：原生hbase只支持从小到大的排序，这样就对于排行榜一类的查询需求很尴尬。那么采用rowkey = Integer.MAX_VALUE-rowkey的方式将rowkey进行转换，最大的变最小，最小的变最大。在应用层再转回来即可完成排序需求。 rowkey的散列原则：如果rowkey是类似时间戳的方式递增的生成，建议不要使用正序直接写入rowkey，而是采用reverse的方式反转rowkey，使得rowkey大致均衡分布，这样设计有个好处是能将regionserver的负载均衡，否则容易产生所有新数据都在一个regionserver上堆积的现象，这一点还可以结合table的预切分一起设计。 columnfamilycolumnfamily尽量少，原因是过多的columnfamily之间会互相影响。 column对于column需要扩展的应用，column可以按普通的方式设计，但是对于列相对固定的应用，最好采用将一行记录封装到一个column中的方式，这样能够节省存储空间。封装的方式推荐protocolbuffer。 摸索与讨论以下会分场景介绍一些特殊的表结构设计方法，只是一些摸索，欢迎讨论： value数目过多场景下的表结构设计：目前我碰到了一种key-value的数据结构，某一个key下面包含的column很多，以致于客户端查询的时候oom，bulkload写入的时候oom，regionsplit的时候失败这三种后果。通常来讲，hbase的column数目不要超过百万这个数量级。在官方的说明和我实际的测试中都验证了这一点。有两种思路可以参考，第一种是单独处理这些特殊的rowkey，第二种如下：可以考虑将column设计到rowkey的方法解决。例如原来的rowkey是uid1,，column是uid2，uid3…。重新设计之后rowkey为&lt;uid2&gt;，&lt;uid1&gt;…当然大家会有疑问，这种方式如何查询，如果要查询uid1下面的所有uid怎么办。这里说明一下hbase并不是只有get一种随机读取的方法。而是含有scan(startkey,endkey)的扫描方法，而这种方法和get的效率相当。需要取得uid1下的记录只需要new Scan(“uid1&quot;,&quot;uid1~”)即可。这里的设计灵感来自于hadoop world大会上的一篇文章，这篇文章本身也很棒，推荐大家看一下http://www.cloudera.com/resource/hadoop-world-2011-presentation-slides-advanced-hbase-schema-design/ 其他参考资料： 表设计优化总结参考HBase性能优化方法总结（一）：表的设计关于cloumn family的描述：不要在一张表里定义太多的****column family。目前Hbase并不能很好的处理超过2~3个column family的表。因为某个column family在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://www.wortyby.com/tags/HBASE/"}]},{"title":"Python 进程管理工具 Supervisor 使用教程","slug":"Python 进程管理工具 Supervisor 使用教程","date":"2018-09-06T11:57:32.000Z","updated":"2018-09-06T11:57:32.000Z","comments":true,"path":"2018/09/06/Python 进程管理工具 Supervisor 使用教程/","link":"","permalink":"http://www.wortyby.com/2018/09/06/Python 进程管理工具 Supervisor 使用教程/","excerpt":"","text":"Supervisor 是基于 Python 的进程管理工具，可以帮助我们更简单的启动、重启和停止服务器上的后台进程，是 Linux 服务器管理的效率工具。什么情况下我们需要进程管理呢？就是执行一些需要以守护进程方式启动的程序，比如一个后台任务、一组 Web 服务的进程（说是一组，是因为经常用 Nginx 来做负载均衡），这些很可能是一些网站、REST API 的服务、消息推送的后台服务、日志数据的处理分析服务等等。需要注意的是 Supervisor 是通用的进程管理工具，可以用来启动任意进程，不仅仅是用来管理 Python 进程。Supervisor 经常被用来管理由 gunicorn 启动的 Django 或 Flask 等 Web 服务的进程。我最常用的是用来管理和启动一组 Tornado 进程来实现负载均衡。除此之外，Supervisor 还能很友好的管理程序在命令行上输出的日志，可以将日志重定向到自定义的日志文件中，还能按文件大小对日志进行分割。目前 Supervisor 只能运行在 Unix-Like 的系统上，也就是无法运行在 Windows 上。Supervisor 官方版目前只能运行在 Python 2.4 以上版本，但是还无法运行在 Python 3 上，不过已经有一个 Python 3 的移植版 supervisor-py3k。Supervisor 有两个主要的组成部分：supervisord，运行 Supervisor 时会启动一个进程 supervisord，它负责启动所管理的进程，并将所管理的进程作为自己的子进程来启动，而且可以在所管理的进程出现崩溃时自动重启。supervisorctl，是命令行管理工具，可以用来执行 stop、start、restart 等命令，来对这些子进程进行管理。 安装sudo pip install supervisor 创建配置文件echo_supervisord_conf &gt; /etc/supervisord.conf 如果出现没有权限的问题，可以使用这条命令sudo su - root -c &quot;echo_supervisord_conf &gt; /etc/supervisord.conf&quot; 配置文件说明想要了解怎么配置需要管理的进程，只要打开 supervisord.conf 就可以了，里面有很详细的注释信息。打开配置文件vim /etc/supervisord.conf 默认的配置文件是下面这样的，但是这里有个坑需要注意，supervisord.pid 以及 supervisor.sock 是放在 /tmp 目录下，但是 /tmp 目录是存放临时文件，里面的文件是会被 Linux 系统删除的，一旦这些文件丢失，就无法再通过 supervisorctl 来执行 restart 和 stop 命令了，将只会得到 unix:///tmp/supervisor.sock 不存在的错误 。因此可以单独建一个文件夹，来存放这些文件，比如放在 /home/supervisor/创建文件夹mkdir /home/supervisor mkdir /var/log/supervisor mkdir /etc/supervisor.d 然后对一些配置进行修改1234567891011121314151617181920212223242526272829303132333435363738394041[unix_http_server] ;file=/tmp/supervisor.sock ; (the path to the socket file) ;修改为 /home/supervisor 目录，避免被系统删除 file=/home/supervisor/supervisor.sock ; (the path to the socket file) ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for ;all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ... [supervisord] ;logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log) ;修改为 /var/log 目录，避免被系统删除 logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) ; 日志文件多大时进行分割 logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) ; 最多保留多少份日志文件 logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) ;pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ;修改为 /home/supervisor 目录，避免被系统删除 pidfile=/home/supervisor/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ... ;设置启动supervisord的用户，一般情况下不要轻易用root用户来启动，除非你真的确定要这么做 ;user=chrism ; (default is current user, required if root) ... [supervisorctl] ; 必须和&apos;unix\\_http\\_server&apos;里面的设定匹配 ;serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socket ;修改为 /home/supervisor 目录，避免被系统删除 serverurl=unix:///home/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set ...默认情况下，进程的日志文件达到50MB时，将进行分割，最多保留10个文件，当然这些配置也可以对每个进程单独配置。 权限问题设置好配置文件后，应先创建上述配置文件中新增的文件夹。如果指定了启动用户 user，这里以 oxygen 为例，那么应注意相关文件的权限问题，包括日志文件，否则会出现没有权限的错误。例如设置了启动用户 oxygen，然后启动 supervisord 出现错误Error: Cannot open an HTTP server: socket.error reported errno.EACCES (13) 就是由于上面的配置文件中 /home/supervisor 文件夹，没有授予启动 supervisord 的用户 oxygen 的写权限，可以将这个文件夹的拥有者设置该该账号sudo chown oxygen /home/supervisor 一般情况下，我们可以用 root 用户启动 supervisord 进程，然后在其所管理的进程中，再具体指定需要以那个用户启动这些进程。 使用浏览器来管理supervisor 同时提供了通过浏览器来管理进程的方法，只需要注释掉如下几行就可以了。12345678910;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for ;all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) [supervisorctl] ... ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set 使用 include在配置文件的最后，有一个 [include] 的配置项，跟 Nginx 一样，可以 include 某个文件夹下的所有配置文件，这样我们就可以为每个进程或相关的几个进程的配置单独写成一个文件。12[include]files = /etc/supervisor.d/*.ini 进程的配置样例sudo pip install supervisor 一个简单的例子如下; 设置进程的名称，使用 supervisorctl 来管理进程时需要使用该进程名 [program:your_program_name] command=python server.py --port=9000 ;numprocs=1 ; 默认为1 ;process_name=%(program_name)s ; 默认为 %(program_name)s，即 [program:x] 中的 x directory=/home/python/tornado_server ; 执行 command 之前，先切换到工作目录 user=oxygen ; 使用 oxygen 用户来启动该进程 ; 程序崩溃时自动重启，重启次数是有限制的，默认为3次 autorestart=true redirect_stderr=true ; 重定向输出的日志 stdout_logfile = /var/log/supervisor/tornado_server.log loglevel=info 设置日志级别loglevel 指定了日志的级别，用 Python 的 print 语句输出的日志是不会被记录到日志文件中的，需要搭配 Python 的 logging 模块来输出有指定级别的日志。 多个进程按照官方文档的定义，一个 [program:x] 实际上是表示一组相同特征或同类的进程组，也就是说一个 [program:x] 可以启动多个进程。这组进程的成员是通过 numprocs 和 process_name 这两个参数来确定的，这句话什么意思呢，我们来看这个例子。12345678910111213; 设置进程的名称，使用 supervisorctl 来管理进程时需要使用该进程名 [program:foo] ; 可以在 command 这里用 python 表达式传递不同的参数给每个进程 command=python server.py --port=90%(process_num)02d directory=/home/python/tornado_server ; 执行 command 之前，先切换到工作目录 ; 若 numprocs 不为1，process\\_name 的表达式中一定要包含 process_num 来区分不同的进程 numprocs=2 process_name=%(program_name)s_%(process_num)02d; user=oxygen ; 使用 oxygen 用户来启动该进程 autorestart=true ; 程序崩溃时自动重启 redirect_stderr=true ; 重定向输出的日志 stdout_logfile = /var/log/supervisor/tornado_server.log loglevel=info上面这个例子会启动两个进程，process_name 分别为 foo:foo_01 和 foo:foo_02。通过这样一种方式，就可以用一个 [program:x] 配置项，来启动一组非常类似的进程。再介绍两个配置项 stopasgroup 和 killasgroup12345; 默认为 false，如果设置为 true，当进程收到 stop 信号时，会自动将该信号发给该进程的子进程。如果这个配置项为 true，那么也隐含 killasgroup 为 true。例如在 Debug 模式使用 Flask 时，Flask 不会将接收到的 stop 信号也传递给它的子进程，因此就需要设置这个配置项。 stopasgroup=false ; send stop signal to the UNIX process ; 默认为 false，如果设置为 true，当进程收到 kill 信号时，会自动将该信号发给该进程的子进程。如果这个程序使用了 python 的 multiprocessing 时，就能自动停止它的子线程。 killasgroup=false ; SIGKILL the UNIX process group (def false)更详细的配置例子，可以参考如下，官方文档在这里123456789101112131415161718192021222324252627282930;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;autorestart=unexpected ; whether/when to restart (default: unexpected) ;startsecs=1 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; &apos;expected&apos; exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout\\_logfile\\_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_capture\\_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=&quot;1&quot;,B=&quot;2&quot; ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) 将多个进程按组管理Supervisor 同时还提供了另外一种进程组的管理方式，通过这种方式，可以使用 supervisorctl 命令来管理一组进程。跟 [program:x] 的进程组不同的是，这里的进程是一个个的 [program:x] 。123[group:thegroupname] programs=progname1,progname2 ; each refers to &apos;x&apos; in \\[program:x\\] definitions priority=999 ; the relative start priority (default 999)当添加了上述配置后，progname1 和 progname2 的进程名就会变成 thegroupname:progname1 和 thegroupname:progname2 以后就要用这个名字来管理进程了，而不是之前的 progname1。以后执行 supervisorctl stop thegroupname: 就能同时结束 progname1 和 progname2，执行 supervisorctl stop thegroupname:progname1 就能结束 progname1。supervisorctl 的命令我们稍后介绍。 启动 supervisord执行 supervisord 命令，将会启动 supervisord 进程，同时我们在配置文件中设置的进程也会相应启动。123456# 使用默认的配置文件 /etc/supervisord.conf supervisord # 明确指定配置文件 supervisord -c /etc/supervisord.conf # 使用 user 用户启动 supervisord supervisord -u user更多参数请参考文档 supervisorctl 命令介绍12345678910111213141516# 停止某一个进程，program_name 为 [program:x] 里的 x supervisorctl stop program_name # 启动某个进程 supervisorctl start program_name # 重启某个进程 supervisorctl restart program_name # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理) supervisorctl stop groupworker: # 结束 groupworker:name1 这个进程 (start，restart 同理) supervisorctl stop groupworker:name1 # 停止全部进程，注：start、restart、stop 都不会载入最新的配置文件 supervisorctl stop all # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程 supervisorctl reload # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启 supervisorctl update注意：显示用 stop 停止掉的进程，用 reload 或者 update 都不会自动重启。也可以参考这里 开机自动启动 Supervisord 方法1有一个简单的方法，因为 Linux 在启动的时候会执行 /etc/rc.local 里面的脚本，所以只要在这里添加执行命令就可以12# 如果是 Ubuntu 添加以下内容 /usr/local/bin/supervisord -c /etc/supervisord.conf12# 如果是 Centos 添加以下内容 /usr/bin/supervisord -c /etc/supervisord.conf以上内容需要添加在 exit 命令前，而且由于在执行 rc.local 脚本时，PATH 环境变量未全部初始化，因此命令需要使用绝对路径。可以用 which supervisord 查看一下 supervisord 所在的路径。在添加前，先在终端测试一下命令是否能正常执行，如果找不到 supervisord，可以用如下命令找到sudo find / -name supervisord 如果是 Ubuntu 16.04 以上，rc.local 被当成了服务，而且默认是不会启动，需要手动启用一下服务。https://askubuntu.com/questions/765120/after-upgrade-to-16-04-lts-rc-local-not-executing-command启用 rc.local 服务sudo systemctl enable rc-local.service 方法2Supervisord 默认情况下并没有被安装成服务，它本身也是一个进程。官方已经给出了脚本可以将 Supervisord 安装成服务，可以参考这里查看各种操作系统的安装脚本，但是我用官方这里给的 Ubuntu 脚本却无法运行。安装方法可以参考 serverfault 上的回答。比如我是 Ubuntu 系统，可以这么安装，这里选择了另外一个脚本123456789# 下载脚本 sudo su - root -c &quot;sudo curl https://gist.githubusercontent.com/howthebodyworks/176149/raw/d60b505a585dda836fadecca8f6b03884153196b/supervisord.sh &gt; /etc/init.d/supervisord&quot; # 设置该脚本为可以执行 sudo chmod +x /etc/init.d/supervisord # 设置为开机自动运行 sudo update-rc.d supervisord defaults # 试一下，是否工作正常 service supervisord stop service supervisord start注意：这个脚本下载下来后，还需检查一下与我们的配置是否相符合，比如默认的配置文件路径，pid 文件路径等，如果存在不同则需要进行一些修改。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"python","slug":"python","permalink":"http://www.wortyby.com/tags/python/"}]},{"title":"矩阵运算和文本处理中的分类问题","slug":"矩阵运算和文本处理中的分类问题","date":"2018-09-05T06:57:58.000Z","updated":"2018-09-05T06:57:58.000Z","comments":true,"path":"2018/09/05/矩阵运算和文本处理中的分类问题/","link":"","permalink":"http://www.wortyby.com/2018/09/05/矩阵运算和文本处理中的分类问题/","excerpt":"","text":"背景我在大学学习线性代数时，实在想不出它除了告诉我们如何解线性方程外，还能有什么别的用途。关于矩阵的许多概念，比如特征值等等，更是脱离日常生活。后来在数值分析中又学了很多矩阵的近似算法，还是看不到可以应用的地方。当时选这些课，完全是为了混学分的学位。我想，很多同学都多多少少有过类似的经历。直到后来长期做自然语言处理的研究，我才发现数学家们提出那些矩阵的概念和算法，是有实际应用的意义的。 语音分类问题在自然语言处理中，最常见的两类的分类问题分别是，将文本按主题归类（比如将所有介绍亚运会的新闻归到体育类）和将词汇表中的字词按意思归类（比如将各种体育运动的名称个归成一类）。这两种分类问题都可用通过矩阵运算来圆满地、同时解决。为了说明如何用矩阵这个工具类解决这两个问题的，让我们先来来回顾一下我们在余弦定理和新闻分类中介绍的方法。分类的关键是计算相关性。我们首先对两个文本计算出它们的内容词，或者说实词的向量，然后求这两个向量的夹角。当这两个向量夹角为零时，新闻就相关；当它们垂直或者说正交时，新闻则无关。当然，夹角的余弦等同于向量的内积。从理论上讲，这种算法非常好。但是计算时间特别长。通常，我们要处理的文章的数量都很大，至少在百万篇以上，二次回标有非常长，比如说有五十万个词（包括人名地名产品名称等等）。如果想通过对一百万篇文章两篇两篇地成对比较，来找出所有共同主题的文章，就要比较五千亿对文章。现在的计算机一秒钟最多可以比较一千对文章，完成这一百万篇文章相关性比较就需要十五年时间。注意，要真正完成文章的分类还要反复重复上述计算。 矩阵处理在文本分类中，另一种办法是利用矩阵运算中的奇异值分解（Singular Value Decomposition，简称 SVD)。现在让我们来看看奇异值分解是怎么回事。首先，我们可以用一个大矩阵A来描述这一百万篇文章和五十万词的关联性。这个矩阵中，每一行对应一篇文章，每一列对应一个词。在上面的图中，M=1,000,000，N=500,000。第 i 行，第 j 列的元素，是字典中第 j 个词在第 i 篇文章中出现的加权词频（比如，TF/IDF)。读者可能已经注意到了，这个矩阵非常大，有一百万乘以五十万，即五千亿个元素。奇异值分解就是把上面这样一个大矩阵，分解成三个小矩阵相乘，如下图所示。比如把上面的例子中的矩阵分解成一个一百万乘以一百的矩阵X，一个一百乘以一百的矩阵B，和一个一百乘以五十万的矩阵Y。这三个矩阵的元素总数加起来也不过1.5亿，仅仅是原来的三千分之一。相应的存储量和计算量都会小三个数量级以上。三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次**奇异值分解**，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。现在剩下的唯一问题，就是如何用计算机进行奇异值分解。这时，线性代数中的许多概念，比如矩阵的特征值等等，以及数值分析的各种算法就统统用上了。在很长时间内，奇异值分解都无法并行处理。（虽然 Google 早就有了MapReduce 等并行计算的工具，但是由于奇异值分解很难拆成不相关子运算，即使在 Google 内部以前也无法利用并行计算的优势来分解矩阵。）最近，Google 中国的张智威博士和几个中国的工程师及实习生已经实现了奇异值分解的并行算法，我认为这是 Google 中国对世界的一个贡献。利用相对熵，我们可以到处信息检索中最重要的一个概念：词频率-逆向文档频率（TF/IDF)。我们下回会介绍如何根据相关性对搜索出的网页进行排序，就要用的餐TF/IDF 的概念。另外，在新闻的分类中也要用到相对熵和 TF/IDF。原文链接信息熵","categories":[{"name":"数学","slug":"数学","permalink":"http://www.wortyby.com/categories/数学/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"http://www.wortyby.com/tags/线性代数/"},{"name":"分类","slug":"分类","permalink":"http://www.wortyby.com/tags/分类/"}]},{"title":"马尔可夫链的扩展 贝叶斯网络","slug":"马尔可夫链的扩展 贝叶斯网络 ","date":"2018-09-05T06:29:01.000Z","updated":"2018-09-05T06:29:01.000Z","comments":true,"path":"2018/09/05/马尔可夫链的扩展 贝叶斯网络 /","link":"","permalink":"http://www.wortyby.com/2018/09/05/马尔可夫链的扩展 贝叶斯网络 /","excerpt":"","text":"我们在前面的系列中多次提到马尔可夫链 (MarkovChain)，它描述了一种状态序列，其每个状态值取决于前面有限个状态。这种模型，对很多实际问题来讲是一种很粗略的简化。在现实生活中，很多事物相互的关系并不能用一条链来串起来。它们之间的关系可能是交叉的、错综复杂的。比如在下图中可以看到，心血管疾病和它的成因之间的关系是错综复杂的。显然无法用一个链来表示。 信念网络我们可以把上述的有向图看成一个网络，它就是**贝叶斯网络。其中每个圆圈表示一个状态。状态之间的连线表示它们的因果关系。比如从心血管疾病出发到吸烟的弧线表示心血管疾病可能和吸烟有关。当然，这些关系可以有一个量化的可信度 (belief)，用一个概率描述。我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名。由于网络的每个弧有一个可信度，贝叶斯网络也被称作信念网络** (belief networks)。和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述事件之间的相关性。可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。 贝叶斯网络与马尔科夫链的比较使用贝叶斯网络必须知道各个状态之间相关的概率。得到这些参数的过程叫做训练。和训练马尔可夫模型一样，训练贝叶斯网络要用一些已知的数据。比如在训练上面的网络，需要知道一些心血管疾病和吸烟、家族病史等有关的情况。相比马尔可夫链，贝叶斯网络的训练比较复杂，从理论上讲，它是一个 NP-complete 问题，也就是说，对于现在的计算机是不可计算的。但是，对于某些应用，这个训练过程可以简化，并在计算上实现。 贝叶斯网络的工具与应用值得一提的是 IBM Watson 研究所的茨威格博士 (Geoffrey Zweig) 和西雅图华盛顿大学的比尔默 (Jeff Bilmes) 教授完成了一个通用的贝叶斯网络的工具包，提供给对贝叶斯网络有兴趣的研究者。贝叶斯网络在图像处理、文字处理、支持决策等方面有很多应用。在文字处理方面，语义相近的词之间的关系可以用一个贝叶斯网络来描述。我们利用贝叶斯网络，可以找出近义词和相关的词，在 Google 搜索和 Google 广告中都有直接的应用。原文链接","categories":[{"name":"数学","slug":"数学","permalink":"http://www.wortyby.com/categories/数学/"}],"tags":[{"name":"分类","slug":"分类","permalink":"http://www.wortyby.com/tags/分类/"},{"name":"图","slug":"图","permalink":"http://www.wortyby.com/tags/图/"}]},{"title":"PySpark 初体验之DataFrame","slug":"PySpark 初体验之DataFrame","date":"2018-09-04T11:47:24.000Z","updated":"2018-09-04T11:47:24.000Z","comments":true,"path":"2018/09/04/PySpark 初体验之DataFrame/","link":"","permalink":"http://www.wortyby.com/2018/09/04/PySpark 初体验之DataFrame/","excerpt":"","text":"##背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 鉴于这篇文章介绍过RDD，本篇文章重点介绍DataFrame。DataFrame 是一个组织成命名列的数据集。它在概念上等同于关系数据库中的表或是Python 中的里的数据框架(pandas)，但其经过了优化。DataFrames 可以从各种各样的源构建，例如结构化数据文件,Hive 中的表,外部数据库,RDD 转换DataFrame API 可以被Scala，Java，Python和R调用。Python 中 DataFrame 由 Row 的数据集表示 。由于我们现在是在PySpark 的环境中使用，所以我们就谈谈 DataFrame 在 Python 中的使用。或许之前了解过 pandas 里的dataframe 操作，也可以对比着了解，同时 也支持 PySpark DataFrame 与 pandas 的 dataframe 之间的转换。该文将以 Spark初始化，DataFrame 初始化，DataFrame 运算，RDD 与 DataFrame 转化，DataFrame 与 pandas dataframe 转化 四个部分展开 Spark 初始化：在 进行 DataFrame 操作 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkSession 与 SparkContext 初始化123456789from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(&quot;RDD_and_DataFrame&quot;) \\ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \\ .getOrCreate() sc = spark.sparkContextsparkContext 初始化之后，需要进行数据读取来初始化DataFrame DataFrame 初始化我们以从 txt 文档读取数据的方式来初始化DataFrame 的例子来说明 DataFrame 的相关运算方式以及相关性质 创建DataFrame 以及 DataFrame 与 RDD 的转换123456789101112131415161718192021 lines = sc.textFile(&quot;employee.txt&quot;)parts = lines.map(lambda l: l.split(&quot;,&quot;))employee = parts.map(lambda p: Row(name=p[0], salary=int(p[1]))) #RDD转换成DataFrameemployee_temp = spark.createDataFrame(employee) #显示DataFrame数据employee_temp.show() #创建视图employee_temp.createOrReplaceTempView(&quot;employee&quot;)#过滤数据employee_result = spark.sql(&quot;SELECT name,salary FROM employee WHERE salary &gt;= 14000 AND salary &lt;= 20000&quot;) # DataFrame转换成RDDresult = employee_result.rdd.map(lambda p: &quot;name: &quot; + p.name + &quot; salary: &quot; + str(p.salary)).collect() #打印RDD数据for n in result:","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"DataFrame","slug":"DataFrame","permalink":"http://www.wortyby.com/tags/DataFrame/"}]},{"title":"PySpark 初体验之RDD","slug":"PySpark 初体验之运算规则","date":"2018-09-04T11:43:21.000Z","updated":"2018-09-04T11:43:21.000Z","comments":true,"path":"2018/09/04/PySpark 初体验之运算规则/","link":"","permalink":"http://www.wortyby.com/2018/09/04/PySpark 初体验之运算规则/","excerpt":"","text":"背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 本文重点介绍RDD，下篇文章重点介绍DataFrame。RDD 可以导入外部存储系统的数据集，例如：HDFS，HBASE 或者 本地文件 *.JSON 或者*.XML 等数据源。该文将以 SparkContext初始化，RDD 初始化，运算，持久化 四个部分展开 SparkContext 初始化：在 进行RDD 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkContext123from pyspark import SparkConf, SparkContextconf = *** #spark运行环境的配置信息初始化sc = SparkContext(conf)sparkContext 初始化之后，需要进行数据读取来初始化RDD RDD 初始化我们以初始化数字和字符串的例子来说明 RDD的相关运算方式以及相关性质 创建RDD1kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3)]) RDD 转化为 Python 数据类型RDD 类型的数据可以使用 collect 方法转化为 Python 的数据类型:1print (kvRDD.collect())输出为1[(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;, 3)] RDD 基本运算 map 运算map运算可以通过传入的函数，将每一个元素经过函数运算产生另外一个RDD。下面这个例子将 value 值统一加 1之后进行返回 关于 value 的list 。1print ((kvRDD.map(lambda x:x[1]+1).collect()))输出为1[5, 7, 7, 3, 4] filter 运算filter可以用于对RDD内每一个元素进行筛选，并产生另外一个RDD。下面的例子中，我们筛选kvRDD中数字小于3的元素。1print(kvRDD.filter(lambda a:a[0]&gt;3).collect())输出为1[(5, 6), (&apos;sd&apos;, 3)] distinct 运算distinct运算会删除重复的元素：下面的例子中，我们去除kvRDD中重复的的元素('sd',3)。12kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])print(kvRDD. distinct().collect())输出为1[(&apos;sd&apos;, 3), (5, 6), (1, 2), (3, 4), (3, 6)] randomSplit 运算randomSplit 运算将整个集合以随机数的方式按照比例分为多个RDD，比如按照0.4和0.6的比例将kvRDD分为两个RDD，并输出：1234567kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])sRDD = kvRDD.randomSplit([0.4,0.6])print (len(sRDD))print (sRDD[0].collect())print (sRDD[1].collect())输出为1232[(5, 6), (1, 2)][(3, 4), (3, 6), (&apos;sd&apos;, 3), (&apos;sd&apos;, 3)] groupBy 运算groupBy运算可以按照传入匿名函数的规则，将数据分为多个Array。123kvRDD = sc.parallelize([1,2,3,4,5])result = kvRDD.groupBy(lambda x: x % 2).collect()print(sorted([(x, sorted(y)) for (x, y) in result]))输出为1[(0, [2, 4]), (1, [1, 3, 5])] RDD 复合运算RDD 也支持执行多个RDD的运算,即支持 复合运算RDD 复合运算主要包含 并集运算、交集运算、差集运算、笛卡尔积运算先准备三个DEMO RDD123RDD1 = sc.parallelize([3,1,2,5,5])RDD2 = sc.parallelize([5,6])RDD3 = sc.parallelize([2,7]) 并集运算通过使用 union 函数进行并集运算:输入1print(RDD1.union(RDD2).union(RDD3).collect())输出1[3, 1, 2, 5, 5, 5, 6, 2, 7] 交集运算可以使用intersection进行交集运算：输入1print(RDD1.intersection(RDD3).collect())输出1[2] 差集运算可以使用subtract函数进行差集运算:输入1print (RDD1.subtract(RDD2).collect())由于两个RDD的重复部分为5，所以输出为[1,2,3]:输出1[1,2,3] 笛卡尔积运算可以使用cartesian函数进行笛卡尔乘积运算:输入1print (RDD1.cartesian(RDD2).collect())由于两个RDD分别有5个元素和2个元素，所以返回结果有10各元素：输出1[(3, 5), (3, 6), (1, 5), (1, 6), (2, 5), (2, 6), (5, 5), (5, 5), (5, 6), (5, 6)] RDD 基本 “动作” 运算 读取元素可以使用下列命令读取RDD内的元素，这是Actions运算，所以会马上执行：12345678#取第一条数据print (RDD1.first())#取前两条数据print (RDD1.take(2))#升序排列，并取前3条数据print (RDD1.takeOrdered(10))#降序排列，并取前3条数据print (RDD1.takeOrdered(3,lambda x:-x))输出为:12343[3, 1][1, 2, 3, 5, 5][5, 5, 3] 统计功能可以将RDD内的元素进行统计运算：1234567891011121314#统计print (RDD1.stats())#最小值print (RDD1.min())#最大值print (RDD1.max())#标准差print (RDD1.stdev())#计数print (RDD1.count())#求和print (RDD1.sum())#平均print (RDD1.mean())输出为:1234567(count: 5, mean: 3.2, stdev: 1.6, max: 5.0, min: 1.0)151.65163.2 RDD key-Value 基本 “转换” 运算Spark RDD支持键值对运算，Key-Value运算是 MR*(mapreduce)*运算的基础，本节介绍RDD键值的基本“转换”运算。 初始化我们用元素类型为tuple元组的数组初始化我们的RDD，这里，每个tuple的第一个值将作为键，而第二个元素将作为值。1kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)]) 得到key和value值可以使用keys和values函数分别得到RDD的键数组和值数组：12print (kvRDD1.keys().collect())print (kvRDD1.values().collect())输出为：12[3, 3, 5, 1][4, 6, 6, 2] 筛选元素可以按照键进行元素筛选，也可以通过值进行元素筛选，和之前的一样，使用filter函数，这里要注意的是，虽然RDD中是以键值对形式存在，但是本质上还是一个二元组，二元组的第一个值代表键，第二个值代表值，所以按照如下的代码既可以按照键进行筛选，我们筛选键值小于5的数据：1print (kvRDD1.filter(lambda x:x[0] &lt; 5).collect())输出为：1[(3, 4), (3, 6), (1, 2)]同样，将x[0]替换为x[1]就是按照值进行筛选，我们筛选值小于5的数据：1print (kvRDD1.filter(lambda x:x[1] &lt; 5).collect())输出为：1[(3, 4), (1, 2)] 值运算我们可以使用mapValues方法处理value值，下面的代码将value值进行了平方处理：1print (kvRDD1.mapValues(lambda x:x**2).collect())输出为：1[(3, 16), (3, 36), (5, 36), (1, 4)] 按照key排序可以使用sortByKey按照key进行排序，传入参数的默认值为true，是按照从小到大排序，也可以传入参数false，表示从大到小排序：123print (kvRDD1.sortByKey().collect())print (kvRDD1.sortByKey(True).collect())print (kvRDD1.sortByKey(False).collect())输出为：123[(1, 2), (3, 4), (3, 6), (5, 6)][(1, 2), (3, 4), (3, 6), (5, 6)][(5, 6), (3, 4), (3, 6), (1, 2)] 合并相同key值的数据使用reduceByKey函数可以对具有相同key值的数据进行合并。比如下面的代码，由于RDD中存在（3,4）和（3,6）两条key值均为3的数据，他们将被合为一条数据：1print (kvRDD1.reduceByKey(lambda x,y:x+y).collect())输出为1[(1, 2), (3, 10), (5, 6)] 多个RDD Key-Value“转换”运算 初始化首先我们初始化两个k-v的RDD：12kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])kvRDD2 = sc.parallelize([(3,8)]) 内连接运算join运算可以实现类似数据库的内连接，将两个RDD按照相同的key值join起来，kvRDD1与kvRDD2的key值唯一相同的是3，kvRDD1中有两条key值为3的数据（3,4）和（3,6），而kvRDD2中只有一条key值为3的数据（3,8），所以join的结果是（3，（4,8）） 和（3，（6，8））：1print (kvRDD1.join(kvRDD2).collect())输出为:1[(3, (4, 8)), (3, (6, 8))] 左外连接使用leftOuterJoin可以实现类似数据库的左外连接，如果kvRDD1的key值对应不到kvRDD2，就会显示None1print (kvRDD1.leftOuterJoin(kvRDD2).collect())输出为:1[(1, (2, None)), (3, (4, 8)), (3, (6, 8)), (5, (6, None))] 右外连接使用rightOuterJoin可以实现类似数据库的右外连接，如果kvRDD2的key值对应不到kvRDD1，就会显示None1print (kvRDD1.rightOuterJoin(kvRDD2).collect())输出为：1[(3, (4, 8)), (3, (6, 8))] 删除相同key值数据使用subtractByKey运算会删除相同key值得数据：1print (kvRDD1.subtractByKey(kvRDD2).collect())结果为：1[(1, 2), (5, 6)] Key-Value“动作”运算 读取数据可以使用下面的几种方式读取RDD的数据：1234567891011121314#读取第一条数据print (kvRDD1.first())#读取前两条数据print (kvRDD1.take(2))#读取第一条数据的key值print (kvRDD1.first()[0])#读取第一条数据的value值print (kvRDD1.first()[1])输出为:(3, 4)[(3, 4), (3, 6)]34 按key值统计：使用countByKey函数可以统计各个key值对应的数据的条数：1print (kvRDD1.countByKey().collect())输出为：1defaultdict(&lt;type &apos;int&apos;&gt;, &#123;1: 1, 3: 2, 5: 1&#125;) lookup查找运算使用lookup函数可以根据输入的key值来查找对应的Value值：1print (kvRDD1.lookup(3))输出为：1[4, 6] 持久化操作spark RDD的持久化机制，可以将需要重复运算的RDD存储在内存中，以便大幅提升运算效率，有两个主要的函数：持久化使用persist函数对RDD进行持久化：1kvRDD1.persist()在持久化的同时我们可以指定持久化存储等级：等级说明MEMORY_ONLY以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将不会被缓存， 这样当再次需要这些分区的时候，将会重新计算。这是默认的级别。MEMORY_AND_DISK以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将将会缓存在磁盘上，再次需要的时候从磁盘读取。MEMORY_ONLY_SER以序列化JAVA对象的方式存储 (每个分区一个字节数组). 相比于反序列化的方式,这样更高效的利用空间， 尤其是使用快速序列化时。但是读取是CPU操作很密集。MEMORY_AND_DISK_SER与MEMORY_ONLY_SER相似, 区别是但内存不足时，存储在磁盘上而不是每次重新计算。DISK_ONLY只存储RDD在磁盘MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.与上面的级别相同，只不过每个分区的副本只存储在两个集群节点上。OFF_HEAP (experimental)将RDD以序列化的方式存储在 Tachyon. 与 MEMORY_ONLY_SER相比, OFF_HEAP减少了垃圾回收。允许执行体更小通过共享一个内存池。因此对于拥有较大堆内存和高并发的环境有较大的吸引力。更重要的是，因为RDD存储在Tachyon上，执行体的崩溃不会造成缓存的丢失。在这种模式下.Tachyon中的内存是可丢弃的，这样 Tachyon 对于从内存中挤出的块不会试图重建它。如果你打算使用Tachyon作为堆缓存，Spark提供了与Tachyon相兼容的版本。首先我们导入相关函数：1from pyspark.storagelevel import StorageLevel在scala中可以直接使用上述的持久化等级关键词，但是在pyspark中封装为了一个类，StorageLevel类，并在初始化时指定一些参数，通过不同的参数组合，可以实现上面的不同存储等级。StorageLevel类的初始化函数如下：123456def __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1): self.useDisk = useDisk self.useMemory = useMemory self.useOffHeap = useOffHeap self.deserialized = deserialized self.replication = replication那么不同的存储等级对应的参数为:1234567891011121314151617181920StorageLevel.DISK_ONLY = StorageLevel(True, False, False, False)StorageLevel.DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)StorageLevel.MEMORY_ONLY = StorageLevel(False, True, False, False)StorageLevel.MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)StorageLevel.MEMORY_AND_DISK = StorageLevel(True, True, False, False)StorageLevel.MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)StorageLevel.OFF_HEAP = StorageLevel(True, True, True, False, 1)&quot;&quot;&quot;.. note:: The following four storage level constants are deprecated in 2.0, since the records \\will always be serialized in Python.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER = StorageLevel.MEMORY_ONLY&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER_2 = StorageLevel.MEMORY_ONLY_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY_2`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER = StorageLevel.MEMORY_AND_DISK&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER_2 = StorageLevel.MEMORY_AND_DISK_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK_2`` instead.&quot;&quot;&quot; 取消持久化使用unpersist函数对RDD进行持久化：1kvRDD1.unpersist() 综上所述:哇，有关pyspark的RDD的基本操作就是上面这些啦，想要了解更多的盆友们可以参照官网给出的官方文档今天主要介绍了两种RDD，基本的RDD和Key-Value形式的RDD，介绍了他们的几种“转换”运算和“动作”运算，整理如下：RDD运算说明基本RDD“转换”运算map（对各数据进行转换），filter（过滤符合条件的数据），distinct（去重运算），randomSplit（根据指定的比例随机分为N各RDD），groupBy（根据条件对数据进行分组），union（两个RDD取并集），intersection（两个RDD取交集），subtract（两个RDD取差集）。cartesian（两个RDD进行笛卡尔积运算）基本RDD“动作”运算first（取第一条数据），take（取前几条数据），takeOrdered（排序后取前N条数据），统计函数Key-Value形式 RDD“转换”运算filter（过滤符合条件的数据），mapValues（对value值进行转换），sortByKey（根据key值进行排序），reduceByKey（合并相同key值的数据），join（内连接两个KDD），leftOuterJoin（左外连接两个KDD），rightOuterJoin（右外连接两个RDD），subtractByKey（相当于key值得差集运算）Key-Value形式 RDD“动作”运算first（取第一条数据），take（取前几条数据），countByKey（根据key值分组统计），lookup（根据key值查找value值）RDD持久化persist用于对RDD进行持久化，unpersist取消RDD的持久化，注意持久化的存储等级","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"RDD","slug":"RDD","permalink":"http://www.wortyby.com/tags/RDD/"}]},{"title":"tmpwatch 工具使用说明","slug":"tmpwatch 工具使用说明","date":"2018-08-31T09:14:56.000Z","updated":"2018-08-31T09:14:56.000Z","comments":true,"path":"2018/08/31/tmpwatch 工具使用说明/","link":"","permalink":"http://www.wortyby.com/2018/08/31/tmpwatch 工具使用说明/","excerpt":"","text":"背景tmp 目录原来只有在启动的时候才会被清理…… 不同的 Linux 发行版其实对 /tmp 目录的清理方式有所不同：在 Debian-like 的系统，启动的时候才会清理 (规则定义在 /etc/default/rcS)在 RedHat-like 的系统，按文件存在时间定时清理 (RHEL6 规则定义在 /etc/cron.daily/tmpwatch; RHEL7 以及 RedHat-like with systemd 规则定义在 /usr/lib/tmpfiles.d/tmp.conf, 通过 systemd-tmpfiles-clean.service 服务调用)在 CentOS 里，也是按文件存在时间清理的 (通过 crontab 的配置 /etc/cron.daily 定时执行 tmpwatch 来实现)在 Gentoo 里也是启动清理，规则定义在 /etc/conf.d/bootmisc，大 Gentoo 就是不走寻常路对于那些只能开机清理临时文件的发行版，如果作为服务器，这种不重启就对临时文件目录的垃圾不问不管的做事风格实在是很不靠谱。不过从上面其他发行版大家估计也会发现，解决此问题的关键就在于 tmpwatch 和定时任务的配合使用。 服务器除了调用用户的计划任务外，还会执行系统自己的，比如：12345/etc/cron.hourly/etc/cron.daily/etc/cron.daily目的能够自动删除不经常使用的临时文件为了保证tmp目录不爆满，系统默认情况下每日会处理一次tmp目录文件。安装1yum install tmpwatch -y功能说明删除暂存文件(默认是240小时，10天)tmpwatch 是专门用于解决“删除 xxx 天没有被访问/修改过的文件”这样需求的命令。使用方式也极其简单：除了删除tmp 文件夹之外也可以删除任何其他指定文件夹语法tmpwatch [-afqv][–test][超期时间][目录…]执行tmpwatch指令可删除不必要的暂存文件，您可以设置文件超期时间，单位以小时计算。 tmpwatch 参数说明123456789101112131415-u, --atime 基于访问时间来删除文件，默认的。-m, --mtime 基于修改时间来删除文件。-c, --ctime 基于创建时间来删除文件，对于目录，基于mtime。-M, --dirmtime 删除目录基于目录的修改时间而不是访问时间。-a, --all 删除所有的文件类型，不只是普通文件，符号链接和目录。-d, --nodirs 不尝试删除目录，即使是空目录。-d, --nosymlinks 不尝试删除符号链接。-f, --force 强制删除。-q, --quiet 只报告错误信息。-s, --fuser 如果文件已经是打开状态在删除前，尝试使用“定影”命令。默认不启用。-t, --test 仅作测试，并不真的删除文件或目录。-U, --exclude-user=user 不删除属于谁的文件。-v, --verbose 打印详细信息。-x, --exclude=path 排除路径，如果路径是一个目录，它包含的所有文件被排除了。如果路径不存在，它必须是一个绝对路径不包含符号链接。-X, --exclude-pattern=pattern 排除某规则下的路径。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"清理","slug":"清理","permalink":"http://www.wortyby.com/tags/清理/"}]}]}