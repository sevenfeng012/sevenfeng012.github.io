{"meta":{"title":"值得荐","subtitle":"值得推荐的原创文章，技术文章或是科普文章","description":"同乐科技","author":"1One's Dad","url":"http://www.wortyby.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-08-29T02:08:14.000Z","updated":"2018-08-29T05:24:24.709Z","comments":false,"path":"/404.html","permalink":"http://www.wortyby.com//404.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-31T09:06:48.000Z","updated":"2018-08-31T09:12:59.268Z","comments":true,"path":"categories/index.html","permalink":"http://www.wortyby.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-08-31T09:08:27.000Z","updated":"2018-08-31T09:13:13.095Z","comments":true,"path":"tags/index.html","permalink":"http://www.wortyby.com/tags/index.html","excerpt":"","text":""},{"title":"datastructure","date":"2018-08-31T09:09:28.000Z","updated":"2018-08-31T09:12:41.723Z","comments":true,"path":"datastructure/index.html","permalink":"http://www.wortyby.com/datastructure/index.html","excerpt":"","text":""}],"posts":[{"title":"PySpark 初体验之DataFrame","slug":"PySpark 初体验之DataFrame","date":"2018-09-04T11:06:16.164Z","updated":"2018-09-04T11:47:24.031Z","comments":true,"path":"2018/09/04/PySpark 初体验之DataFrame/","link":"","permalink":"http://www.wortyby.com/2018/09/04/PySpark 初体验之DataFrame/","excerpt":"","text":"##背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 鉴于这篇文章介绍过RDD，本篇文章重点介绍DataFrame。DataFrame 是一个组织成命名列的数据集。它在概念上等同于关系数据库中的表或是Python 中的里的数据框架(pandas)，但其经过了优化。DataFrames 可以从各种各样的源构建，例如结构化数据文件,Hive 中的表,外部数据库,RDD 转换DataFrame API 可以被Scala，Java，Python和R调用。Python 中 DataFrame 由 Row 的数据集表示 。由于我们现在是在PySpark 的环境中使用，所以我们就谈谈 DataFrame 在 Python 中的使用。或许之前了解过 pandas 里的dataframe 操作，也可以对比着了解，同时 也支持 PySpark DataFrame 与 pandas 的 dataframe 之间的转换。该文将以 Spark初始化，DataFrame 初始化，DataFrame 运算，RDD 与 DataFrame 转化，DataFrame 与 pandas dataframe 转化 四个部分展开 Spark 初始化：在 进行 DataFrame 操作 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkSession 与 SparkContext 初始化123456789from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(&quot;RDD_and_DataFrame&quot;) \\ .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) \\ .getOrCreate() sc = spark.sparkContextsparkContext 初始化之后，需要进行数据读取来初始化DataFrame DataFrame 初始化我们以从 txt 文档读取数据的方式来初始化DataFrame 的例子来说明 DataFrame 的相关运算方式以及相关性质 创建DataFrame 以及 DataFrame 与 RDD 的转换123456789101112131415161718192021 lines = sc.textFile(&quot;employee.txt&quot;)parts = lines.map(lambda l: l.split(&quot;,&quot;))employee = parts.map(lambda p: Row(name=p[0], salary=int(p[1]))) #RDD转换成DataFrameemployee_temp = spark.createDataFrame(employee) #显示DataFrame数据employee_temp.show() #创建视图employee_temp.createOrReplaceTempView(&quot;employee&quot;)#过滤数据employee_result = spark.sql(&quot;SELECT name,salary FROM employee WHERE salary &gt;= 14000 AND salary &lt;= 20000&quot;) # DataFrame转换成RDDresult = employee_result.rdd.map(lambda p: &quot;name: &quot; + p.name + &quot; salary: &quot; + str(p.salary)).collect() #打印RDD数据for n in result:","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"DataFrame","slug":"DataFrame","permalink":"http://www.wortyby.com/tags/DataFrame/"}]},{"title":"PySpark 初体验之RDD","slug":"PySpark 初体验之运算规则","date":"2018-09-03T08:24:27.468Z","updated":"2018-09-04T11:43:21.196Z","comments":true,"path":"2018/09/03/PySpark 初体验之运算规则/","link":"","permalink":"http://www.wortyby.com/2018/09/03/PySpark 初体验之运算规则/","excerpt":"","text":"背景PySpark 进行数据运算的时候，有2个基本概念是需要我们重点掌握的。分别是DataFrame与RDD(Resilient Distributed DataSet)，RDD 主要负责数据计算，DataFrame 主要负责数据操作。 本文重点介绍RDD，下篇文章重点介绍DataFrame。RDD 可以导入外部存储系统的数据集，例如：HDFS，HBASE 或者 本地文件 *.JSON 或者*.XML 等数据源。该文将以 SparkContext初始化，RDD 初始化，运算，持久化 四个部分展开 SparkContext 初始化：在 进行RDD 的时候需要对Spark 上下文环境进行初始化，在PySpark 中通过如下方式进行初始化SparkContext123from pyspark import SparkConf, SparkContextconf = *** #spark运行环境的配置信息初始化sc = SparkContext(conf)sparkContext 初始化之后，需要进行数据读取来初始化RDD RDD 初始化我们以初始化数字和字符串的例子来说明 RDD的相关运算方式以及相关性质 创建RDD1kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3)]) RDD 转化为 Python 数据类型RDD 类型的数据可以使用 collect 方法转化为 Python 的数据类型:1print (kvRDD.collect())输出为1[(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;, 3)] RDD 基本运算 map 运算map运算可以通过传入的函数，将每一个元素经过函数运算产生另外一个RDD。下面这个例子将 value 值统一加 1之后进行返回 关于 value 的list 。1print ((kvRDD.map(lambda x:x[1]+1).collect()))输出为1[5, 7, 7, 3, 4] filter 运算filter可以用于对RDD内每一个元素进行筛选，并产生另外一个RDD。下面的例子中，我们筛选kvRDD中数字小于3的元素。1print(kvRDD.filter(lambda a:a[0]&gt;3).collect())输出为1[(5, 6), (&apos;sd&apos;, 3)] distinct 运算distinct运算会删除重复的元素：下面的例子中，我们去除kvRDD中重复的的元素('sd',3)。12kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])print(kvRDD. distinct().collect())输出为1[(&apos;sd&apos;, 3), (5, 6), (1, 2), (3, 4), (3, 6)] randomSplit 运算randomSplit 运算将整个集合以随机数的方式按照比例分为多个RDD，比如按照0.4和0.6的比例将kvRDD分为两个RDD，并输出：1234567kvRDD = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2), (&apos;sd&apos;,3),(&apos;sd&apos;,3)])sRDD = kvRDD.randomSplit([0.4,0.6])print (len(sRDD))print (sRDD[0].collect())print (sRDD[1].collect())输出为1232[(5, 6), (1, 2)][(3, 4), (3, 6), (&apos;sd&apos;, 3), (&apos;sd&apos;, 3)] groupBy 运算groupBy运算可以按照传入匿名函数的规则，将数据分为多个Array。123kvRDD = sc.parallelize([1,2,3,4,5])result = kvRDD.groupBy(lambda x: x % 2).collect()print(sorted([(x, sorted(y)) for (x, y) in result]))输出为1[(0, [2, 4]), (1, [1, 3, 5])] RDD 复合运算RDD 也支持执行多个RDD的运算,即支持 复合运算RDD 复合运算主要包含 并集运算、交集运算、差集运算、笛卡尔积运算先准备三个DEMO RDD123RDD1 = sc.parallelize([3,1,2,5,5])RDD2 = sc.parallelize([5,6])RDD3 = sc.parallelize([2,7]) 并集运算通过使用 union 函数进行并集运算:输入1print(RDD1.union(RDD2).union(RDD3).collect())输出1[3, 1, 2, 5, 5, 5, 6, 2, 7] 交集运算可以使用intersection进行交集运算：输入1print(RDD1.intersection(RDD3).collect())输出1[2] 差集运算可以使用subtract函数进行差集运算:输入1print (RDD1.subtract(RDD2).collect())由于两个RDD的重复部分为5，所以输出为[1,2,3]:输出1[1,2,3] 笛卡尔积运算可以使用cartesian函数进行笛卡尔乘积运算:输入1print (RDD1.cartesian(RDD2).collect())由于两个RDD分别有5个元素和2个元素，所以返回结果有10各元素：输出1[(3, 5), (3, 6), (1, 5), (1, 6), (2, 5), (2, 6), (5, 5), (5, 5), (5, 6), (5, 6)] RDD 基本 “动作” 运算 读取元素可以使用下列命令读取RDD内的元素，这是Actions运算，所以会马上执行：12345678#取第一条数据print (RDD1.first())#取前两条数据print (RDD1.take(2))#升序排列，并取前3条数据print (RDD1.takeOrdered(10))#降序排列，并取前3条数据print (RDD1.takeOrdered(3,lambda x:-x))输出为:12343[3, 1][1, 2, 3, 5, 5][5, 5, 3] 统计功能可以将RDD内的元素进行统计运算：1234567891011121314#统计print (RDD1.stats())#最小值print (RDD1.min())#最大值print (RDD1.max())#标准差print (RDD1.stdev())#计数print (RDD1.count())#求和print (RDD1.sum())#平均print (RDD1.mean())输出为:1234567(count: 5, mean: 3.2, stdev: 1.6, max: 5.0, min: 1.0)151.65163.2 RDD key-Value 基本 “转换” 运算Spark RDD支持键值对运算，Key-Value运算是 MR*(mapreduce)*运算的基础，本节介绍RDD键值的基本“转换”运算。 初始化我们用元素类型为tuple元组的数组初始化我们的RDD，这里，每个tuple的第一个值将作为键，而第二个元素将作为值。1kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)]) 得到key和value值可以使用keys和values函数分别得到RDD的键数组和值数组：12print (kvRDD1.keys().collect())print (kvRDD1.values().collect())输出为：12[3, 3, 5, 1][4, 6, 6, 2] 筛选元素可以按照键进行元素筛选，也可以通过值进行元素筛选，和之前的一样，使用filter函数，这里要注意的是，虽然RDD中是以键值对形式存在，但是本质上还是一个二元组，二元组的第一个值代表键，第二个值代表值，所以按照如下的代码既可以按照键进行筛选，我们筛选键值小于5的数据：1print (kvRDD1.filter(lambda x:x[0] &lt; 5).collect())输出为：1[(3, 4), (3, 6), (1, 2)]同样，将x[0]替换为x[1]就是按照值进行筛选，我们筛选值小于5的数据：1print (kvRDD1.filter(lambda x:x[1] &lt; 5).collect())输出为：1[(3, 4), (1, 2)] 值运算我们可以使用mapValues方法处理value值，下面的代码将value值进行了平方处理：1print (kvRDD1.mapValues(lambda x:x**2).collect())输出为：1[(3, 16), (3, 36), (5, 36), (1, 4)] 按照key排序可以使用sortByKey按照key进行排序，传入参数的默认值为true，是按照从小到大排序，也可以传入参数false，表示从大到小排序：123print (kvRDD1.sortByKey().collect())print (kvRDD1.sortByKey(True).collect())print (kvRDD1.sortByKey(False).collect())输出为：123[(1, 2), (3, 4), (3, 6), (5, 6)][(1, 2), (3, 4), (3, 6), (5, 6)][(5, 6), (3, 4), (3, 6), (1, 2)] 合并相同key值的数据使用reduceByKey函数可以对具有相同key值的数据进行合并。比如下面的代码，由于RDD中存在（3,4）和（3,6）两条key值均为3的数据，他们将被合为一条数据：1print (kvRDD1.reduceByKey(lambda x,y:x+y).collect())输出为1[(1, 2), (3, 10), (5, 6)] 多个RDD Key-Value“转换”运算 初始化首先我们初始化两个k-v的RDD：12kvRDD1 = sc.parallelize([(3,4),(3,6),(5,6),(1,2)])kvRDD2 = sc.parallelize([(3,8)]) 内连接运算join运算可以实现类似数据库的内连接，将两个RDD按照相同的key值join起来，kvRDD1与kvRDD2的key值唯一相同的是3，kvRDD1中有两条key值为3的数据（3,4）和（3,6），而kvRDD2中只有一条key值为3的数据（3,8），所以join的结果是（3，（4,8）） 和（3，（6，8））：1print (kvRDD1.join(kvRDD2).collect())输出为:1[(3, (4, 8)), (3, (6, 8))] 左外连接使用leftOuterJoin可以实现类似数据库的左外连接，如果kvRDD1的key值对应不到kvRDD2，就会显示None1print (kvRDD1.leftOuterJoin(kvRDD2).collect())输出为:1[(1, (2, None)), (3, (4, 8)), (3, (6, 8)), (5, (6, None))] 右外连接使用rightOuterJoin可以实现类似数据库的右外连接，如果kvRDD2的key值对应不到kvRDD1，就会显示None1print (kvRDD1.rightOuterJoin(kvRDD2).collect())输出为：1[(3, (4, 8)), (3, (6, 8))] 删除相同key值数据使用subtractByKey运算会删除相同key值得数据：1print (kvRDD1.subtractByKey(kvRDD2).collect())结果为：1[(1, 2), (5, 6)] Key-Value“动作”运算 读取数据可以使用下面的几种方式读取RDD的数据：1234567891011121314#读取第一条数据print (kvRDD1.first())#读取前两条数据print (kvRDD1.take(2))#读取第一条数据的key值print (kvRDD1.first()[0])#读取第一条数据的value值print (kvRDD1.first()[1])输出为:(3, 4)[(3, 4), (3, 6)]34 按key值统计：使用countByKey函数可以统计各个key值对应的数据的条数：1print (kvRDD1.countByKey().collect())输出为：1defaultdict(&lt;type &apos;int&apos;&gt;, &#123;1: 1, 3: 2, 5: 1&#125;) lookup查找运算使用lookup函数可以根据输入的key值来查找对应的Value值：1print (kvRDD1.lookup(3))输出为：1[4, 6] 持久化操作spark RDD的持久化机制，可以将需要重复运算的RDD存储在内存中，以便大幅提升运算效率，有两个主要的函数：持久化使用persist函数对RDD进行持久化：1kvRDD1.persist()在持久化的同时我们可以指定持久化存储等级：等级说明MEMORY_ONLY以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将不会被缓存， 这样当再次需要这些分区的时候，将会重新计算。这是默认的级别。MEMORY_AND_DISK以反序列化的JAVA对象的方式存储在JVM中. 如果内存不够， RDD的一些分区将将会缓存在磁盘上，再次需要的时候从磁盘读取。MEMORY_ONLY_SER以序列化JAVA对象的方式存储 (每个分区一个字节数组). 相比于反序列化的方式,这样更高效的利用空间， 尤其是使用快速序列化时。但是读取是CPU操作很密集。MEMORY_AND_DISK_SER与MEMORY_ONLY_SER相似, 区别是但内存不足时，存储在磁盘上而不是每次重新计算。DISK_ONLY只存储RDD在磁盘MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.与上面的级别相同，只不过每个分区的副本只存储在两个集群节点上。OFF_HEAP (experimental)将RDD以序列化的方式存储在 Tachyon. 与 MEMORY_ONLY_SER相比, OFF_HEAP减少了垃圾回收。允许执行体更小通过共享一个内存池。因此对于拥有较大堆内存和高并发的环境有较大的吸引力。更重要的是，因为RDD存储在Tachyon上，执行体的崩溃不会造成缓存的丢失。在这种模式下.Tachyon中的内存是可丢弃的，这样 Tachyon 对于从内存中挤出的块不会试图重建它。如果你打算使用Tachyon作为堆缓存，Spark提供了与Tachyon相兼容的版本。首先我们导入相关函数：1from pyspark.storagelevel import StorageLevel在scala中可以直接使用上述的持久化等级关键词，但是在pyspark中封装为了一个类，StorageLevel类，并在初始化时指定一些参数，通过不同的参数组合，可以实现上面的不同存储等级。StorageLevel类的初始化函数如下：123456def __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1): self.useDisk = useDisk self.useMemory = useMemory self.useOffHeap = useOffHeap self.deserialized = deserialized self.replication = replication那么不同的存储等级对应的参数为:1234567891011121314151617181920StorageLevel.DISK_ONLY = StorageLevel(True, False, False, False)StorageLevel.DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)StorageLevel.MEMORY_ONLY = StorageLevel(False, True, False, False)StorageLevel.MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)StorageLevel.MEMORY_AND_DISK = StorageLevel(True, True, False, False)StorageLevel.MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)StorageLevel.OFF_HEAP = StorageLevel(True, True, True, False, 1)&quot;&quot;&quot;.. note:: The following four storage level constants are deprecated in 2.0, since the records \\will always be serialized in Python.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER = StorageLevel.MEMORY_ONLY&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_ONLY_SER_2 = StorageLevel.MEMORY_ONLY_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_ONLY_2`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER = StorageLevel.MEMORY_AND_DISK&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK`` instead.&quot;&quot;&quot;StorageLevel.MEMORY_AND_DISK_SER_2 = StorageLevel.MEMORY_AND_DISK_2&quot;&quot;&quot;.. note:: Deprecated in 2.0, use ``StorageLevel.MEMORY_AND_DISK_2`` instead.&quot;&quot;&quot; 取消持久化使用unpersist函数对RDD进行持久化：1kvRDD1.unpersist() 综上所述:哇，有关pyspark的RDD的基本操作就是上面这些啦，想要了解更多的盆友们可以参照官网给出的官方文档今天主要介绍了两种RDD，基本的RDD和Key-Value形式的RDD，介绍了他们的几种“转换”运算和“动作”运算，整理如下：RDD运算说明基本RDD“转换”运算map（对各数据进行转换），filter（过滤符合条件的数据），distinct（去重运算），randomSplit（根据指定的比例随机分为N各RDD），groupBy（根据条件对数据进行分组），union（两个RDD取并集），intersection（两个RDD取交集），subtract（两个RDD取差集）。cartesian（两个RDD进行笛卡尔积运算）基本RDD“动作”运算first（取第一条数据），take（取前几条数据），takeOrdered（排序后取前N条数据），统计函数Key-Value形式 RDD“转换”运算filter（过滤符合条件的数据），mapValues（对value值进行转换），sortByKey（根据key值进行排序），reduceByKey（合并相同key值的数据），join（内连接两个KDD），leftOuterJoin（左外连接两个KDD），rightOuterJoin（右外连接两个RDD），subtractByKey（相当于key值得差集运算）Key-Value形式 RDD“动作”运算first（取第一条数据），take（取前几条数据），countByKey（根据key值分组统计），lookup（根据key值查找value值）RDD持久化persist用于对RDD进行持久化，unpersist取消RDD的持久化，注意持久化的存储等级","categories":[{"name":"大数据","slug":"大数据","permalink":"http://www.wortyby.com/categories/大数据/"}],"tags":[{"name":"SPARK","slug":"SPARK","permalink":"http://www.wortyby.com/tags/SPARK/"},{"name":"RDD","slug":"RDD","permalink":"http://www.wortyby.com/tags/RDD/"}]},{"title":"tmpwatch 工具使用说明","slug":"tmpwatch 工具使用说明","date":"2018-08-28T11:47:00.266Z","updated":"2018-08-31T09:14:56.837Z","comments":true,"path":"2018/08/28/tmpwatch 工具使用说明/","link":"","permalink":"http://www.wortyby.com/2018/08/28/tmpwatch 工具使用说明/","excerpt":"","text":"背景tmp 目录原来只有在启动的时候才会被清理…… 不同的 Linux 发行版其实对 /tmp 目录的清理方式有所不同：在 Debian-like 的系统，启动的时候才会清理 (规则定义在 /etc/default/rcS)在 RedHat-like 的系统，按文件存在时间定时清理 (RHEL6 规则定义在 /etc/cron.daily/tmpwatch; RHEL7 以及 RedHat-like with systemd 规则定义在 /usr/lib/tmpfiles.d/tmp.conf, 通过 systemd-tmpfiles-clean.service 服务调用)在 CentOS 里，也是按文件存在时间清理的 (通过 crontab 的配置 /etc/cron.daily 定时执行 tmpwatch 来实现)在 Gentoo 里也是启动清理，规则定义在 /etc/conf.d/bootmisc，大 Gentoo 就是不走寻常路对于那些只能开机清理临时文件的发行版，如果作为服务器，这种不重启就对临时文件目录的垃圾不问不管的做事风格实在是很不靠谱。不过从上面其他发行版大家估计也会发现，解决此问题的关键就在于 tmpwatch 和定时任务的配合使用。 服务器除了调用用户的计划任务外，还会执行系统自己的，比如：12345/etc/cron.hourly/etc/cron.daily/etc/cron.daily目的能够自动删除不经常使用的临时文件为了保证tmp目录不爆满，系统默认情况下每日会处理一次tmp目录文件。安装1yum install tmpwatch -y功能说明删除暂存文件(默认是240小时，10天)tmpwatch 是专门用于解决“删除 xxx 天没有被访问/修改过的文件”这样需求的命令。使用方式也极其简单：除了删除tmp 文件夹之外也可以删除任何其他指定文件夹语法tmpwatch [-afqv][–test][超期时间][目录…]执行tmpwatch指令可删除不必要的暂存文件，您可以设置文件超期时间，单位以小时计算。 tmpwatch 参数说明123456789101112131415-u, --atime 基于访问时间来删除文件，默认的。-m, --mtime 基于修改时间来删除文件。-c, --ctime 基于创建时间来删除文件，对于目录，基于mtime。-M, --dirmtime 删除目录基于目录的修改时间而不是访问时间。-a, --all 删除所有的文件类型，不只是普通文件，符号链接和目录。-d, --nodirs 不尝试删除目录，即使是空目录。-d, --nosymlinks 不尝试删除符号链接。-f, --force 强制删除。-q, --quiet 只报告错误信息。-s, --fuser 如果文件已经是打开状态在删除前，尝试使用“定影”命令。默认不启用。-t, --test 仅作测试，并不真的删除文件或目录。-U, --exclude-user=user 不删除属于谁的文件。-v, --verbose 打印详细信息。-x, --exclude=path 排除路径，如果路径是一个目录，它包含的所有文件被排除了。如果路径不存在，它必须是一个绝对路径不包含符号链接。-X, --exclude-pattern=pattern 排除某规则下的路径。","categories":[{"name":"工具","slug":"工具","permalink":"http://www.wortyby.com/categories/工具/"}],"tags":[{"name":"Linux 工具","slug":"Linux-工具","permalink":"http://www.wortyby.com/tags/Linux-工具/"},{"name":"清理","slug":"清理","permalink":"http://www.wortyby.com/tags/清理/"}]}]}