<!-- build time:Wed Dec 19 2018 15:27:50 GMT+0800 (CST) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="s7SsnD6XoVEjf4lGw6yIe12eRfFcaVgdbKYMDPHk0zU"><meta name="yandex-verification" content="4671e153e7289572"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="FM,算法,"><link rel="alternate" href="/atom.xml" title="值得荐" type="application/atom+xml"><meta name="description" content="内容列表 写在前面 因子分解机因子分解机（Factorization Machine，简称FM），又称分解机器。是由Konstanz大学（德国康斯坦茨大学）Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？用户在网站上的行为数据会被Server端以日志的形式"><meta name="keywords" content="FM,算法"><meta property="og:type" content="article"><meta property="og:title" content="深入浅出ML之Factorization家族"><meta property="og:url" content="http://www.wortyby.com/2018/09/21/深入浅出ML之Factorization家族/index.html"><meta property="og:site_name" content="值得荐"><meta property="og:description" content="内容列表 写在前面 因子分解机因子分解机（Factorization Machine，简称FM），又称分解机器。是由Konstanz大学（德国康斯坦茨大学）Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？用户在网站上的行为数据会被Server端以日志的形式"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_1_fm_mf_example.png"><meta property="og:image" content="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_2_ffm_samples.png"><meta property="og:updated_time" content="2018-09-21T09:51:49.570Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="深入浅出ML之Factorization家族"><meta name="twitter:description" content="内容列表 写在前面 因子分解机因子分解机（Factorization Machine，简称FM），又称分解机器。是由Konstanz大学（德国康斯坦茨大学）Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？用户在网站上的行为数据会被Server端以日志的形式"><meta name="twitter:image" content="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_1_fm_mf_example.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.wortyby.com/2018/09/21/深入浅出ML之Factorization家族/"><title>深入浅出ML之Factorization家族 | 值得荐</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">值得荐</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">值得推荐的原创文章，技术文章或是科普文章</h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>站点地图</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.wortyby.com/2018/09/21/深入浅出ML之Factorization家族/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="1One's Dad"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="值得荐"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">深入浅出ML之Factorization家族</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-21T15:01:51+08:00">2018-09-21 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/推荐/" itemprop="url" rel="index"><span itemprop="name">推荐</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">5,337 字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">20 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><h3 id="内容列表"><a class="markdownIt-Anchor" href="#内容列表"></a> 内容列表</h3><h4 id="写在前面"><a class="markdownIt-Anchor" href="#写在前面"></a> 写在前面</h4><h5 id="因子分解机"><a class="markdownIt-Anchor" href="#因子分解机"></a> 因子分解机</h5><p>因子分解机（Factorization Machine，简称FM），又称分解机器。是由Konstanz大学（德国康斯坦茨大学）Steffen Rendle（现任职于Google）于2010年最早提出的，旨在解决大规模稀疏数据下的特征组合问题。在系统介绍FM之前，我们先了解一下在实际应用场景中，稀疏数据是怎样产生的？</p><p>用户在网站上的行为数据会被Server端以日志的形式记录下来，这些数据通常会存放在多台存储机器的硬盘上。</p><blockquote><p>以我浪为例，各产品线纪录的用户行为日志会通过flume等日志收集工具交给数据中心托管，它们负责把数据定时上传至HDFS上，或者由数据中心生成Hive表。</p></blockquote><p>我们会发现日志中大多数出现的特征是categorical类型的，这种特征类型的取值仅仅是一个标识，本身并没有实际意义，更不能用其取值比较大小。比如日志中记录了用户访问的频道（channel）信息，如”news”, “auto”, “finance”等。</p><blockquote><p>假设channel特征有10个取值，分别为{“auto”,“finance”,“ent”,“news”,“sports”,“mil”,“weather”,“house”,“edu”,“games”}。部分训练数据如下：</p></blockquote><table><thead><tr><th>user</th><th>channel</th></tr></thead><tbody><tr><td>user1</td><td>sports</td></tr><tr><td>user2</td><td>news</td></tr><tr><td>user3</td><td>finance</td></tr><tr><td>user4</td><td>house</td></tr><tr><td>user5</td><td>edu</td></tr><tr><td>user6</td><td>news</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><p>特征 <strong>ETL</strong> 过程中，需要对categorical型特征进行one-hot编码（独热编码），即将categorical型特征转化为数值型特征。channel特征转化后的结果如下：</p><blockquote></blockquote><table><thead><tr><th>user</th><th>chn-auto</th><th>chn-finance</th><th>chn-ent</th><th>chn-news</th><th>chn-sports</th><th>chn-mil</th><th>chn-weather</th><th>chn-house</th><th>chn-edu</th><th>chn-games</th></tr></thead><tbody><tr><td>user1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>user2</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>user3</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>user4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>user5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>user6</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table><p>可以发现，由 <strong>one-hot编码</strong> 带来的数据稀疏性会导致特征空间变大。上面的例子中，一维categorical特征在经过one-hot编码后变成了10维数值型特征。真实应用场景中，未编码前特征总维度可能仅有数十维或者到数百维的categorical型特征，经过one-hot编码后，达到数千万、数亿甚至更高维度的数值特征在业内都是常有的。</p><blockquote><p>我组广告和推荐业务的点击预估系统，编码前是特征不到100维，编码后（包括feature hashing）的维度达百万维量级。</p></blockquote><p>此外也能发现，特征空间增长的维度取决于categorical型特征的取值个数。在数据稀疏性的现实情况下，我们如何去利用这些特征来提升learning performance？</p><h3 id="特征关联以及表征形式"><a class="markdownIt-Anchor" href="#特征关联以及表征形式"></a> 特征关联以及表征形式</h3><p>或许在学习过程中考虑特征之间的关联信息。针对特征关联，我们需要讨论两个问题：1. 为什么要考虑特征之间的关联信息？2. 如何表达特征之间的关联？</p><h4 id="为什么要考虑特征之间的关联信息"><a class="markdownIt-Anchor" href="#为什么要考虑特征之间的关联信息"></a> 为什么要考虑特征之间的关联信息？</h4><p>大量的研究和实际数据分析结果表明：某些特征之间的关联信息（相关度）对事件结果的的发生会产生很大的影响。从实际业务线的广告点击数据分析来看，也正式了这样的结论。</p><h4 id="如何表达特征之间的关联"><a class="markdownIt-Anchor" href="#如何表达特征之间的关联"></a> 如何表达特征之间的关联？</h4><p>表示特征之间的关联，最直接的方法的是构造组合特征。样本中特征之间的关联信息在one-hot编码和浅层学习模型（如LR、SVM）是做不到的。目前工业界主要有两种手段得到组合特征：</p><p>人工特征工程（数据分析＋人工构造）；<br>通过模型做组合特征的学习（深度学习方法、FM/FFM方法）</p><p>本章主要讨论FM和FFM用来学习特征之间的关联。我们在<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/" target="_blank" rel="noopener">《第01章：深入浅出ML之Regression家族》</a>看到的多项式回归模型，其中的交叉因子项xixj就是组合特征最直观的例子。</p><blockquote><p>xixj表示特征xi和xj的组合，当xi和xj都非零时，组合特征xixj才有意义。</p></blockquote><p>这里我们以二阶多项式模型（degree=2时）为例，来分析和探讨FM原理和参数学习过程。</p><h3 id="fm模型表达"><a class="markdownIt-Anchor" href="#fm模型表达"></a> FM模型表达</h3><blockquote><p>为了更好的介绍FM模型，我们先从多项式回归、交叉组合特征说起，然后自然地过度到FM模型。</p></blockquote><p><strong>二阶多项式回归模型</strong></p><p>我们先看二阶多项式模型的表达式：</p><blockquote></blockquote><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;</mo></mover></mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:=</mo><munder><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><munder><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>&#x23DF;</mo></munder></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&#x7EBF;&#x6027;&#x56DE;&#x5F52;</mtext></mrow></munder><mo>+</mo><munder><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><munder><mrow><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>&#x23DF;</mo></munder></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&#x4EA4;&#x53C9;&#x9879;&#xFF08;&#x7EC4;&#x5408;&#x7279;&#x5F81;&#xFF09;</mtext></mrow></munder><mspace width="2em"><mtext>(n.ml.1.9.1)</mtext></mspace></math><p>其中，n表示样本特征维度，截距 <strong>w0∈R,w＝{w1,w2,⋯,wn}∈Rn,wij∈Rn×n</strong> 为模型参数。</p><p>从公式(n.ml.1.9.1)可知，交叉项中的组合特征参数总共有n(n−1)2个。在这里，<strong>任意两个交叉项参数wij都是独立的</strong>。然而，在数据非常稀疏的实际应用场景中，交叉项参数的学习是很困难的。why？</p><p>因为我们知道，<strong>回归模型的参数w的学习结果就是从训练样本中计算充分统计量（凡是符合<a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank" rel="noopener">指数族分布</a>的模型都具有此性质）</strong>，而在这里交叉项的每一个参数wij的学习过程需要大量的xi、xj同时非零的训练样本数据。由于样本数据本来就很稀疏，能够满足“xi和xj都非零”的样本数就会更少。训练样本不充分，学到的参数wij就不是充分统计量结果，导致参数wij不准确，而这会严重影响模型预测的效果（performance）和稳定性。How to do it ?</p><p>那么，如何在降低数据稀疏问题给模型性能带来的重大影响的同时，有效地解决二阶交叉项参数的学习问题呢？矩阵分解方法已经给出了解决思路。这里借用CMU讨论课中提到的<a href="http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf" target="_blank" rel="noopener">FM课件</a>和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">美团－深入FFM原理与实践</a>中提到的矩阵分解例子（美团技术团队的分享很赞👍）。</p><blockquote><p>在基于Model-Based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。如下图所示。<br><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_1_fm_mf_example.png" alt=""><br>上图把每一个user表示成了一个二维向量，同时也把item表示成一个二维向量，两个向量的内积就是矩阵中user对item的打分。</p></blockquote><p>根据矩阵分解的启发，如果把多项式模型中二阶交叉项参数wij组成一个对称矩阵W（对角元素设为正实数），那么这个矩阵就可以分解为W=VVT，V∈Rn×k称为系数矩阵，其中第i行对应着第i维特征的隐向量 (这部分在FM公式解读中详细介绍)。</p><p>将每个交叉项参数wij用隐向量的内积⟨vi,vj⟩表示，是FM模型的核心思想。下面对FM模型表达式和参数求解过程，给出详细解读。</p><h3 id="fm模型表达-2"><a class="markdownIt-Anchor" href="#fm模型表达-2"></a> FM模型表达</h3><p>这里我们只讨论二阶FM模型（degree＝2），其表达式为：</p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">&#x005E;</mo></mover></mrow><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>:=</mo><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>j</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>j</mi></msub><mspace width="2em"><mtext>(ml.1.9.1)</mtext></mspace></math><p>其中，vi表示第i特征的隐向量，⟨⋅,⋅⟩表示两个长度为k的向量的内积，计算公式为：</p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>j</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo><mo>:=</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><mo>&#x22C5;</mo><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>,</mo><mi>f</mi></mrow></msub><mspace width="2em"><mtext>(ml.1.9.2)</mtext></mspace></math><h4 id="公式解读"><a class="markdownIt-Anchor" href="#公式解读"></a> 公式解读：</h4><h5 id="线性模型交叉项"><a class="markdownIt-Anchor" href="#线性模型交叉项"></a> 线性模型＋交叉项</h5><p>直观地看FM模型表达式，前两项是<a href="http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">线性回归模型</a>的表达式，最后一项是二阶特征交叉项（又称组合特征项），表示模型将两个互异的特征分量之间的关联信息考虑进来。<em>用交叉项表示组合特征，从而建立特征与结果之间的非线性关系。</em></p><h5 id="交叉项系数-隐向量内积"><a class="markdownIt-Anchor" href="#交叉项系数-隐向量内积"></a> 交叉项系数 → 隐向量内积</h5><p>由于FM模型是在线性回归基础上加入了特征交叉项，模型求解时不直接求特征交叉项的系数wij（因为对应的组合特征数据稀疏，参数学习不充分），故而采用隐向量的内积⟨vi,vj⟩表示wij。</p><p>具体的，FM求解过程中的做法是：对每一个特征分量xi引入<strong>隐向量vi＝(vi,1,vi,2,⋯,vi,k)</strong>，利用vivTj内积结果对交叉项的系数wij进行估计，公式表示：</p><blockquote></blockquote><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>w</mi><mo stretchy="false">&#x005E;</mo></mover></mrow><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mi>j</mi></mrow></msub><mo>:=</mo><msub><mi>v</mi><mi>i</mi></msub><msubsup><mi>v</mi><mi>j</mi><mi>T</mi></msubsup></math><p>隐向量的长度k称为超参数(k∈N+,k≪n)，vi=(vi,1,vi,2,⋯,vi,k)的含义是用k个描述特征的因子来表示第i维特征。根据公式(ml.1.9.1)，二阶交叉项的参数由n⋅n个减少到n⋅k个，远少于二阶多项式模型中的参数数量。</p><p>此外，<strong>参数因子化表示后，使得xhxi的参数与xixj的参数不再相互独立</strong>。这样我们就可以在样本稀疏情况下相对合理的估计FM模型交叉项的参数。具体地：</p><blockquote></blockquote><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>h</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mtd><mtd><mi></mi><mo>:=</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>h</mi><mo>,</mo><mi>f</mi></mrow></msub><mo>&#x22C5;</mo><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><mspace width="1em"><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mspace></mtd></mtr><mtr><mtd><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>j</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mtd><mtd><mi></mi><mo>:=</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><mo>&#x22C5;</mo><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>,</mo><mi>f</mi></mrow></msub><mspace width="thickmathspace"><mspace width="1em"><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mspace></mspace></mtd></mtr></mtable><mspace width="2em"><mo stretchy="false">(</mo><mi>n</mi><mo>.</mo><mi>m</mi><mi>l</mi><mn>.1</mn><mn>.9</mn><mn>.2</mn><mo stretchy="false">)</mo></mspace></math><p>xhxi 与xixj的系数分别为⟨vh,vi⟩和⟨vi,vj⟩，他们之间有共同项vi。也就是说，所有包含xi的非零组合特征（存在某个j≠i，使得xixj≠0）的样本都可以用来学习隐向量vi，这在很大程度上避免了数据稀疏行造成参数估计不准确的影响。</p><blockquote><p>在二阶多项式模型中，参数whi和wij的学习过程是相互独立的。</p></blockquote><p>论文中还提到FM模型的应用场景，并且说公式(ml.1.9.1)作为一个通用的拟合模型（Generic Model），可以采用不同的损失函数来解决具体问题。比如：</p><table><thead><tr><th>FM应用场景</th><th>损失函数</th><th>说明</th></tr></thead><tbody><tr><td>回归</td><td>均方误差（MSE）损失</td><td>Mean Square Error，与平方误差类似</td></tr><tr><td>二类分类</td><td>Hinge/Cross-Entopy损失</td><td>分类时，结果需要做sigmoid变换</td></tr><tr><td>排序</td><td>.</td><td></td></tr></tbody></table><h3 id="fm参数学习"><a class="markdownIt-Anchor" href="#fm参数学习"></a> FM参数学习</h3><h4 id="等式变换"><a class="markdownIt-Anchor" href="#等式变换"></a> 等式变换</h4><p>公式(ml.1.9.1)中直观地看，FM模型的复杂度为O(kn2)，但是通过下面的等价转换，可以将FM的二次项化简，其复杂度可优化到O(kn)。即：</p><math display="block"><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>j</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mrow><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mrow><mo>&#x27EE;</mo><mrow><msup><mrow><mo>(</mo><mrow><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>&#x2212;</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msubsup><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow><mn>2</mn></msubsup><msubsup><mi>x</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>&#x27EF;</mo></mrow></mrow><mspace width="2em"><mo stretchy="false">(</mo><mi>m</mi><mi>l</mi><mn>.1</mn><mn>.9</mn><mn>.3</mn><mo stretchy="false">)</mo></mspace></math><p>下面给出详细推导：</p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd><mtd><mi></mi><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>j</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mrow><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>j</mi></msub><mspace width="2em"><mspace width="2em"><mspace width="2em"><mspace width="2em"><mspace width="2em"><mspace width="2em"><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mspace></mspace></mspace></mspace></mspace></mspace></mtd></mtd></mtr><mtr><mtd><mo>=</mo></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>j</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mrow><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>j</mi></msub><mo>&#x2212;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mrow><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mspace width="2em"><mspace width="2em"><mspace width="thickmathspace"><mspace width="thickmathspace"><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mspace></mspace></mspace></mspace></mtd></mtr><mtr><mtd><mo>=</mo></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo>(</mo><mrow><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>j</mi></msub><mo>&#x2212;</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mspace width="2em"><mspace width="thinmathspace"><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mspace></mspace></mtd></mtr><mtr><mtd><mo>=</mo></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mrow><mo>&#x27EE;</mo><mrow><mrow><mo>(</mo><mrow><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mo>&#x22C5;</mo><mrow><mo>(</mo><mrow><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow><mo>&#x2212;</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msubsup><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow><mn>2</mn></msubsup><msubsup><mi>x</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>&#x27EF;</mo></mrow></mrow><mspace width="1em"><mspace width="thickmathspace"><mspace width="thickmathspace"><mspace width="thinmathspace"><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mspace></mspace></mspace></mspace></mtd></mtr><mtr><mtd><mo>=</mo></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mrow><mo>&#x27EE;</mo><mrow><msup><mrow><mo>(</mo><mrow><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>&#x2212;</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msubsup><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow><mn>2</mn></msubsup><msubsup><mi>x</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>&#x27EF;</mo></mrow></mrow><mspace width="2em"><mspace width="2em"><mspace width="2em"><mspace width="thickmathspace"><mspace width="thickmathspace"><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mspace></mspace></mspace></mspace></mspace></mtd></mtr></mtable><mspace width="2em"><mo stretchy="false">(</mo><mi>n</mi><mo>.</mo><mi>m</mi><mi>l</mi><mn>.1</mn><mn>.9</mn><mn>.3</mn><mo stretchy="false">)</mo></mspace></math> 解读第（1）步到第（2）步，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角与上三角相等，有下式成立： <math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><mn>2</mn><mi>A</mi><mo>+</mo><mi>B</mi><mo stretchy="false">)</mo><mo>&#x2212;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>B</mi><mo>.</mo><mspace width="1em"><munder><mrow><mi>A</mi><mo>=</mo><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>j</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mrow><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>&#x005F;</mo></munder><mo>;</mo><mspace width="1em"><munder><mrow><mi>B</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><mo fence="false" stretchy="false">&#x27E8;</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo>,</mo><msub><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">v</mi></mrow><mi>i</mi></msub><mo fence="false" stretchy="false">&#x27E9;</mo></mrow><msub><mi>x</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>&#x005F;</mo></munder><mspace width="1em"><mo stretchy="false">(</mo><mi>n</mi><mo>.</mo><mi>m</mi><mi>l</mi><mn>.1</mn><mn>.9</mn><mn>.4</mn><mo stretchy="false">)</mo></mspace></mspace></mspace></math><p>如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：</p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mfrac><mi mathvariant="normal">&#x2202;</mi><mrow><mi mathvariant="normal">&#x2202;</mi><mi>&#x03B8;</mi></mrow></mfrac><mi>y</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mtr><mtd><mn>1</mn><mo>,</mo></mtd><mtd><mtext>if</mtext><mspace width="thickmathspace"><mi>&#x03B8;</mi><mspace width="thickmathspace"><mtext>is</mtext><mspace width="thickmathspace"><msub><mi>w</mi><mn>0</mn></msub><mspace width="2em"><mtext>(&#x5E38;&#x6570;&#x9879;)</mtext></mspace></mspace></mspace></mspace></mtd></mtr><mtr><mtd><msub><mi>x</mi><mi>i</mi></msub></mtd><mtd><mtext>if</mtext><mspace width="thickmathspace"><mi>&#x03B8;</mi><mspace width="thickmathspace"><mtext>is</mtext><mspace width="thickmathspace"><msub><mi>w</mi><mi>i</mi></msub><mspace width="thickmathspace"><mspace width="2em"><mtext>(&#x7EBF;&#x6027;&#x9879;)</mtext></mspace></mspace></mspace></mspace></mspace></mtd></mtr><mtr><mtd><msub><mi>x</mi><mi>i</mi></msub><munderover><mo>&#x2211;</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></munderover><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>,</mo><mi>f</mi></mrow></msub><msub><mi>x</mi><mi>j</mi></msub><mo>&#x2212;</mo><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><msubsup><mi>x</mi><mi>i</mi><mn>2</mn></msubsup><mo>,</mo></mtd><mtd><mtext>if</mtext><mspace width="thickmathspace"><mi>&#x03B8;</mi><mspace width="thickmathspace"><mtext>is</mtext><mspace width="thickmathspace"><msub><mi>v</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>,</mo><mi>f</mi></mrow></msub><mspace width="2em"><mtext>(&#x4EA4;&#x53C9;&#x9879;)</mtext></mspace></mspace></mspace></mspace></mtd></mtr></mtable><mo fence="true" stretchy="true" symmetric="true"></mo></mrow><mspace width="2em"><mspace width="1em"><mo stretchy="false">(</mo><mi>m</mi><mi>l</mi><mn>.1</mn><mn>.9</mn><mn>.4</mn><mo stretchy="false">)</mo></mspace></mspace></math> 其中， vj,f是隐向量vj的第f个元素。<h3 id="梯度法训练fm"><a class="markdownIt-Anchor" href="#梯度法训练fm"></a> 梯度法训练FM</h3><p>给出伪代码</p><h3 id="fm训练复杂度"><a class="markdownIt-Anchor" href="#fm训练复杂度"></a> FM训练复杂度</h3><p>由于∑nj=1vj,fxj只与f有关，在参数迭代过程中，只需要计算第一次所有f的∑nj=1vj,fxj，就能够方便地得到所有vi,f的梯度。显然，计算所有f的∑nj=1vj,fxj的复杂度是O(kn)；已知∑nj=1vj,fxj时，计算每个参数梯度的复杂度是O(n)；得到梯度后，更新每个参数的复杂度是 O(1)；模型参数一共有nk+n+1个。因此，FM参数训练的时间复杂度为O(kn)。</p><p><strong>综上可知</strong>，FM算法可以在线性时间内完成模型训练，以及对新样本做出预测，所以说FM是一个非常高效的模型。</p><h3 id="fm总结"><a class="markdownIt-Anchor" href="#fm总结"></a> FM总结</h3><p>上面我们主要是从FM模型引入（多项式开始）、模型表达和参数学习的角度介绍的FM模型，这里我把我认为FM最核心的精髓和价值总结出来，与大家讨论。FM模型的核心作用可以概括为以下3个：</p><h4 id="1-fm降低了交叉项参数学习不充分的影响"><a class="markdownIt-Anchor" href="#1-fm降低了交叉项参数学习不充分的影响"></a> 1. FM降低了交叉项参数学习不充分的影响</h4><p>one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。作者借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数wij用对应特征隐向量的内积表示，即⟨vi,vj⟩（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数wij的过程，转变为学习n个单特征对应k维隐向量的过程。</p><p>很明显，单特征参数（k维隐向量vi）的学习要比交叉项参数wij学习得更充分。示例说明：</p><blockquote><p>假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下：</p></blockquote><table><thead><tr><th>共现交叉特征</th><th>样本数</th><th>注</th></tr></thead><tbody><tr><td>&lt;女性，汽车&gt;</td><td>500</td><td>同时出现&lt;女性，汽车&gt;的样本数</td></tr><tr><td>&lt;女性，化妆品&gt;</td><td>1000</td><td>同时出现&lt;女性，化妆品&gt;的样本数</td></tr><tr><td>&lt;男性，汽车&gt;</td><td>1500</td><td>同时出现&lt;男性，汽车&gt;的样本数</td></tr><tr><td>&lt;男性，化妆品&gt;</td><td>0</td><td>样本中无此特征组合项</td></tr></tbody></table><p><strong>&lt;女性，汽车&gt;</strong> 的含义是 <em>女性看汽车广告</em>。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。</p><p>因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响。</p><h4 id="2-fm提升了模型预估能力"><a class="markdownIt-Anchor" href="#2-fm提升了模型预估能力"></a> 2. FM提升了模型预估能力</h4><p>依然看上面的示例，样本中没有*&lt;男性，化妆品&gt;<em>交叉特征，即</em>没有男性看化妆品广告<em>的数据。如果用多项式模型来建模，对应的交叉项参数</em>w男性,化妆品<em>是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的</em>男性看化妆品广告*场景给出准确地预估。</p><p>FM模型是否能得到交叉项参数w男性,化妆品呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为w男性,化妆品=⟨v男性,v化妆品⟩。</p><p>用男性特征隐向量v男性和化妆品特征隐向量v化妆品的内积表示交叉项参数w男性,化妆品。</p><p>由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用⟨v男性,v化妆品⟩得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估能力。</p><h4 id="3-fm提升了参数学习效率"><a class="markdownIt-Anchor" href="#3-fm提升了参数学习效率"></a> 3. FM提升了参数学习效率</h4><p>这个显而易见，参数个数由(n2+n+1)变为(nk+n+1)个，模型训练复杂度也由O(mn2)变为O(mnk)。m为训练样本数。对于训练样本和特征数而言，都是线性复杂度。</p><p>此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。</p><p>从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑<em>User-Ad-Context</em>三个维度特征之间的关系，在FM模型中对应的degree为3。</p><h4 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h4><p>最后一句话总结，FM最大特点和优势：</p><p>FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力。</p><h3 id="场感知分解机"><a class="markdownIt-Anchor" href="#场感知分解机"></a> 场感知分解机</h3><p>场感知分解机器（Field-aware Factorization Machine ，简称FFM）最初的概念来自于Yu-Chin Juan与其比赛队员，它们借鉴了辣子Michael Jahrer的论文中field概念，提出了FM的升级版模型。</p><p>通过引入field的概念，FFM吧相同性质的特征归于同一个field。在FM开头one-hot编码中提到用于访问的channel，编码生成了10个数值型特征，这10个特征都是用于说明用户PV时对应的channel类别，因此可以将其放在同一个field中。那么，我们可以把同一个categorical特征经过one-hot编码生成的数值型特征都可以放在同一个field中。</p><blockquote><p>同一个categorical特征可以包括用户属性信息（年龄、性别、职业、收入、地域等），用户行为信息（兴趣、偏好、时间等），上下文信息（位置、内容等）以及其它信息（天气、交通等）。</p></blockquote><p>在FFM中，每一维特征xi，针对其它特征的每一种”field” fj，都会学习一个隐向量vi,fj。因此，隐向量不仅与特征相关，也与field相关。</p><p>假设每条样本的n个特征属于f个field，那么FFM的二次项有nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。因此可以吧FM看作是FFM的特例，即把所有的特征都归属到一个field是的FFM模型。根据FFM的field敏感特性，可以导出其模型表达式：<br>\[<br>ŷ (x):=w0+∑i=1nwixi+∑i=1n∑j=i+1n⟨vi,fj,vj,fi⟩xixj(ml.1.9.5)<br>\]</p><p>其中，fj是第j个特征所属的field。如果隐向量的长度为k，那么FFM的二交叉项参数就有nfk个，远多于FM模型的nk个。此外，由于隐向量与field相关，FFM的交叉项并不能够像FM那样做化简，其预测复杂度为O(kn2)。</p><p>这里以<a href="http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf" target="_blank" rel="noopener">NTU_FFM.pdf</a>和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">美团－深入FFM原理与实践</a>都提到的例子，给出FFM－Fields特征组合的工作过程。</p><blockquote><p>给出一下输入数据：</p></blockquote><table><thead><tr><th>User</th><th>Movie</th><th>Genre</th><th>Price</th></tr></thead><tbody><tr><td>YuChin</td><td>3Idiots</td><td>Comedy, Drama</td><td>$9.99</td></tr></tbody></table><p>Price是数值型特征，实际应用中通常会把价格划分为若干个区间（即连续特征离散化），然后再one-hot编码，这里假设$9.99对应的离散化区间tag为”2”。当然不是所有的连续型特征都要做离散化，比如某广告位、某类广告／商品、抑或某类人群统计的历史CTR（pseudo－CTR）通常无需做离散化。</p><blockquote></blockquote><p>该条记录可以编码为5个数值特征，即<em>User^YuChin</em>, <em>Movie^3Idiots</em>, <em>Genre^Comedy</em>, <em>Genre^Drama</em>, <em>Price<sup>2*。其中*Genre</sup>Comedy</em>, <em>Genre^Drama</em>属于同一个field。为了说明FFM的样本格式，我们把所有的特征和对应的field映射成整数编号。</p><blockquote></blockquote><table><thead><tr><th>Field Name</th><th>Field Index</th><th>Feature Name</th><th>Feature Index</th></tr></thead><tbody><tr><td>User</td><td>1</td><td>User^YuChin</td><td>1</td></tr><tr><td>Movie</td><td>2</td><td>Movie^3Idiots</td><td>2</td></tr><tr><td>Genre</td><td>3</td><td>Genre^Comedy</td><td>3</td></tr><tr><td>－</td><td>－</td><td>Genre^Drama</td><td>4</td></tr><tr><td>Price</td><td>4</td><td>Price^2</td><td>5</td></tr></tbody></table><p>那么，FFM所有的（二阶）组合特征共有10项（C25=5×42!=10），即为：</p><blockquote></blockquote><p><img src="https://raw.githubusercontent.com/ComputationalAdvertising/spark_lr/master/img/ml_9_2_2_ffm_samples.png" alt=""></p><blockquote></blockquote><p>其中，红色表示Field编码，蓝色表示Feature编码，绿色表示样本的组合特征取值（离散化后的结果）。二阶交叉项的系数是通过与Field相关的隐向量的内积得到的。如果单特征有n个，全部做二阶特征组合的话，会有C2n=n(n−1)2个。</p><h4 id="ffm应用场景"><a class="markdownIt-Anchor" href="#ffm应用场景"></a> FFM应用场景</h4><p>在我们的广告业务系统、商业推荐以及自媒体－推荐系统中，<strong>FFM模型作为点击预估系统中的核心算法之一，用于预估广告、商品、文章的点击率（CTR）和转化率（CVR）</strong>。</p><p>在鄙司广告算法团队，点击预估系统已成为基础设施，支持并服务于不同的业务线和应用场景。预估模型都是离线训练，然后定时更新到线上实时计算，因此预估问题最大的差异就体现在数据场景和特征工程。以广告的点击率为例，特征主要分为如下几类：</p><ul><li>用户属性与行为特征：</li><li>广告特征：</li><li>上下文环境特征：</li></ul><p>为了使用开源的FFM模型，所以的特征必须转化为<code>field_id:feat_id:value</code>格式，其中<code>field_id</code>表示特征所属<code>field</code>的编号，<code>feat_id</code>表示特征编号，<code>value</code>为特征取值。数值型的特征如果无需离散化，只需分配单独的field编号即可，如历史<code>pseudo-ctr</code>。<code>categorical</code>特征需要经过<code>one-hot</code>编码转化为数值型，编码产生的所有特征同属于一个<code>field</code>，特征<code>value</code>只能是0/1, 如用户年龄区间、性别、兴趣、人群等。</p><p><strong>开源工具FFM使用时，注意事项</strong>（参考新浪广告算法组的实战经验和<a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">美团－深入FFM原理与实践</a>）:</p><h5 id="样本归一化"><a class="markdownIt-Anchor" href="#样本归一化"></a> 样本归一化：</h5><h5 id="特征归一化"><a class="markdownIt-Anchor" href="#特征归一化"></a> 特征归一化：</h5><h5 id="省略0值特征"><a class="markdownIt-Anchor" href="#省略0值特征"></a> 省略0值特征：</h5><p>回归、分类、排序等。推荐算法，预估模型（如CTR预估等）</p><h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h4><ul><li>Sina广告点击预估系统实践</li><li>FM、FFM相关Paper、技术博客</li><li><a href="http://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">美团技术团队</a></li></ul><p><a href="http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/" target="_blank" rel="noopener">原文</a></p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>~赏你叻~</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.jpg" alt="1One's Dad 支付宝"><p>支付宝</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/FM/" rel="tag"># FM</a> <a href="/tags/算法/" rel="tag"># 算法</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/09/21/FM(因式分解机推荐算法原理）/" rel="next" title="FM(因式分解机推荐算法原理)"><i class="fa fa-chevron-left"></i> FM(因式分解机推荐算法原理)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/09/30/大数据分析利器KYLIN 之用户权限篇/" rel="prev" title="大数据分析利器KYLIN 之用户权限篇">大数据分析利器KYLIN 之用户权限篇 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">1One's Dad</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">44</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><span class="site-state-item-count">20</span> <span class="site-state-item-name">分类</span></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">63</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/sevenfeng012" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=a1hfWFpbU15TXSsaGkUIBAY" target="_blank" title="E-Mail"><i class="fa fa-fw fa-邮件"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/feng-seven-48/activities" target="_blank" title="ZHIHU"><i class="fa fa-fw fa-知乎"></i>ZHIHU</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#内容列表"><span class="nav-number">1.</span> <span class="nav-text">内容列表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#写在前面"><span class="nav-number">1.1.</span> <span class="nav-text">写在前面</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#因子分解机"><span class="nav-number">1.1.1.</span> <span class="nav-text">因子分解机</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征关联以及表征形式"><span class="nav-number">2.</span> <span class="nav-text">特征关联以及表征形式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要考虑特征之间的关联信息"><span class="nav-number">2.1.</span> <span class="nav-text">为什么要考虑特征之间的关联信息？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何表达特征之间的关联"><span class="nav-number">2.2.</span> <span class="nav-text">如何表达特征之间的关联？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fm模型表达"><span class="nav-number">3.</span> <span class="nav-text">FM模型表达</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fm模型表达-2"><span class="nav-number">4.</span> <span class="nav-text">FM模型表达</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#公式解读"><span class="nav-number">4.1.</span> <span class="nav-text">公式解读：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#线性模型交叉项"><span class="nav-number">4.1.1.</span> <span class="nav-text">线性模型＋交叉项</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#交叉项系数-隐向量内积"><span class="nav-number">4.1.2.</span> <span class="nav-text">交叉项系数 → 隐向量内积</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fm参数学习"><span class="nav-number">5.</span> <span class="nav-text">FM参数学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#等式变换"><span class="nav-number">5.1.</span> <span class="nav-text">等式变换</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度法训练fm"><span class="nav-number">6.</span> <span class="nav-text">梯度法训练FM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fm训练复杂度"><span class="nav-number">7.</span> <span class="nav-text">FM训练复杂度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fm总结"><span class="nav-number">8.</span> <span class="nav-text">FM总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-fm降低了交叉项参数学习不充分的影响"><span class="nav-number">8.1.</span> <span class="nav-text">1. FM降低了交叉项参数学习不充分的影响</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-fm提升了模型预估能力"><span class="nav-number">8.2.</span> <span class="nav-text">2. FM提升了模型预估能力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-fm提升了参数学习效率"><span class="nav-number">8.3.</span> <span class="nav-text">3. FM提升了参数学习效率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-number">8.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#场感知分解机"><span class="nav-number">9.</span> <span class="nav-text">场感知分解机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ffm应用场景"><span class="nav-number">9.1.</span> <span class="nav-text">FFM应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#样本归一化"><span class="nav-number">9.1.1.</span> <span class="nav-text">样本归一化：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#特征归一化"><span class="nav-number">9.1.2.</span> <span class="nav-text">特征归一化：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#省略0值特征"><span class="nav-number">9.1.3.</span> <span class="nav-text">省略0值特征：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参考资料"><span class="nav-number">9.2.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">1One's Dad</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">站点总文字&#58;</span> <span title="站点总文字">75.6k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html><!-- rebuild by neat -->